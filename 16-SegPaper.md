# 1. 修改deeplab，简化语义标签，提高实时性。

# 2. 在MultiNet基础上修改，完成道路分割、车辆、行人等检测。

# 3. 打破FCN Encoder-Decoder架构,设计one step end-to-end网络。预测语义分割每一类物体在图片上的像素轮廓，而不是端到端输出图片。

# 4. 将[pooling层]换成[边缘检测层]试一下效果; 能不能让Pooling层在降size的同时，层内参数可训练化（让网络自己选择保留哪些信息，而不仅仅是MaxPooling或AVGPooling）

# 5. 注意力模型

# 6. 现在训练网络都是让网络“猜dog在哪里？在这里”(前向传播), "不对，你离正确答案多远"（计算loss）；“我再猜一下”（反向传播）；能不能设计一种训练结构，告诉网络“这张图中dog在哪里”。

# 7. 在分割网络中加入ROI-Pooling层
* Input -> ROI Pooling -> FCN

# 8. 在分割网络中加入边缘检测层，结合边缘对score层进行采样分类，当一个轮廓中大于阈值的像素点属于A类，则划为A类。

# 9. 在激活函数Relu等上做文章，让网络逐渐收敛到真值（比如正负二分类），不用softmax和argmax等。

# 10. 在feature map中手工加入边缘检测层， 让网络自己学习把边缘信息加入分割判断中。

# 11. 将卷积操作的 'bias'值换成相应区域的边缘图像试试。

# 12. COS_Net.jpg

#　13. 分割网络输出层21channels，每个channels表达一个类别的mask， 将分类和分割解耦。

# 14. yolo检测道路主要类别+语义分割道路，实时跑在北三环数据集上。

# 15. `DANet` + `DeepLabV3+` + `# 13.` + `Pooling -> Conv`

# 16. 在FCN Decoder 中加入多个seg loss (ROI Align生成feature map),训练网络。
The `auxiliary loss` helps `optimize the learning process`, while the master branch loss takes the most responsibility. We add weight to balance the auxiliary loss <br>
In the `testing phase`, we abandon this auxiliary branch and only use the well optimized master branch for final prediction.

# 17. 分割网络输入Image手工添加其他特征channels，比如Canny,Gray,hsv, self-attention等。可以增加显式特征表示。

## 使用ResNet实现的分割网络效果state-of-the-art(SOTA). 

------------------
# 2019.01.08
- [x] **跑通KittiSeg evaluate.py程序**	
- [x] **整理KittiSeg和MultiNet代码**	

# 2019.01.09(评价指标)
- [x] **阅读MaxF1论文(THE KITTI-ROAD DATASET)**
* [blog1](https://blog.csdn.net/sinat_28576553/article/details/80258619)
* ego-lane vs.opposing lane,(自我车道与对方车道)
* 2D Bird’s Eye View (BEV) space.
* For methods that output confidence maps (in contrast to binary road classification), the classification threshold τ is chosen to maximize the F-measure.

- [x] **IOU**
* See KittiSeg code.

# 2019.01.13
- [x] **评价MultNet训练结果**
```python
segmentation Evaluation Finished. Results
Raw Results:
[train] MaxF1 (raw)    :  98.9767 
[train] BestThresh (raw)    :  68.2353 
[train] Average Precision (raw)    :  92.5437 
[val] MaxF1 (raw)    :  95.9975 
[val] BestThresh (raw)    :  14.9020 
[val] Average Precision (raw)    :  92.3125 
Speed (msec) (raw)    :  42.8566 
Speed (fps) (raw)    :  23.3336
```

- [x] **读AP指标（VOC DATASET）**
* [AP wiki](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision)
* [blog1](https://blog.csdn.net/niaolianjiulin/article/details/53098437)
* [blog2](https://blog.csdn.net/hysteric314/article/details/54093734)
* 以recall为横坐标(0,1),precision为纵坐标(0,1)作图。得到一条曲线。该曲线下的面积即为AP.

$$ AP = \int_0^1 {p(r)} dr $$ 

# 2019.01.14
- [x] **标注Cityscape数据集**
* [blog1](https://blog.csdn.net/fabulousli/article/details/78633531)
* [labelme 工具](https://github.com/wkentaro/labelme) 

# 2019.01.15
- [x] **发现好博客**
* [深度学习数据集介绍及相互转换](https://www.cnblogs.com/ranjiewen/p/9860953.html)

- [x] **labelme 工具生成Kitti Road数据集**
* img.putpalette([0,0,0,0,255,0,0,0,255])
	
- [x] **labelme 工具生成CityScapes数据集：**
* from cityscapesscripts.helpers.labels     import name2label 

- [ ] **cityscapesScripts标注自己图片**

# 2019.01.16
- [x] **[Semantic Segmentation using Fully Convolutional Networks over the years]**
* [link](https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html)
* [vis of FCN](http://ethereon.github.io/netscope/#/preset/fcn-8s-pascal)
* [vis of common networks](http://ethereon.github.io/netscope/quickstart.html)
* [Deconvolutions](https://distill.pub/2016/deconv-checkerboard/)
* LinkNet: A feature map with shape [H, W, n_channels] is first convolved with a [1x1] kernel to get a feature map with shape [H, W, n_channels / 4 ] and then a deconvolution takes it to [2*H, 2*W, n_channels / 4 ] a final [1x1] kernel convolution to take it to [2*H, 2*W, n_channels / 2 ]. Thus the decoder block fewer parameters due to this channel reduction scheme.

# 2019.01.20
- [x] **[Deconvolutions]**
* [link](https://distill.pub/2016/deconv-checkerboard/)
* When we look very closely at `images generated by neural networks`, we often see a strange `checkerboard pattern` of artifacts.
* `In theory`, our models could learn to carefully write to unevenly overlapping positions so that the output is evenly balanced; `In fact`, not only do models with uneven overlap not learn to avoid this, but models with even overlap often learn kernels that cause similar artifacts! 
* Better Upsampling
> 1. `One approach` is to make sure you use `a kernel size that is divided by your stride`, avoiding the overlap issue.
> 2. Another approach is to `separate out` `upsampling` to a higher resolution from `convolution` to compute features. For example: `resize the image (using nearest-neighbor interpolation or bilinear interpolation) and then do a convolutional layer`.

![](https://distill.pub/2016/deconv-checkerboard/assets/upsample_DeconvTypes.svg)

* We don’t necessarily think that either approach is the final solution to upsampling, but they do fix the checkerboard artifacts.
* Whenever we `compute the gradients of a convolutional layer`, we do `deconvolution` (transposed convolution) on the `backward pass`.

- [x] **[2017-course_by_Tao Kong.pdf]**
* The region proposal network is a FCN which outputs `K*(4+2) sized vectors`.
* `Mask R-CNN` = Faster R-CNN with FCN on ROIs.
* Useful links for learning detection:
[RCNN/Fast R-CNN/Faster R-CNN](https://github.com/rbgirshick)
[YOLO/YOLOv2](https://pjreddie.com/darknet/yolo/)
[SSD](https://github.com/weiliu89/caffe/tree/ssd)
[R-FCN](https://github.com/daijifeng001/R-FCN)
[Tensorflow detector](https://github.com/tensorflow/models/tree/master/research/object_detection)

# 2019.01.21
- [x] **基于深度卷积神经网络的目标检测算法研究_黄莉芝.caj**
* 注意力模型
* 语义分割分为“阈值分割”，“边缘分割”， “区域分割”
* 计算边框回归

# 2019.01.22
- [x] **看第四章“目标定位优化”** 

- [x] **专知[目标检测专栏]**
* [link](https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247485072&idx=2&sn=e9f4f0d1daeb3a144e37fcfcc61e908f&chksm=fc85e783cbf26e95d153af7c825ef9b8fc4fc6fd37f75709de98eae769e6b70f8da29ae65a73&scene=21#wechat_redirect)
* [CVPR'17 Tutorial](http://deeplearning.csail.mit.edu/)
* [图像目标检测（Object Detection）原理与实现 （1-6）](https://blog.csdn.net/marvin521/article/details/9058735)
* 下载了[基于特征共享的高效物体检测_任少卿.pdf]
Done Reading !
* 下载了[Bounding-box_regression详解.pdf]

- [x] **[RCNN, Fast-RCNN, Faster-RCNN的一些事]**
* [link](http://closure11.com/rcnn-fast-rcnn-faster-rcnn%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/)
* 在Fast-RCNN中，作者巧妙的把bbox regression放进了神经网络内部，与region分类和并成为了一个multi-task模型
* Bounding-box Regression
* 我的CVPR 2016论文里自己弄的一个数据集就借助了Fast-RCNN.（`可以利用现有分割网络进行数据集粗略分割`）

- [x] **[目标检测——从RCNN到Faster RCNN 串烧]**
* [link](https://blog.csdn.net/xyy19920105/article/details/50817725)

# 2019.01.24 计划
- [x] **[机器学习随笔]**
* [link](https://zhuanlan.zhihu.com/p/35058212)

- [x] **[图像语义分割+FCN/U-Net]**
* [link](https://zhuanlan.zhihu.com/p/31428783)
* [visualization of FCN](http://ethereon.github.io/netscope/#/preset/fcn-8s-pascal)
* 对于FCN-8s，首先进行pool4+2x upsampled feature`逐点相加`，然后又进行pool3+2x upsampled`逐点相加`，即进行更多次特征融合。
* 为了解决图像过小后 1/32 下采样后输出feature map太小情况，FCN原作者在第一个卷积层`conv1_1加入pad=100`。
* 在特征融合的时候，如何保证逐点相加的feature map是一样大的呢？这就要引入crop层了
* 在caffe中，存储数据的方式为：caffe blob = [num, channel, height, width]
1. 而score_pool4c设置了axis=2，相当于从第2维(index start from 0!)往后开始裁剪，即裁剪height和width两个维度，同时不改变num和channel纬度
2. 同时设置crop在height和width纬度的开始点为offset=5
3. 用Python语法表示，相当于score_pool4c层的输出为：
```python
crop-pool4 = score-pool4[:, :, 5:5+score2.height, 5:5+score2.width]
```
* `Deconvelution`计算：`score-fr`[1,21,16,16] -> `score2` [1,21,34,34] , kernel_size:4, stride:2
Conv: $out = (in+2*pad-kernel)/stride + 1$
DeConv: $out = (in-1)*stride + kernel -2*pad$
And Now: $34 = (16-1)*2+4-2*0$

# 2019.01.25 
- [x] **[FCN学习:Semantic Segmentation]**
* [link](https://zhuanlan.zhihu.com/p/22976342)
* Faster-RCNN中使用了RPN(Region Proposal Network)替代Selective Search等产生候选区域的方法。`RPN是一种全卷积网络`，所以为了透彻理解这个网络，首先学习一下FCN
* [Caffe 中如何计算卷积](https://www.zhihu.com/question/28385679)
1. Input feature to Matrix

![Input feature to Matrix](https://pic3.zhimg.com/80/69e44f61e8ede5ba84534ca3b764d302_hd.jpg)

2. Filters to matrix

![Filters to matrix](https://pic1.zhimg.com/80/v2-339657291663a4e791a9b34952d5859c_hd.png)

3. `Filter Matrix`乘以`Feature Matrix`的转置,得到输出矩阵`Cout x (H x W)`,就可以解释为输出的三维Blob`(Cout x H x W)`

![](https://pic3.zhimg.com/80/6b1dde11bf30688b4f526ea77d54a196_hd.jpg)

4. Detail sample.

![Caffe conv](https://pic4.zhimg.com/80/a6421bae22236c0509623b8b7f7bbb03_hd.jpg)

5. 多.通道卷积计算方式

![多通道卷积](https://pic1.zhimg.com/80/v2-8d72777321cbf1336b79d839b6c7f9fc_hd.jpg)

* 矩阵微分公式:
$$\frac {d(Ax+b)}{dx} = {A^T}$$

# 2019.01.26
- [x] **[一文读懂Faster RCNN]**
* [link](https://zhuanlan.zhihu.com/p/31426458)
* Faster R-CNN网络结构图:
![Faster R-CNN](https://pic4.zhimg.com/80/v2-e64a99b38f411c337f538eb5f093bdf3_hd.jpg)
* bounding box regression原理
* 对于一副任意大小PxQ图像,传入Faster RCNN前首先reshape到固定MxN,im_info=[M, N, scale_factor]则保存了此次缩放的所有信息,然后经过Conv Layers,经过4次pooling变为WxH=(M/16)x(N/16)大小,其中feature_stride=16则保存了该信息,用于计算anchor偏移量.
------
## update 2019.02.27
* VGG Feature Map: "conv5_3".
* 9个anchors(矩形)共有3种形状，长宽比为大约为{width:height} = {1:1, 1:2, 2:1} 三种(ps. 每种比例有三个尺度)，如图所示:
![anchors](https://pic4.zhimg.com/80/v2-7abead97efcc46a3ee5b030a2151643f_hd.jpg)
* 在caffe基本数据结构blob中以如下形式保存数据：
```python
blob = [batch, channel, height, width] = [N, C, H, W]
```
* bounding box regression原理
> 窗口一般使用四维向量 (x, y, w, h) 表示，分别表示窗口的`中心点`坐标、`宽`和`高`.
> 先做平移，再做缩放。
* 对于训练bouding box regression网络回归分支，输入是`cnn feature Φ`，监督信号是Anchor与GT的差距`(t_x, t_y, t_w, t_h)`，即`训练目标`是：输入 Φ的情况下使网络输出与监督信号尽可能接近。
那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 (t_x, t_y, t_w, t_h)，显然即可用来修正Anchor位置了。
* 解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，`im_info`=[M, N, scale_factor]则保存了此次缩放的所有信息。
* 经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中`feature_stride=16`则保存了该信息，用于计算anchor偏移量。
* RPN网络处理流程：
> 生成anchors -> softmax分类器提取fg anchors -> bbox reg回归fg anchors -> Proposal Layer生成proposal boxes=[x1, y1, x2, y2]
* See [COS_Net.jpg].
* 从PoI Pooling获取到`7x7=49`大小的proposal feature maps.
* 在训练和检测阶段生成和存储anchors的顺序完全一样，这样训练结果才能被用于检测！

# 2019.01.27
- [x] **GDB调试工具**
* [RMS's gdb Debugger Tutorial](http://www.unknownroad.com/rtfm/gdbtut/)
```shell
$ gcc -g inf.c			# Compile the program with debugging flags.
$ gdb a.out				# Lets load up gdb.
$ run & ctrl+c			# Set off the infinite loop, then press `Ctrl-C` to send the program a SIGINT. 
$ backtrace				# We will use the `backtrace` command to examine the stack
$ frame 1
```

- [ ] **gdb 调试入门**
* [gdb 调试入门](http://blog.jobbole.com/107759/)

## Qt学习
- [ ] **Qt入门学习——Qt Creator的使用**
* [Qt入门学习——Qt Creator的使用](https://blog.csdn.net/tennysonsky/article/details/48004119)

- [ ] **Qt 学习之路 2**
* [Qt 学习之路 2](https://www.devbean.net/category/qt-study-road-2/)

# 2019.01.28
- [x] **服务器安装Anaconda环境**
* see `18-Anaconda.md`
* [blog](https://blog.csdn.net/qq_17534301/article/details/80869998)
* [Installing on Linux](http://docs.anaconda.com/anaconda/install/linux/)
* [Anaconda installer archive](https://repo.anaconda.com/archive/)

# 2019.01.29
- [x] **安装Caffe**
* see update of `03-Install_caffe.md`

- [x] **跑FCN**
* git clone https://github.com/shelhamer/fcn.berkeleyvision.org.git
* Download voc-fcn8s caffe model.
```shell
$ cd fcn.berkeleyvision.org/voc-fcn8s/
$ proxychains wget http://dl.caffe.berkeleyvision.org/fcn8s-heavy-pascal.caffemodel
```
* run
```shell
$ conda activate py27	# for `added by Anaconda3 5.3.0 installer` in '~/.bashrc'
or
$ source activate py27	# for `export PATH="/home/jun/anaconda3/bin:$PATH"` in '~/.bashrc'
$ python infer.py
```

* vis of caffe net.
[NetScope](http://ethereon.github.io/netscope/#/editor)

## sublime 高亮当前行
* [blog](https://yijile.com/log/128.html)


# 2019.02.13
- [x] **跑通Faster R-CNN**
* [github](https://github.com/smallcorgi/Faster-RCNN_TF)

## Install dependeces.
```shell
# change to python 2.7
$ sudo pip install easydict
```
## Build.
```shell
$ cd Faster-RCNN_TF/lib
$ make
```

## Run demo.py
```shell
$ python ./tools/demo.py --model ./data/model/VGGnet_fast_rcnn_iter_70000.ckpt
```
### Run Errors.
* ERROR 1
> tensorflow.python.framework.errors_impl.NotFoundError: /home/jun/Documents/Faster-RCNN_TF/tools/../lib/roi_pooling_layer/roi_pooling.so: undefined symbol: _ZTIN10tensorflow8OpKernelE

```shell
TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')
# Define `TF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')` and modify the `g++` call to include `-L$TF_LIB -ltensorflow_framework`
TF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')

CUDA_PATH=/usr/local/cuda/
CXXFLAGS=''

if [[ "$OSTYPE" =~ ^darwin ]]; then
	CXXFLAGS+='-undefined dynamic_lookup'
fi

cd roi_pooling_layer

if [ -d "$CUDA_PATH" ]; then
	nvcc -std=c++11 -c -o roi_pooling_op.cu.o roi_pooling_op_gpu.cu.cc \
		-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC $CXXFLAGS \
# sm_61 for 1080Ti
		-arch=sm_61
# add `-L$TF_LIB -ltensorflow_framework` & `-D_GLIBCXX_USE_CXX11_ABI=`
	g++ -std=c++11 -shared -o roi_pooling.so roi_pooling_op.cc \
		roi_pooling_op.cu.o -I $TF_INC  -D GOOGLE_CUDA=1 -fPIC $CXXFLAGS -D_GLIBCXX_USE_CXX11_ABI=0 \
		-lcudart -L $CUDA_PATH/lib64 -L $TF_LIB -ltensorflow_framework
else
	g++ -std=c++11 -shared -o roi_pooling.so roi_pooling_op.cc \
		-I $TF_INC -fPIC $CXXFLAGS
fi

cd ..
```
* ERROR 2
> feed_in, dim = (input, int(input_shape[-1])) TypeError: __int__ returned non-int (type NoneType)
```shell
# https://github.com/smallcorgi/Faster-RCNN_TF/issues/316
# add the following lines to lib/roi_pooling_layer/roi_pooling_op.cc, and make
L27 #include "tensorflow/core/framework/shape_inference.h"
L41-L53
.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      //https://github.com/tensorflow/.../core/framework/shape_inference.h
      int pooled_height;
      int pooled_width;
      c->GetAttr("pooled_height", &pooled_height);
      c->GetAttr("pooled_width", &pooled_width);
      auto pooled_height_h = c->MakeDim(pooled_height);
      auto pooled_width_h = c->MakeDim(pooled_width);

      auto output_shape = c->MakeShape({ c->Dim(c->input(1), 0), pooled_height_h, pooled_width_h, c->Dim(c->input(0), 3) });
      c->set_output(0, output_shape);
      return Status::OK();
    });
```

# 2019.02.13
- [x] **Object Detection and Classification using R-CNNs**
* [link](http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/)
* Faster R-CNN Network Architecture

![network](http://www.telesens.co/wp-content/uploads/2018/03/img_5a9ffec911c19.png)

question: 
`bbx_pred_net` shape=[21*4]?



# 2019.02.16
* `seg featuremap` 在voc数据集上`输出`为[1, 21, 500, 500],经过np.argmax(axis=0)得到[500, 500]的segmentation image of class IDs，相当于在channel方向上对每个像素进行分类，取channel方向上概率最大值作为该像素类别。

# 2019.02.21 - 2019.02.27
- [x] **MultiNet源码**
* See [11-MultiNet.md](https://github.com/kinglintianxia/note_book/blob/master/11-MultiNet.md)

# 2019.02.27
- [x] **复习 [一文读懂Faster RCNN]**
* See `update 2019.02.27`.
* [Faster-rcnn详解](https://blog.csdn.net/WZZ18191171661/article/details/79439212)


# 2019.02.28
## CSDN翻译专栏
[CSDN翻译专栏](https://blog.csdn.net/quincuntial/article/details/77263607)

# 2019.02.29 - 2019.03.03
## Mask R-CNN
* [专知语义分割专栏](https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247485464&idx=3&sn=77fd30180cf66e1cb7b276509b38f358&chksm=fc85e90bcbf2601dfe6439076bb00896befe621dcdd75dfb77a931839b3b362d3226c4cd2ad5&mpshare=1&scene=24&srcid=10099bEid5dfACiAXiznsOX8#rd)
```python
Mask-RCNN [https://arxiv.org/pdf/1703.06870.pdf]

https://github.com/CharlesShang/FastMaskRCNN [Tensorflow]

https://github.com/TuSimple/mx-maskrcnn [MxNet]

https://github.com/matterport/Mask_RCNN [Keras]

https://github.com/jasjeetIM/Mask-RCNN [Caffe]
```

--------------------------------------------------
# 2019.03.01
## Mask R-CNN
- [x] [Mask R-CNN详解](https://blog.csdn.net/WZZ18191171661/article/details/79453780)

- [x] **Mask_R-CNN.mp4**

- [x] **maskrcnn_slides.pdf**
* [region-of-interest-pooling-explained](https://deepsense.ai/region-of-interest-pooling-explained/)
* Backbone (`ResNeXt`): +1.6 AP(bbox)

## `RoIPooling` & `RoIAlign`
### `RoIPooling`
1. Let’s consider a small example to see how it works. We’re going to perform region of interest pooling on a single 8×8 feature map, one region of interest and an output size of 2×2. Our input feature map looks like this:

![Feature Map](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/1.jpg)

2. Let’s say we also have a region proposal (top left, bottom right coordinates): (0, 3), (7, 8). In the picture it would look like this:

![region proposal](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/2.jpg)

3. Normally, there’d be multiple feature maps and multiple proposals for each of them, but we’re keeping things simple for the example.
By dividing it into (2×2) sections (because the output size is 2×2) we get:

![2×2 sections](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/3.jpg)

4. The max values in each of the sections are:

![output](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/output.jpg)

5. Here’s our example presented in form of a nice animation:

![gif](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/roi_pooling-1.gif)


### `RoIAlign`
1. unquantized (2×2) sections:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi0.png)

2. Sampling locations:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi1.png)

3. Bilinear interpolated values:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi2.png)

4. Max pooling output:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi3.png)

---------------------------------------------------
# 2019.03.02
## Mask R-CNN

---------------
- [x] **Mask-RCNN-Arc-How-RoI_Pooling-RoI_Warping_RoI-Align_Work.mp4**
* ROI Pooling和ROIAlign最大的区别是：前者使用了两次量化操作，而后者并没有采用量化操作，使用了线性插值算法，具体的解释如下所示:

![ROIPooling](https://img-blog.csdn.net/20180306110240257)
![ROIAlign](https://img-blog.csdn.net/20180306110334767)

----------------
- [x] **2017-Mask R-CNN.pdf**
* [Mask R-CNN 论文翻译](https://alvinzhu.xyz/2017/10/07/mask-r-cnn/#fn:18)
* [Mask R-CNN完整翻译](https://blog.csdn.net/myGFZ/article/details/79136610)
* `RoIPool` performs coarse spatial quantizationfor feature extraction. To fix the misalignment, we propose a simple, quantization-free layer, called `RoIAlign`, that faithfully preserves exact spatial locations.
*  Second, we found it essential to decouple mask and class prediction: we predict a binary mask for each class independently, without competition among classes, and rely on the network’s RoI classification branch to predict the category. 
* Our models can run at about 200ms per frame on a GPU, and training on COCO takes one to two days on a single 8-GPU machine. 
* Instead, our method is based on `parallel prediction of masks and class labels`, which is simpler and more flexible.
* The `mask branch` has a Km2-dimensional output for `each RoI`, which encodes K binary masks of resolution m × m, one for each of the `K classes`.
* For an RoI associated with ground-truth class k, Lmask is only defined on the k-th mask (other mask outputs do not contribute to the loss).
* we rely on the dedicated classification branch to predict the class label used to `select the output mask`. 
* collapsing it into a `vector representation` that `lacks spatial dimensions`.
* `RoIPool` is a standard operation for extracting a small feature map (e.g., 7×7) from each RoI
* non-maximum suppression
* The `mask branch` can predict K masks per RoI, but we only `use the k-th mask`, where k is the predicted class by the classification branch. The m×m floating-number mask output is then `resized to the RoI size`, and binarized at a threshold of 0.5

* 2016-FCIS.pdf
* 2016-Segment Proposal.pdf
* 2015-DeepMask.pdf

--------------------
- [x] [Mask RCNN笔记](https://blog.csdn.net/xiamentingtao/article/details/78598511)
* ROI Align 的反向传播
> 常规的`ROI Pooling`的反向传播公式如下：

![](http://1.file.leanote.top/59fbd202ab644135b00006fa/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20171103101817.png?e=1551535688&token=ym9ZIrtxjozPN4G9he3-FHPOPxAe-OQmxzol5EOk:HQGMn_aJNffPMlsmOIilYXI1jR8)

> 这里，xi代表池化前特征图上的像素点；yrj代表池化后的第r个候选区域的第j个点；i*(r,j)代表点yrj像素值的来源（最大池化的时候选出的最大像素值所在点的坐标）。由上式可以看出，只有当池化后某一个点的像素值在池化过程中采用了当前点Xi的像素值（即满足i=i*(r，j)），才在xi处回传梯度。
> 类比于ROIPooling，`ROIAlign的反向传播`需要作出稍许修改：首先，在ROIAlign中，xi*（r,j）是一个浮点数的坐标位置(前向传播时计算出来的采样点)，在池化前的特征图中，每一个与 xi*(r,j) 横纵坐标均小于1的点都应该接受与此对应的点yrj回传的梯度，故ROI Align 的反向传播公式如下: 

![](http://1.file.leanote.top/59fbe350ab644137db000a4e/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20171103113216.png?e=1551535689&token=ym9ZIrtxjozPN4G9he3-FHPOPxAe-OQmxzol5EOk:8JFZp9cEc2vf2099pKVB_jqA4rE)

> 上式中，d(.)表示两点之间的距离，Δh和Δw表示 xi 与 xi*(r,j) 横纵坐标的差值，这里作为双线性内插的系数乘在原始的梯度上。

----------------------------------------------------
# 2019.03.03
## SE-Net
- [x] **85-ImageNet冠军模型SE-Net详解**
* SE-Net: `channel relationship`; GoogleNet: `spatial relationship`
* Mini-batch data sampling: 按照`类别`，不是按照`图像`进行训练。
* SE-module流程：
1.  将输入特征进行`Global AVE pooling`，得到 `1*1* Channel` 
2. 然后`bottleneck`特征交互一下，先压缩 channel数，再重构回channel数最后接个 `sigmoid`，生成channel 间`0~1`的 attention weights，最后scale乘回原输入特征.

![](https://pic4.zhimg.com/80/v2-2e8c37ad7e40b7f1cdfd81ecbae4e95f_hd.jpg)

* ResNet & SE_Net
![](https://github.com/kinglintianxia/note_book/blob/master/imgs/SE_Net.png)

* GFLOPs

网络        		| GFLOPs
--------   		| -----
ResNet-50  		| 3.86
ResNet-101 		| 7.58
ResNet-152 		| 11.30
BN-Inception  	| 2.03

* ResNet-50 `Conv5层`没必要加`SE`模块


-----------------------
## Conv卷积，边缘检测
- [x] **吴恩达Conv卷积，边缘检测部分**
* 第一周 卷积神经网络-1.2/1.3节
* Why convolutions:
1. Parameter sharing.
2. Sparsity of connections.

### **第三周　目标检测**
* 跑道检测是`classification with localization`问题。
`yolov3` + `self-driving-car -> Project 4 - Advanced Lane Finding`
* The Network output is:<br>

|	output		|	meaning					| example 'pedestrian', 'car', 'motorcycle'	|	|	
|	------		|	------					|	------		|	------					|
|	Pc			|	Is there any object		|	1			|	0						|
|	bx			|	bbox center x			|	0.5			|	?	'Don't care'		|
|	by			|	bbox center y			|	0.5			|	?	'Don't care'		|
|	bh			|	bbox height				|	0.4			|	?	'Don't care'		|
|	bw			|	bbox width				|  	0.6			|	?	'Don't care'		|
|	c1			|	class 1 prob			|	0			|	?	'Don't care'		|
|	c2			|	class 2 prob			|	1			|	?	'Don't care'		|
|	c3			|	class 3 prob			|	0			|	?	'Don't care'		|



* The loss may be:

loss				|	case
------				|	------
sum(y^i-yi)^2		|	y0 = 1
(y^0-y0)^2			|	y0 = 0

* [bx, by, bh, bw] are parameted relative to the grid cell.


-----------------------
## YOLO
- [x] **跑通YOLO**
* [YOLO](https://pjreddie.com/darknet/yolo/)
* Darknet is an open source neural network framework written in `C and CUDA`. It is fast, easy to install, and supports `CPU and GPU computation`.

### Build & run
```shell
# Build with CPU
$ git clone https://github.com/pjreddie/darknet
$ cd darknet
$ make
$ wget https://pjreddie.com/media/files/yolov3.weights
# run with CPU
$ ./darknet detect cfg/yolov3.cfg weights/yolov3.weights data/dog.jpg

# Build with GPU
## change the first line of the `Makefile`
GPU=1		# build with CUDA to accelerate by using GPU (CUDA should be in /usr/local/cuda)
CUDNN=1		# build with cuDNN v5-v7 to accelerate training by using GPU ( /usr/local/cudnn)
OPENCV=1	# build with OpenCV 3.x/2.4.x - allows to detect on video files and video streams
OPENMP=1	# build with OpenMP support to accelerate Yolo by using multi-core CPU
DEBUG=0
ARCH= -gencode arch=compute_30,code=sm_30 \
      -gencode arch=compute_35,code=sm_35 \
      -gencode arch=compute_50,code=[sm_50,compute_50] \
      -gencode arch=compute_52,code=[sm_52,compute_52] \
      -gencode arch=compute_61,code=[sm_61,compute_61]

## run with GPU
$ ./darknet detector test cfg/coco.data cfg/yolov3.cfg weights/yolov3.weights data/dog.jpg 
$ ./darknet detect cfg/yolov3.cfg weights/yolov3.weights data/dog.jpg
## use GPU 1
$ ./darknet -i 1 detect cfg/yolov3.cfg weights/yolov3.weights data/dog.jpg
## run with video file
### FPS:18.2
$ ./darknet detector demo cfg/coco.data cfg/yolov3.cfg weights/yolov3.weights data/1.avi 
### FPS:180.9
$ ./darknet detector demo cfg/coco.data cfg/yolov3-tiny.cfg weights/yolov3-tiny.weights data/1.avi

## Train on VOC
### modify 'cfg/yolov3-voc.cfg', uncomment the 'Training' parameters.
# Testing
# batch=1
# subdivisions=1
# Training
 batch=64
 subdivisions=16
###########################################
# max_batches = 50200,迭代次数
$ ./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg  weights/darknet53.conv.74 -gpus 0,1
# Test training
## FPS:47.6
$ ./darknet detector -i 1 demo cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3-voc.backup data/1.avi



## Train on COCO
# max_batches = 500200,迭代次数
$ ./darknet detector train cfg/coco.data cfg/yolov3.cfg weights/darknet53.conv.74 -gpus 0,1
```

* Where `x, y, width, and height` are `relative to the image's width and height`.

----------------------------------------------
# 2019.03.03
## YOLOv3
- [x] [How to implement a YOLO (v3) object detector from scratch in PyTorch: Part 1](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)
* `YOLO`, `SSD`, `Mask RCNN` and `RetinaNet`.

### What is YOLO?
* YOLO makes use of only convolutional layers, making it a fully convolutional network (FCN). It has `75 convolutional layers`, with `skip connections` and `upsampling` layers. `No form of pooling` is used, and a convolutional layer with stride 2 is used to downsample the feature maps. 
* `Being a FCN`, YOLO is invariant to the size of the input image. However, `in practice`, we might want to stick to a `constant input size`:
> Process our images in `batches` (images in batches can be processed in parallel by the GPU, leading to speed boosts), we need to have all images of fixed height and width. 
* The network `downsamples` the image by a factor called the **stride** of the network.

### Interpreting the output
* Now, the first thing to notice is `our output is a feature map`.
* `Depth-wise`, we have `(B x (5 + C)) entries` in the feature map.
> `B` represents the number of bounding boxes each cell can predict.  Each of the bounding boxes have 5 + C attributes, which describe the `center coordinates`, the `dimensions`, the `objectness score` and `C class confidences` for each bounding box. YOLO v3 predicts `3 bounding boxes for every cell`.
* To do that, we `divide` the `input image` into `a grid of dimensions` equal to that of `the final feature map`.
> Let us consider an example below, where the input image is 416 x 416, and stride of the network is 32. As pointed earlier, the dimensions of the feature map will be 13 x 13. We then divide the input image into 13 x 13 cells.

![](https://blog.paperspace.com/content/images/2018/04/yolo-5.png)
> Then, the cell (on the input image) containing `the center of the ground truth box` of an object is chosen to be the one `responsible for predicting the object`. In the image, it is the cell which marked `red`, which contains the center of the ground truth box (marked yellow).
* We divide the `input image` into a `grid` just to determine which cell of the prediction `feature map` is `responsible for prediction`.

### Anchor Boxes
* It might make sense to predict the width and the height of the bounding box, but `in practice`, that leads to `unstable gradients` during training. `Instead`, most of the modern object detectors predict log-space transforms, or `simply` offsets to pre-defined default bounding boxes called `anchors`.
* The bounding box `responsible for detecting the dog` will be the one whose anchor has the `highest IoU` with the `ground truth box`.

### Making Predictions
* The following formulae describe how the network output is transformed to obtain bounding box predictions.

![](https://blog.paperspace.com/content/images/2018/04/Screen-Shot-2018-04-10-at-3.18.08-PM.png)

> `bx, by, bw, bh` are the x,y center cvoc persono-ordinates(坐标), width and height of our prediction. `tx, ty, tw, th` is what the `network outputs`. `cx and cy` are the top-left co-ordinates of the grid. `pw and ph` are anchors dimensions for the box.

### Center Coordinates
* Notice we are running our `center coordinates prediction` through a `sigmoid` function. This forces the value of the `output` to be between `0 and 1`. 
> For example, consider the case of our `dog image`. If the `prediction for center` is (0.4, 0.7), then this means that the center lies at (6.4, 6.7) on the 13 x 13 feature map. (Since the top-left co-ordinates of the red cell are (6,6)).

### Dimensions of the Bounding Box
* The `dimensions(bw, bh)` of the bounding box are predicted by applying a log-space transform to the `output(tw, th)` and then multiplying with an `anchor(pw, ph)`.
$$ {bw = pw*e^{tw}}, {bh = ph*e^{th}}  $$

![](https://blog.paperspace.com/content/images/2018/04/yolo-regression-1.png)

### Objectness Score
* `Object score` represents the `probability` that an object is contained inside a bounding box. It should be nearly 1 for the red and the neighboring grids, whereas almost 0 for, say, the grid at the corners.

* The `objectness score` is also passed through a `sigmoid`, as it is to be interpreted as a `probability`.

### Class Confidences
* Class confidences represent the probabilities of the detected object belonging to a particular class (Dog, cat, banana, car etc).
* Before v3, YOLO used to `softmax` the class scores.
* In v3, and authors have opted for using `sigmoid` instead.
* The reason is that `Softmaxing` class scores assume that the classes are `mutually exclusive`(相互排斥).
> In simple words, if an object belongs to one class, then it's guaranteed it cannot belong to another class. This is true for COCO database on which we will base our detector.

> `However`, this assumptions may not hold when we have classes like `Women` and `Person`. This is the reason that authors have steered clear of using a Softmax activation.

### Prediction across different scales.
* YOLO v3 makes prediction across `3 different scales`. The detection layer is used make detection at feature maps of three different sizes, having `strides 32, 16, 8 respectively`. This means, with an `input of 416 x 416`, we make detections on `scales 13 x 13, 26 x 26 and 52 x 52`.
* The network downsamples the input image until the first detection layer, where a detection is made using feature maps of a layer with strivoc personde 32. Further, layers are upsampled by a factor of 2 and concatenated with feature maps of a previous layers having identical feature map sizes. Another detection is now made at layer with stride 16. The same upsampling procedure is repeated, and a final detection is made at the layer of stride 8.

![](https://blog.paperspace.com/content/images/2018/04/yolo_Scales-1.png)

* `Upsampling` can help the network learn `fine-grained(细粒度) features` which are instrumental for detecting `small objects`.

### Output Processing
* For an image of size `416 x 416`, YOLO predicts ((52 x 52) + (26 x 26) + 13 x 13)) x 3 = `10647 bounding boxes`. 
* Thresholding by Object Confidence.
* Non-maximum Suppression.

---------------------------------------------
# 2019.03.05
## YOLOv3
- [x] [How to implement a YOLO (v3) object detector from scratch in PyTorch: Part 2](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-2/)
* Configuration File(cfg/文件下)详细解析。
```python
[net]
# Testing
batch=1
subdivisions=1
# Training
# batch=64
# subdivisions=16
```

- [x] [YOLOv3网络结构细致解析](https://blog.csdn.net/sum_nap/article/details/80568873)

### layer filters size input output

#### VOC dataset
```python
0 conv 32 3 x 3 / 1  416 x 416 x 3 -> 416 x 416 x 32  0.299 BFLOPs	
1 conv 64 3 x 3 / 2  416 x 416 x 32 -> 208 x 208 x 64  1.595 BFLOPs	
2 conv 32 1 x 1 / 1 208 x 208 x 64 -> 208 x 208 x 32 0.177 BFLOPs
3 conv 64 3 x 3 / 1 208 x 208 x 32 -> 208 x 208 x 64 1.595 BFLOPs
4 res 1 208 x 208 x 64 -> 208 x 208 x 64
5 conv 128 3 x 3 / 2 208 x 208 x 64 -> 104 x 104 x 128 1.595 BFLOPs
6 conv 64 1 x 1 / 1 104 x 104 x 128 -> 104 x 104 x 64 0.177 BFLOPs
7 conv 128 3 x 3 / 1 104 x 104 x 64 -> 104 x 104 x 128 1.595 BFLOPs
8 res 5 104 x 104 x 128 -> 104 x 104 x 128
9 conv 64 1 x 1 / 1 104 x 104 x 128 -> 104 x 104 x 64 0.177 BFLOPs
10 conv 128 3 x 3 / 1 104 x 104 x 64 -> 104 x 104 x 128 1.595 BFLOPs
11 res 8 104 x 104 x 128 -> 104 x 104 x 128
12 conv 256 3 x 3 / 2 104 x 104 x 128 -> 52 x 52 x 256 1.595 BFLOPs
13 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
14 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
15 res 12 52 x 52 x 256 -> 52 x 52 x 25voc person6
16 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
17 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
18 res 15 52 x 52 x 256 -> 52 x 52 x 256
19 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
20 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
21 res 18 52 x 52 x 256 -> 52 x 52 x 256
22 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
23 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
24 res 21 52 x 52 x 256 -> 52 x 52 x 256
25 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
26 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
27 res 24 52 x 52 x 256 -> 52 x 52 x 256
28 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
29 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
30 res 27 52 x 52 x 256 -> 52 x 52 x 256
31 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
32 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
33 res 30 52 x 52 x 256 -> 52 x 52 x 256
34 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
35 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
36 res 33 52 x 52 x 256 -> 52 x 52 x 256
37 conv 512 3 x 3 / 2 52 x 52 x 256 -> 26 x 26 x 512 1.595 BFLOPs
38 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
39 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
40 res 37 26 x 26 x 512 -> 26 x 26 x 512
41 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
42 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
43 res 40 26 x 26 x 512 -> 26 x 26 x 512
44 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
45 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
46 res 43 26 x 26 x 512 -> 26 x 26 x 512
47 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
48 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
49 res 46 26 x 26 x 512 -> 26 x 26 x 512
50 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
51 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
52 res 49 26 x 26 x 512 -> 26 x 26 x 512
53 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
54 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
55 res 52 26 x 26 x 512 -> 26 x 26 x 512
56 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
57 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
58 res 55 26 x 26 x 512 -> 26 x 26 x 512
59 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
60 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
61 res 58 26 x 26 x 512 -> 26 x 26 x 5voc person12
62 conv 1024 3 x 3 / 2 26 x 26 x 512 -> 13 x 13 x1024 1.595 BFLOPs
63 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
64 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
65 res 62 13 x 13 x1024 -> 13 x 13 x1024
66 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
67 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
68 res 65 13 x 13 x1024 -> 13 x 13 x1024
69 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
70 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
71 res 68 13 x 13 x1024 -> 13 x 13 x1024
72 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
73 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
74 res 71 13 x 13 x1024 -> 13 x 13 x1024
75 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
76 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
77 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
78 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
79 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
80 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
81 conv 75 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 75 0.026 BFLOPs
82 yolo
83 route 79
84 conv 256 1 x 1 / 1 13 x 13 x 512 -> 13 x 13 x 256 0.044 BFLOPs
85 upsample 2x 13 x 13 x 256 -> 26 x 26 x 256
86 route 85 61
87 conv 256 1 x 1 / 1 26 x 26 x 768 -> 26 x 26 x 256 0.266 BFLOPs
88 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
89 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
90 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
91 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
92 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
93 conv 75 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 75 0.052 BFLOPs
94 yolo
95 route 91
96 conv 128 1 x 1 / 1 26 x 26 x 256 -> 26 x 26 x 128 0.044 BFLOPs
97 upsample 2x 26 x 26 x 128 -> 52 x 52 x 128
98 route 97 36
99 conv 128 1 x 1 / 1 52 x 52 x 384 -> 52 x 52 x 128 0.266 BFLOPs
100 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
101 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
102 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
103 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
104 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
105 conv 75 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 75 0.104 BFLOPs
106 yolo
```

#### COCO dataset
```python
layer     filters    size              input                output
    0 conv     32  3 x 3 / 1   608 x 608 x   3   ->   608 x 608 x  32  0.639 BFLOPs
    1 conv     64  3 x 3 / 2   608 x 608 x  32   ->   304 x 304 x  64  3.407 BFLOPs
    2 conv     32  1 x 1 / 1   304voc person x 304 x  64   ->   304 x 304 x  32  0.379 BFLOPs
    3 conv     64  3 x 3 / 1   304 x 304 x  32   ->   304 x 304 x  64  3.407 BFLOPs
    4 res    1                 304 x 304 x  64   ->   304 x 304 x  64
    5 conv    128  3 x 3 / 2   304 x 304 x  64   ->   152 x 152 x 128  3.407 BFLOPs
    6 conv     64  1 x 1 / 1   152 x 152 x 128   ->   152 x 152 x  64  0.379 BFLOPs
    7 conv    128  3 x 3 / 1   152 x 152 x  64   ->   152 x 152 x 128  3.407 BFLOPs
    8 res    5                 152 x 152 x 128   ->   152 x 152 x 128
    9 conv     64  1 x 1 / 1   152 x 152 x 128   ->   152 x 152 x  64  0.379 BFLOPs
   10 conv    128  3 x 3 / 1   152 x 152 x  64   ->   152 x 152 x 128  3.407 BFLOPs
   11 res    8                 152 x 152 x 128   ->   152 x 152 x 128
   12 conv    256  3 x 3 / 2   152 x 152 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   13 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   14 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   15 res   12                  76 x  76 x 256   ->    76 x  76 x 256
   16 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   17 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   18 res   15                  76 x  76 x 256   ->    76 x  76 x 256
   19 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   20 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   21 res   18                  76 x  76 x 256   ->    76 x  76 x 256
   22 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   23 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   24 res   21                  76 x  76 x 256   ->    76 x  76 x 256
   25 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   26 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   27 res   24                  76 x  76 x 256   ->    76 x  76 x 256
   28 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   29 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   30 res   27                  76 x  76 x 256   ->    76 x  76 x 256
   31 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   32 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   33 res   30                  76 x  76 x 256   ->    76 x  76 x 256
   34 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   35 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   36 res   33                  76 x  76 x 256   ->    76 x  76 x 256
   37 conv    512  3 x 3 / 2    76 x  76 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   38 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   39 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   40 res   37                  38 x  38 x 512   ->    38 x  38 x 512
   41 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   42 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   43 res   40                  38 x  38 x 512   ->    38 x  38 x 512
   44 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   45 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   46 res   43                  38 x  38 x 512   ->    38 x  38 x 512
   47 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   48 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   49 res   46                  38 x  38 x 512   ->    38 x  38 x 512
   50 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   51 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   52 res   49                  38 x  38 x 512   ->    38 x  38 x 512
   53 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   54 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   55 res   52                  38 x  38 x 512   ->    38 x  38 x 512
   56 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   57 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   58 res   55                  38 x  38 x 512   ->    38 x  38 x 512
   59 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   60 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   61 res   58                  38 x  38 x 512   ->    38 x  38 x 512
   62 conv   1024  3 x 3 / 2    38 x  38 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   63 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   64 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   65 res   62                  19 x  19 x1024   ->    19 x  19 x1024
   66 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   67 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   68 res   65                  19 x  19 x1024   ->    19 x  19 x1024
   69 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   70 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   71 res   68                  19 x  19 x1024   ->    19 x  19 x1024
   72 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   73 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   74 res   71                  19 x  19 x1024   ->    19 x  19 x1024
   75 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   76 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   77 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   78 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   79 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   80 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   81 conv    255  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 255  0.189 BFLOPs
   82 yolo
   83 route  79
   84 conv    256  1 x 1 / 1    19 x  19 x 512   ->    19 x  19 x 256  0.095 BFLOPs
   85 upsample            2x    19 x  19 x 256   ->    38 x  38 x 256
   86 route  85 61
   87 conv    256  1 x 1 / 1    38 x  38 x 768   ->    38 x  38 x 256  0.568 BFLOPs
   88 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   89 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   90 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   91 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   92 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   93 conv    255  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 255  0.377 BFLOPs
   94 yolo
   95 route  91
   96 conv    128  1 x 1 / 1    38 x  38 x 256   ->    38 x  38 x 128  0.095 BFLOPs
   97 upsample            2x    38 x  38 x 128   ->    76 x  76 x 128
   98 route  97 36
   99 conv    128  1 x 1 / 1    76 x  76 x 384   ->    76 x  76 x 128  0.568 BFLOPs
  100 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
  101 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
  102 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
  103 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
  104 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
  105 conv    255  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 255  0.754 BFLOPs
  106 yolo
```


### 从75到105层我为yolo网络的特征交互层，分为`三个尺度`，每个尺度内，通过卷积核的方式实现局部的特征交互，作用类似于全连接层但是是通过卷积核（3x3和1x1）的方式实现feature map之间的局部特征（fc层实现的是全局的特征交互）交互。

1. 最小尺度yolo层：
> 输入：`13x13`的feature map(74 res) ，一共1024个通道。
> 操作：一系列的卷积操作，feature map的大小不变，但是通道数最后减少为75个。
> 输出；输出13*13大小的feature map，`75`个通道，在此基础上进行分类和位置回归。
> VOC dataset:[tx,ty,tw,th,Objectness Score(, Class Confidences(20)]x3 = [4,1,20]x3 = 25x3 = 75.

2. 中尺度yolo层：
> 输入：将`79层`的13*13,512通道的feature map进行卷积操作,生成13x13、256通道的feature map,然后进行上采样,生成26x26 256通道的feature map,同时于`61层`的26x26、512通道的中尺度的feature map合并。再进行一系列卷积操作，
> 操作：一系列的卷积操作，feature map的大小不变，但是通道数最后减少为75个。
> 输出：26x26大小的feature map，75个通道，然后在此进行分类和位置回归。

3. 大尺度的yolo层：
> 输入：将`91层`的26x26、256通道的feature map进行卷积操作，生成26x26、128通道的feature map，然后进行上采样生成52x52、128通道的feature map，同时于`36层`的52x52、256通道的中尺度的feature map合并。再进行一系列卷积操作，
> 操作：一系列的卷积操作，feature map的大小不变，但是通道数最后减少为75个。
> 输出：52x52大小的feature map，75个通道，然后在此进行分类和位置回归。
------------------------------------------ 

## YOLOv3
- [x] [yolo系列之yolo v3 深度解析](https://blog.csdn.net/leviopku/article/details/82660381)
* yolo_v3结构图(COCO dataset):

![](https://img-blog.csdn.net/2018100917221176?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xldmlvcGt1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
> `DBL`: 如图1左下角所示，也就是代码中的Darknetconv2d_BN_Leaky，是yolo_v3的基本组件。就是卷积+BN+Leaky relu。对于v3来说，BN和leaky relu已经是和卷积层不可分离的部分了(最后一层卷积除外)，共同构成了最小组件。
> `resn`：n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。这是yolo_v3的大组件，yolo_v3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深(从v2的darknet-19上升到v3的darknet-53，前者没有残差结构)。对于res_block的解释，可以在图1的右下角直观看到，其基本组件也是DBL。
> `concat`：张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。
> COCO dataset:[tx,ty,tw,th,Objectness Score(, Class Confidences(80)]*3 = [4,1,80]*3 = 85*3 = 255.

* [**模型结构可视化(vis)工具**](https://github.com/lutzroeder/Netron)
* [Netron browser version](https://lutzroeder.github.io/netron/)

* 轻量化网络: SqueezeNet.
* v3毫无疑问现在成为了工程界首选的检测算法之一了，结构清晰，实时性好。
* `疑问`:输出y1,y2,y3的预测是叠加一起成为最后的输出的吗?
2019.03.13
Ans: For an image of size `416 x 416`, YOLO predicts ((52 x 52) + (26 x 26) + 13 x 13)) x 3 = `10647 bounding boxes`. 
* loss function
```python
xy_loss = object_mask * box_loss_scale * K.`binary_crossentropy`(raw_true_xy, raw_pred[..., 0:2],
                                                                       from_logits=True)
wh_loss = object_mask * box_loss_scale * 0.5 * K.`square`(raw_true_wh - raw_pred[..., 2:4])
confidence_loss = object_mask * K.`binary_crossentropy`(object_mask, raw_pred[..., 4:5], from_logits=True) + \
                          (1 - object_mask) * K.binary_crossentropy(object_mask, raw_pred[..., 4:5],
                                                                    from_logits=True) * ignore_mask
class_loss = object_mask * K.`binary_crossentropy`(true_class_probs, raw_pred[..., 5:], from_logits=True)

xy_loss = K.sum(xy_loss) / mf
wh_loss = K.sum(wh_loss) / mf
confidence_loss = K.sum(confidence_loss) / mf
class_loss = K.sum(class_loss) / mf
`loss += xy_loss + wh_loss + confidence_loss + class_loss`

```
> 除了w, h的损失函数依然采用`总方误差`之外，其他部分的损失函数用的是`二值交叉熵`.

-------------------------
## YOLOv3
- [x] [基于keras-yolov3，原理及代码细节的理解](https://blog.csdn.net/KKKSQJ/article/details/83587138)
### anchor box:
* yolov3 anchor box一共有9个，由k-means聚类得到。在COCO数据集上，9个聚类是：（10x13）;（16x30）;（33x23）;（30x61）;（62x45）; （59x119）; （116x90）; （156x198）; （373x326）。
> 不同尺寸特征图对应不同大小的先验框。

    13*13feature map对应[（116*90），（156*198），（373*326）]
    26*26feature map对应[（30*61），（62*45），（59*119）]
    52*52feature map对应[（10*13），（16*30），（33*23）]

原因: 
	特征图越大，感受野越小。对小目标越敏感，所以选用小的anchor box。

    特征图越小，感受野越大。对大目标越敏感，所以选用大的anchor box。

### 边框预测：
1. 预测tx ty tw th
* 对tx和ty进行`sigmoid`，并加上对应的offset（Cx, Cy）.
* 对th和tw进行exp，并乘以对应的锚点值(pw,ph).
* 对tx,ty,th,tw乘以对应的步幅，即：416/13, 416/26, 416/52.
* 最后，使用sigmoid对Objectness和Classes confidence进行sigmoid得到0~1的概率，`之所以用sigmoid取代之前版本的softmax，原因是softmax会扩大最大类别概率值而抑制其他类别概率值`.

![边框预测](https://img2018.cnblogs.com/blog/1505200/201810/1505200-20181030204835020-902505029.png)
> (tx,ty) :目标中心点相对于该点所在网格左上角的偏移量，经过sigmoid归一化。即值属于[0,1]。如图约（0.3 , 0.4）<br>
> (cx,cy):该点所在网格的左上角距离最左上角相差的格子数。如图（1,1）<br>
> `(pw,ph):anchor box 的边长` <br>
> (tw,th):预测边框的宽和高, `maybe > 1`	<br>
> PS：最终得到的边框坐标值是bx,by,bw,bh.而网络学习目标是tx,ty,tw,th

--------------------
## YOLOv3
- [x] [YOLO从零开始：基于YOLOv3的行人检测入门指南](https://zhuanlan.zhihu.com/p/47196727)
### 通过`voc_label.py`转化`voc数据`格式为`yolo支持`的格式.
### 10、性能检测(`AlexeyAB/darknet`)
* 计算mAp
> ./darknet detector map cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3-voc_80172.weights
* 计算recall（2097张的结果）
> ./darknet detector recall cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3-voc_final.weights
* VOC2007test
```shell
# (会在/results生成默认的comp4_det_test_person.txt，这是在VOC2007 test上的结果)
$ ./darknet detector valid cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3-voc_final.weights -gpu 0,1
```

### VOC的图片格式
* 行列分布同pillow.Image，先行后列

### 训练过程
* 训练迭代数：`8w iters`
* [训练技巧](https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects)
* [**yolov3训练的集大成者**](https://blog.csdn.net/lilai619/article/details/79695109)
```python
Region xx: 		# cfg文件中yolo-layer的索引；

Avg IOU:  		# 当前迭代中，预测的box与标注的box的平均交并比，越大越好，期望数值为1；

Class:        	# 标注物体的分类准确率，越大越好，期望数值为1；

obj:            # 越大越好，期望数值为1；

No obj:      	# 越小越好；

.5R:            # 以IOU=0.5为阈值时候的recall; recall = 检出的正样本/实际的正样本

0.75R:         	# 以IOU=0.75为阈值时候的recall;

count:        	# 正样本数目。
```
![cfg](https://img-blog.csdn.net/20180430171058652)

### YOLOv3结构图(VOC dataset):

![网络模型](https://img-blog.csdn.net/20180608100212649)

### 模型什么时候保存？
* 迭代次数小于1000时，每100次保存一次，大于1000时，没10000次保存一次。

### 图片上添加置信值
* 代码比较熟悉的童鞋，使用opencv在画框的函数里面添加一下就行了。

---------------------------
## Paper Reading
- [x] [YOLOv3 全文翻译](https://zhuanlan.zhihu.com/p/34945787)

* `Darknet` is an open source neural network framework written in `C and CUDA`. It is fast, easy to install, and supports `CPU and GPU computation`. 

* 优点：速度快，精度提升，小目标检测有改善；
* 不足：中大目标有一定程度的削弱，遮挡漏检，速度稍慢于V2。

* 哥们论文写的太随意了.

-------------------------------
## YOLOv3 train voc only person
1. 通过`ubuntu/datasets/VOC/extract_person_2007/2012.py`提取含人数据,生成文件：
```shell
$ cd /media/jun/ubuntu/datasets/VOC/
$ python extract_person_2007.py
## 'ubuntu/datasets/VOC/VOCdevkit/VOC2007/ImageSets/Main/train_person.txt'
## 'ubuntu/datasets/VOC/VOCdevkit/VOC2007/ImageSets/Main/test_person.txt'
$ python extract_person_2012.py
## 'ubuntu/datasets/VOC/VOCdevkit/VOC2012/ImageSets/Main/train_person.txt'
```

2. 通过`voc_label_person`转化voc数据格式为yolo支持的格式
```shell
$ python voc_label_person.py
## 'ubuntu/datasets/VOC/2007_train_person.txt'
## 'ubuntu/datasets/VOC/2007_test_person.txt'
## 'ubuntu/datasets/VOC/2012_train_person.txt'
```
3. 整合下训练集、测试集：
```shell
$ cat 2007_train_person.txt 2012_train_person.txt > train_person.txt
```

4. 配置`data/voc_person.names`
```python
person
```

5. 配置`cfg/voc_person.data`
```python
classes= 1
train  = /media/jun/ubuntu/datasets/VOC/train_person.txt
valid  = /media/jun/ubuntu/datasets/VOC/2007_test_person.txt
names = data/voc_person.names
backup = backup_person
```

6. 配置`cfg/yolov3-voc-person.cfg`
```python
# 1. 一共三处
# filters=75
`filters=18`
activation=linear

[yolo]
mask = 6,7,8
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
`classes=1`

# 2. 迭代次数
max_batches = 80200

# 3. uncomment the 'Training' parameters.
# Testing
# batch=1
# subdivisions=1
# Training
 batch=64
 subdivisions=16
```

7. 训练
```shell
## Train
$ ./darknet detector train cfg/voc-person.data cfg/yolov3-voc-person.cfg weights/darknet53.conv.74 -gpus 0,1 |tee -a backup_person/train_voc.txt

## Restart training from a checkpoint:
$ ./darknet detector train cfg/voc-person.data cfg/yolov3-voc-person.cfg backup_person/yolov3-voc-person.backup -gpus 0,1

## Test NetWork
# Modify 'cfg/yolov3-voc-person.cfg'
# Testing
 batch=1
 subdivisions=1
$ ./darknet detector test cfg/voc-person.data cfg/yolov3-voc-person.cfg backup_person/yolov3-voc-person.backup data/person6.jpg
```
## 训练效果很好！！！
8. 用脚本`analyse.py`对`训练日志train7-loss.txt`的训练过程可视化(`AlexeyAB/darknet`)。



------------------------------------------------------
# 2019.03.06
## DANet
* [github](https://github.com/junfu1115/DANet)
* [CityScapes detailed-results](https://www.cityscapes-dataset.com/detailed-results/)
-------------------
- [x] [DANet&CCNet](https://segmentfault.com/a/1190000018271713)
* 两篇文章都是将self-attention机制应用到分割当中，扩大感受野。第二篇文章采用了更巧妙的方法来减少参数。
* `self-attention`在分割中应用的大致思想是：`特征图`与`特征图的转置`进行矩阵相乘，由于特征图有`channel维度`，相当于是每个像素与另外每个元素都进行`点乘操作`，而向量的点乘几何意义为计算`两个向量的相似度`，两个向量越相似，它们点乘越大。看下图，特征图转置与特征图矩阵相乘后用softmax进行归一化就得到了`Attention map S`。S再与特征图的转置进行矩阵相乘，这个操作`把相关性信息重新分布到原始特征图上`，最后再将这个信息与特征图A相加，得到最终输出，这个输出结合了`整张图的相关性结果`。

![](https://i.loli.net/2019/02/25/5c73485641635.png)

* 整个网络的框架如下图：非常简单，特征提取->attention module->上采样得到分割图

![](https://c2.staticflickr.com/8/7896/40180410623_4f9679fd0e_c.jpg)

> 除了上面说的那一部分attention，作者还加了`蓝色channel attention`，在这里计算特征图与特征图转置矩阵相乘操作时，相乘的顺序调换了一下，这相当于是让channel与channel之间进行点乘操作，计算channel之间的相似性，在这里我认为每张channel map代表了不同类别，这样让类别与类别计算距离，来进行辅助。
----------------------
## DANet
- [x] [几篇较新的计算机视觉Self-Attention](https://zhuanlan.zhihu.com/p/44031466?utm_source=wechat_session&utm_medium=social&utm_oi=963402370776072192&from=timeline&isappinstalled=0)
* 总的来说，就是区域权值学习问题：
1. `Hard-attention`，就是0/1问题，哪些区域是被 attentioned，哪些区域不关注.
2. `Soft-attention`，[0,1]间连续分布问题，每个区域被关注的程度高低，用0~1的score表示.
3. `Self-attention`自注意力，就是 feature map 间的自主学习，分配权重（可以是 spatial，可以是 temporal，也可以是 channel间）

### Non-local NN, CVPR2018
* `主要思想`也很简单，CNN中的 convolution单元每次只关注`邻域 kernel size 的区域`，就算后期感受野越来越大，终究还是局部区域的运算，这样就`忽略了全局`其他片区（比如很远的像素）对当前区域的贡献。
* 所以 `non-local blocks` 要做的是，`捕获这种 long-range关系`：对于`2D图像`，就是图像中任何像素对当前像素的关系权值；对于`3D视频`，就是所有帧中的所有像素，对当前帧的像素的关系权值。
* **网络框架图!!!**:

![](https://pic4.zhimg.com/80/v2-b7805f52179e0313c97b67984866a98f_hd.jpg)
* 在这里简单说说在DL框架中最好实现的 Matmul 方式：
1. 首先对输入的 feature map X 进行线性映射（说白了就是[1x1x1]卷积来压缩通道数),然后得到$\theta$,$\phi$, $g$ 特征
2. 通过reshape操作，强行合并上述的三个特征除通道数外的维度，然后对$\theta$和$\phi$进行矩阵点乘操作，得到类似协方差矩阵的东西（这个过程很重要，计算出特征中的自相关性，即得到每帧中每个像素对其他所有帧所有像素的关系）
3. 然后对自相关特征 以列or以行（具体看矩阵$g$的形式而定） 进行 Softmax 操作，得到0~1的weights，这里就是我们需要的 Self-attention 系数
4. 最后将 attention系数，对应乘回特征矩阵$g$中，然后`再上扩channel数`，与原输入feature map X `残差`一下，完整的 bottleneck.

### Interaction-aware Attention, ECCV2018
* 就是在 non-local block 的协方差矩阵基础上，设计了基于 PCA 的新loss，更好地进行特征交互。作者认为，这个过程，特征会在channel维度进行更好的 non-local interact，故称为 Interaction-aware attention.
* 文中不直接使用`协方差矩阵的特征值分解`来实现, 通过`PCA`来获得 `Attention weights`
* 有点`小区别`是，在 X 和 Watten 点乘后，`还加了个b项`，文中说这里可看作 data central processing (subtracting mean) of PCA.

### CBAM: Convolutional Block Attention Module, ECCV2018
* 基于 `SE-Net`中的 Squeeze-and-Excitation module 来进行进一步拓展
* 文中把 `channel-wise attention` 看成是教网络 `Look 'what’`；而`spatial attention` 看成是教网络 `Look 'where'`，所以它比 SE Module 的主要优势就多了后者.
* 先看看SE-module流程：
1. 将输入特征进行`Global AVE pooling`，得到 `[1x1xChannel]` 
2. 然后`bottleneck`特征交互一下，`先压缩channel数`，`再重构回channel数`最后接个`sigmoid`，生成channel间`0~1`的 attention weights，最后scale乘回原输入特征.

![](https://pic4.zhimg.com/80/v2-2e8c37ad7e40b7f1cdfd81ecbae4e95f_hd.jpg)

* 再看看CBAM：
1. `Channel Attention Module`: 基本和 SE-module 是一致的，就额外加入了 Maxpool 的 branch。在 Sigmoid 前，两个 branch 进行 element-wise summation 融合。
2. `Spatial Attention Module`: 对输入特征进行 channel 间的 AVE 和 Max pooling，然后 concatenation，再来个7*7大卷积，最后`Sigmoid`.

![](https://pic1.zhimg.com/80/v2-a5ada5fb9ee0355b44e6a78f81ac1c58_hd.jpg)

### DANet, CVPR2019
* 很早就挂在了arXiv，`最近被CVPR2019接收`，把`Self-attention`的思想用在图像分割，可通过`long-range上下文关系`更好地做到精准分割。
* 把deep feature map进行`spatial-wise self-attention`，同时也进行`channel-wise self-attetnion`，最后将两个结果进行`element-wise sum`融合。
* 好处是：借鉴CBAM`分别进行空间和通道`self-attention的思想上，直接使用了 non-local 的自相关矩阵Matmul的形式进行运算，避免了CBAM手工设计pooling，多层感知器等复杂操作。

----------------------
## DANet
- [x] [DANet PPT](https://blog.csdn.net/mieleizhi0522/article/details/83111183) 
* SOTA(State of the art).
* 位置注意力模块(spatial-wise self-attention)通过所有位置的特征加权总和选择的性的聚集每个位置的特征，无论距离远近，相似的特征都会相互关联。(**类似于全连接条件随机场CRF**)
* 通道注意力模块(channel-wise self-attetnion)通过整合所有通道中的相关特征，有选择的性的强调相关联的通道。
* 基础网络为**DeepLab**
* **PSPNet** & **DeepLabV3**
* 位置注意力模块:

![](https://img-blog.csdn.net/20181017154326441?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pZWxlaXpoaTA1MjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

* 通道注意力模块:

![](https://img-blog.csdn.net/20181017154336438?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pZWxlaXpoaTA1MjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

* Position Attention Module:

![](https://img-blog.csdn.net/20181017154404699?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pZWxlaXpoaTA1MjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

* Channel Attention Module:

![](https://img-blog.csdn.net/20181017154426685?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pZWxlaXpoaTA1MjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

* DANet 网络整体结构

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/DANet.png)
> 我们的注意力模块很简单，可以直接插入现有的FCN模块中，不会增加太多参数，但会有效的增强特征表示。

* Dataset简介：

![](https://img-blog.csdn.net/20181017154447811?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pZWxlaXpoaTA1MjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

* 训练参数和评价指标：

![](https://img-blog.csdn.net/20181017154457283?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pZWxlaXpoaTA1MjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

* Visualization of Attention Module:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/DANet_Vis.png)

![](https://img-blog.csdn.net/20181017154534719?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pZWxlaXpoaTA1MjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

* 对于通道注意力模块来说，很难直接给出关于注意力图的可视化理解:

![](https://img-blog.csdn.net/2018101715462287?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pZWxlaXpoaTA1MjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

* MultiGrid:

![](https://img-blog.csdn.net/20181017154717952?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pZWxlaXpoaTA1MjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


------------------------------------------------------
# 2019.03.07
see [19-deeplabv3+.md](https://github.com/kinglintianxia/note_book/blob/master/19-deeplabv3%2B.md)


------------------------------------------------------
# 2019.03.13
## PSPNet

- [x] **Paper Reading**
### PSPNet网络结构

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/PSPNet.png)

* Our `pyramid pooling module` is a four-level one with bin sizes of `1×1, 2×2, 3×3 and 6×6` respectively. 
* `Average pooling` works `better` than max pooling in all settings.
* we use `1×1 convolution` layer after `each pyramid level` to reduce the dimension of context representation to `1/N` of the original one if the level size of `pyramid is N`. 
* we use the `"poly"` learning rate policy,  Momentum and weight decay are set to `0.9` and `0.0001` respectively.
* we set the `“batchsize” to 16` during training.
* [PSPNet-Pyramid Scene Parsing Network-金字塔场景解析网络](https://blog.csdn.net/jiang1943/article/details/83544326)

### Implementation Details:
|	参数 	|		设置	 		|
|	------	|		------		|
|平台 		|		Caffe		|
|学习率 		|采用“poly”策略，即lr=lrbase∗(1−itermaxiter)power,其中lrbase=0.01,power=0.9 momentum=0.9,weightdecay=0.0001	|
|迭代次数 	|ImageNet上设置150K，PASCAL VOC上设置30K，Cityscapes设置90K|
|数据增强 	|随机镜像翻转，尺寸在0.5-2之间缩放，角度在-10°和10°之间随机旋转、随机的高斯模糊|
|batchsize 	|	16				|
|辅助loss的权重|	weight=0.4		|

### Results on `Cityscapes testing set`:

| Method	|	IoU class	|
| ------	|	------		|
| DeepLabv2	|	70.4		|
| DeepLabv3 |	81.3		|
| DeepLabv3+|	82.1		|
| PSPNet	|	81.2		|




### CityScapes dataset
Cityscapes is a recently released dataset for semantic urban scene understanding. It contains `5,000 high quality pixel-level finely annotated images` collected from 50 cities in different seasons. <br>
The images are divided into sets with numbers `2,975`, `500`, and `1,525` for `training`, `validation` and `testing`. It defines `19 categories` containing both stuff and objects. Also, `20,000 coarsely annotated` images are provided for two settings in comparison.


----------------------------------------------
# 2019.03.14
## MobileNetV2
- [x] **Paper Reading**
### MobileNetV2
* Based on an `inverted residual` structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. 
* Additionally, we find that it is important to `remove non-linearities in the narrow layers` in order to maintain representational power.

### Semantic Segmentation
* we compare `MobileNetV1` and `MobileNetV2` models used as feature extractors with `DeepLabv3`.
* we experimented with three design variations: 
1. different feature extractors, 
2. simplifying the DeepLabv3 heads for faster computation,
3. different inference strategies for boosting the performance.

* We have observed that (Table 7): 
1. the inference strategies, including multi-scale inputs and adding leftright flipped images, significantly increase the MAdds and thus are not suitable for on-device applications,
2. using `output stride = 16` is more efficient than `output stride = 8`
3.  `MobileNetV1` is already a `powerful feature extractor` and only requires about 4.9−5.7 times fewer `MAdds`(number of multiply-adds operators) than `ResNet-101` (e.g., mIOU:78.56 vs 82.70, and MAdds: 941.9B vs 4870.6B)
4. It is more efficient to build DeepLabv3 heads on top of `the second last feature map` of `MobileNetV2` than on the original last-layer feature map, since the second to last feature map contains 320 channels instead of 1280
5. `DeepLabv3` heads are computationally expensive and `removing the ASPP module` significantly reduces the MAdds with only a slight performance degradation. 

* Notably, our architecture combined with the SSDLite detection module is 20× less computation and 10× less parameters than YOLOv2.



# 2019.03.14
## AlexeyAB/darknet

- [ ] **README.md**

* `YOLOv3-spp` better than `YOLOv3` - mAP = 60.6%, FPS = 20

* also create SO-library on Linux and DLL-library on Windows

* improved binary neural network performance 2x-4x times for Detection on CPU and GPU if you trained your own weights by using this XNOR-net model (bit-1 inference)

* improved neural network `performance ~7%` by fusing 2 layers into 1: `Convolutional + Batch-norm`

* added correct calculation of `mAP, F1, IoU, Precision-Recall` using command:
$ ./darknet detector map...

* added drawing of chart of average-Loss and accuracy-mAP (-map flag) during training

* To calculate anchors: 
$ ./darknet detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416

## How to compile on Linux
* `CUDNN_HALF=1` to build for Tensor Cores (on Titan V / Tesla V100 / DGX-2 and later) speedup Detection 3x, Training 2x

* LIBSO=1 to build a library darknet.so and binary runable file uselib that uses this library. 

* How to use this `SO-library` from your own code, you can look at [C++ example](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp) 

## How to train (to detect your custom objects):
* Only for `small datasets` sometimes better to `decrease learning rate`, for 4 GPUs set learning_rate = 0.00025 (i.e. learning_rate = 0.001 / GPUs). In this case also increase 4x times `burn_in` = and `max_batches` = in your cfg-file. I.e. use burn_in = 4000 instead of 1000.

* `Note:`
If during training you see `nan` values for `avg (loss)` field - then training goes wrong, but if `nan` is in some other lines - then training goes well.

* `Note:`
If you changed width= or height= in your cfg-file, then new width and height must be divisible by 32.

* cfg:
```python
# batch size
batch=64
# output stride	
subdivisions=8
# training steps
max_batches = 500200
```

## How to train tiny-yolo (to detect your custom objects):

* Yolo based on other models:
[DenseNet201-Yolo](https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/densenet201_yolo.cfg)
[ResNet50-Yolo](https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/resnet50_yolo.cfg)

## When should I stop training:

* Usually sufficient 2000 iterations for each class(object), but not less than 4000 iterations in total. 

* The `final avgerage loss` can be from 0.05 (for a small model and easy dataset) to 3.0 (for a big model and a difficult dataset).

* Or just train with -map flag:
$ ./darknet detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -map


## How to calculate mAP on PascalVOC 2007:

## How to improve object detection:
* set flag `random=1` in your `\*.cfg` file.

* Increase network resolution in your .cfg-file (`height=608, width=608` or any value `multiple of 32`.

* You should preferably have `2000` different images for each class or more, and you should train 2000*classes iterations or more

* desirable that your training dataset `include images with non-labeled objects` that you do not want to detect

* for training with a large number of objects in each image...
* for training for small objects...
* for training for `both small and large objects` use modified models:
* to speedup training (with decreasing detection accuracy) do `Fine-Tuning(微调)` instead of `Transfer-Learning(迁移学习)`, set param `stopbackward=1`.
* Increase network-resolution by set in your .cfg-file (height=608 and width=608) or (height=832 and width=832) or (any value multiple of 32) - this increases the precision and makes it possible to detect small objects:

## How to use Yolo as DLL and SO libraries
* on Linux - set `LIBSO=1` in the `Makefile` and do `make`.
* There are 2 APIs:
1. [C API](https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h)
2. [C++ API](https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp)
<br>
[C++ example that uses C++ API](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp)


----------------------------------------------
# 2019.03.15
- [x] **Check CityScapes tfrecord is correct?**
* tfrecord correct.


- [x] **Train MobileNet on VOC dataset**
* mobilenet_v2, `re-use all the trained weights except the logits` [OK]
global step 8890: loss = 0.3322 (0.205 sec/step), miou_1.0[0.706052244] <br>
global step 30000: loss = 0.2057 (0.196 sec/step), miou_1.0[0.727050483]

- [x] **Show KittiSeg seg map**
* demo.py L184 
```python
# output: [batch*height*width, NUM_CLASSES].
output = sess.run([softmax], feed_dict=feed)    # Get probility image. <br>
"""
    output: 
     [array([[9.9968946e-01, 3.1052253e-04],
           [9.9980527e-01, 1.9472565e-04],
           [9.9978584e-01, 2.1418222e-04],
           ...,
           [9.9948078e-01, 5.1923015e-04],
           [9.9927455e-01, 7.2546635e-04],
           [9.9853718e-01, 1.4628501e-03]], dtype=float32)]
"""
# road 
    output_image = output[0][:, 1].reshape(shape[0], shape[1])
	out_bg = output[0][:, 0].reshape(shape[0], shape[1])
```
* road

![road](https://github.com/kinglintianxia/note_book/blob/master/imgs/demo_raw_road.png)

* background

![background](https://github.com/kinglintianxia/note_book/blob/master/imgs/demo_raw_bg.png)



# ==TODO==

- [ ] **Pooling -> Conv**
- [ ] **输入Image手工添加其他特征channels，比如Canny,Gray,hsv, self-attention等**
- [ ] **KittiBox 添加其他class**
- [ ] **MultiNet基础上修改，完成道路分割、车辆、行人等检测(或+分割)**
- [ ] **在FCN Decoder 中加入多个seg loss (ROI Align生成feature map),训练网络。**
- [ ] **YOLOv3添加Segmap**
- [ ] **+ DANet**
- [ ] **labelme my own dataset**







----------------------------------------------------------------------------------
- [ ] [UNIX Tutorial for Beginners](http://www.ee.surrey.ac.uk/Teaching/Unix/)
- [ ] [完全解析RNN, Seq2Seq, Attention注意力机制](https://zhuanlan.zhihu.com/p/51383402)
.
