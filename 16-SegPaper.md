# 1. 修改deeplab，简化语义标签，提高实时性。

# 2. 在MultiNet基础上修改，完成道路分割、车辆、行人等检测。

# 3. 打破FCN Encoder-Decoder架构,设计one step end-to-end网络。预测语义分割每一类物体在图片上的像素轮廓，而不是端到端输出图片。

# 4. 将[pooling层]换成[边缘检测层]试一下效果; 能不能让Pooling层在降size的同时，层内参数可训练化（让网络自己选择保留哪些信息，而不仅仅是MaxPooling或AVGPooling）

# 5. 注意力模型

# 6. 现在训练网络都是让网络“猜dog在哪里？在这里”(前向传播), "不对，你离正确答案多远"（计算loss）；“我再猜一下”（反向传播）；能不能设计一种训练结构，告诉网络“这张图中dog在哪里”。

# 7. 在分割网络中加入ROI-Pooling层
* Input -> ROI Pooling -> FCN

# 8. 在分割网络中加入边缘检测层，结合边缘对score层进行采样分类，当一个轮廓中大于阈值的像素点属于A类，则划为A类。

# 9. 在激活函数Relu等上做文章，让网络逐渐收敛到真值（比如正负二分类），不用softmax和argmax等。

# 10. 在feature map中手工加入边缘检测层， 让网络自己学习把边缘信息加入分割判断中。

# 11. 将卷积操作的 'bias'值换成相应区域的边缘图像试试。

# 12. [COS_Net.jpg]

#　13. 分割网络输出层21channels，每个channels表达一个类别的mask， 将分类和分割解耦。


## 使用ResNet实现的分割网络效果state-of-art. 

------------------
# 2019.01.08
- [x] **跑通KittiSeg evaluate.py程序**	
- [x] **整理KittiSeg和MultiNet代码**	

# 2019.01.09(评价指标)
- [x] **阅读MaxF1论文(THE KITTI-ROAD DATASET)**
* [blog1](https://blog.csdn.net/sinat_28576553/article/details/80258619)
* ego-lane vs.opposing lane,(自我车道与对方车道)
* 2D Bird’s Eye View (BEV) space.
* For methods that output confidence maps (in contrast to binary road classification), the classification threshold τ is chosen to maximize the F-measure.

- [x] **IOU**
* See KittiSeg code.

# 2019.01.13
- [x] **评价MultNet训练结果**
```python
segmentation Evaluation Finished. Results
Raw Results:
[train] MaxF1 (raw)    :  98.9767 
[train] BestThresh (raw)    :  68.2353 
[train] Average Precision (raw)    :  92.5437 
[val] MaxF1 (raw)    :  95.9975 
[val] BestThresh (raw)    :  14.9020 
[val] Average Precision (raw)    :  92.3125 
Speed (msec) (raw)    :  42.8566 
Speed (fps) (raw)    :  23.3336
```

- [x] **读AP指标（VOC DATASET）**
* [AP wiki](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision)
* [blog1](https://blog.csdn.net/niaolianjiulin/article/details/53098437)
* [blog2](https://blog.csdn.net/hysteric314/article/details/54093734)
* 以recall为横坐标（0~1),precision为纵坐标（0~1）作图。得到一条曲线。该曲线下的面积即为AP.
$$ AP = \int_0^1 {p(r)} dr $$ 

# 2019.01.14
- [x] **标注Cityscape数据集**
* [blog1](https://blog.csdn.net/fabulousli/article/details/78633531)
* [labelme 工具](https://github.com/wkentaro/labelme) 

# 2019.01.15
- [x] **发现好博客**
* [深度学习数据集介绍及相互转换](https://www.cnblogs.com/ranjiewen/p/9860953.html)

- [x] **labelme 工具生成Kitti Road数据集**
* img.putpalette([0,0,0,0,255,0,0,0,255])
	
- [x] **labelme 工具生成CityScapes数据集：**
* from cityscapesscripts.helpers.labels     import name2label 

- [ ] **cityscapesScripts标注自己图片**

# 2019.01.16
- [x] **[Semantic Segmentation using Fully Convolutional Networks over the years]**
* [link](https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html)
* [vis of FCN](http://ethereon.github.io/netscope/#/preset/fcn-8s-pascal)
* [vis of common networks](http://ethereon.github.io/netscope/quickstart.html)
* [Deconvolutions](https://distill.pub/2016/deconv-checkerboard/)
* LinkNet: A feature map with shape [H, W, n_channels] is first convolved with a 1*1 kernel to get a feature map with shape [H, W, n_channels / 4 ] and then a deconvolution takes it to [2*H, 2*W, n_channels / 4 ] a final 1*1 kernel convolution to take it to [2*H, 2*W, n_channels / 2 ]. Thus the decoder block fewer parameters due to this channel reduction scheme.

# 2019.01.20
- [x] **[Deconvolutions]**
* [link](https://distill.pub/2016/deconv-checkerboard/)
* When we look very closely at `images generated by neural networks`, we often see a strange `checkerboard pattern` of artifacts.
* `In theory`, our models could learn to carefully write to unevenly overlapping positions so that the output is evenly balanced; `In fact`, not only do models with uneven overlap not learn to avoid this, but models with even overlap often learn kernels that cause similar artifacts! 
* Better Upsampling
> `One approach` is to make sure you use `a kernel size that is divided by your stride`, avoiding the overlap issue.
> Another approach is `resize the image (using nearest-neighbor interpolation or bilinear interpolation) and then do a convolutional layer`.
* We don’t necessarily think that either approach is the final solution to upsampling, but they do fix the checkerboard artifacts.
* Whenever we `compute the gradients of a convolutional layer`, we do `deconvolution` (transposed convolution) on the `backward pass`.

- [x] **[2017-course_by_Tao Kong.pdf]**
* The region proposal network is a FCN which outputs `K*(4+2) sized vectors`.
* `Mask R-CNN` = Faster R-CNN with FCN on ROIs.
* Useful links for learning detection:
> RCNN/Fast R-CNN/Faster R-CNN: https://github.com/rbgirshick
> YOLO/YOLOv2: https://pjreddie.com/darknet/yolo/
> SSD: https://github.com/weiliu89/caffe/tree/ssd
> R-FCN: https://github.com/daijifeng001/R-FCN
> Tensorflow detector:https://github.com/tensorflow/models/tree/master/research/object_detection

# 2019.01.21
- [x] **基于深度卷积神经网络的目标检测算法研究_黄莉芝.caj**
* 注意力模型
* 语义分割分为“阈值分割”，“边缘分割”， “区域分割”
* 计算边框回归

# 2019.01.22
- [x] **看第四章“目标定位优化”** 

- [x] **专知[目标检测专栏]**
* [link](https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247485072&idx=2&sn=e9f4f0d1daeb3a144e37fcfcc61e908f&chksm=fc85e783cbf26e95d153af7c825ef9b8fc4fc6fd37f75709de98eae769e6b70f8da29ae65a73&scene=21#wechat_redirect)
* [CVPR'17 Tutorial](http://deeplearning.csail.mit.edu/)
* [图像目标检测（Object Detection）原理与实现 （1-6）](https://blog.csdn.net/marvin521/article/details/9058735)
* 下载了[基于特征共享的高效物体检测_任少卿.pdf]
* 下载了[Bounding-box_regression详解.pdf]

- [x] **[RCNN, Fast-RCNN, Faster-RCNN的一些事]**
* [link](http://closure11.com/rcnn-fast-rcnn-faster-rcnn%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/)
* 在Fast-RCNN中，作者巧妙的把bbox regression放进了神经网络内部，与region分类和并成为了一个multi-task模型
* Bounding-box Regression
* 我的CVPR 2016论文里自己弄的一个数据集就借助了Fast-RCNN.（`可以利用现有分割网络进行数据集粗略分割`）

- [x] **[目标检测——从RCNN到Faster RCNN 串烧]**
* [link](https://blog.csdn.net/xyy19920105/article/details/50817725)

# 2019.01.24 计划
- [x] **[机器学习随笔]**
* [link](https://zhuanlan.zhihu.com/p/35058212)

- [x] **[图像语义分割+FCN/U-Net]**
* [link](https://zhuanlan.zhihu.com/p/31428783)
* [visualization of FCN](http://ethereon.github.io/netscope/#/preset/fcn-8s-pascal)
* 对于FCN-8s，首先进行pool4+2x upsampled feature`逐点相加`，然后又进行pool3+2x upsampled`逐点相加`，即进行更多次特征融合。
* 为了解决图像过小后 1/32 下采样后输出feature map太小情况，FCN原作者在第一个卷积层`conv1_1加入pad=100`。
* 在特征融合的时候，如何保证逐点相加的feature map是一样大的呢？这就要引入crop层了
* 在caffe中，存储数据的方式为：caffe blob = [num, channel, height, width]
1. 而score_pool4c设置了axis=2，相当于从第2维(index start from 0!)往后开始裁剪，即裁剪height和width两个维度，同时不改变num和channel纬度
2. 同时设置crop在height和width纬度的开始点为offset=5
3. 用Python语法表示，相当于score_pool4c层的输出为：
```python
crop-pool4 = score-pool4[:, :, 5:5+score2.height, 5:5+score2.width]
```
* `Deconvelution`计算：`score-fr`[1,21,16,16] -> `score2` [1,21,34,34] , kernel_size:4, stride:2
> Conv: $$ out = (in+2*pad-kernel)/stride + 1 $$
> DeConv: $$ out = (in-1)*stride + kernel -2*pad $$
> And Now: $$ 34 = (16-1)*2+4-2*0 $$

# 2019.01.25 
- [x] **[FCN学习:Semantic Segmentation]**
* [link](https://zhuanlan.zhihu.com/p/22976342)
* Faster-RCNN中使用了RPN(Region Proposal Network)替代Selective Search等产生候选区域的方法。`RPN是一种全卷积网络`，所以为了透彻理解这个网络，首先学习一下FCN
* [Caffe 中如何计算卷积](https://www.zhihu.com/question/28385679)
1. Input feature to Matrix
![Input feature to Matrix](https://pic1.zhimg.com/80/v2-701705db7504bb24f859122545b23174_hd.png)
2. Filters to matrix
![Filters to matrix](https://pic1.zhimg.com/80/v2-339657291663a4e791a9b34952d5859c_hd.png)
3. `Filter Matrix`乘以`Feature Matrix`的转置,得到输出矩阵`Cout x (H x W)`,就可以解释为输出的三维Blob`(Cout x H x W)`
4. Detail sample.
![Caffe conv](https://pic4.zhimg.com/80/a6421bae22236c0509623b8b7f7bbb03_hd.jpg)
5. 多.通道卷积计算方式
![多通道卷积](https://pic1.zhimg.com/80/v2-8d72777321cbf1336b79d839b6c7f9fc_hd.jpg)
* 矩阵微分公式:
$$ \frac {d(Ax+b)}{dx} = {A^T}$$

# 2019.01.26
- [x] **[一文读懂Faster RCNN]**
* [link](https://zhuanlan.zhihu.com/p/31426458)
* Faster R-CNN网络结构图:
![Faster R-CNN](https://pic4.zhimg.com/80/v2-e64a99b38f411c337f538eb5f093bdf3_hd.jpg)
* bounding box regression原理
* 对于一副任意大小PxQ图像,传入Faster RCNN前首先reshape到固定MxN,im_info=[M, N, scale_factor]则保存了此次缩放的所有信息,然后经过Conv Layers,经过4次pooling变为WxH=(M/16)x(N/16)大小,其中feature_stride=16则保存了该信息,用于计算anchor偏移量.
------
## update 2019.02.27
* VGG Feature Map: "conv5_3".
* 9个anchors(矩形)共有3种形状，长宽比为大约为{width:height} = {1:1, 1:2, 2:1} 三种(ps. 每种比例有三个尺度)，如图所示:
![anchors](https://pic4.zhimg.com/80/v2-7abead97efcc46a3ee5b030a2151643f_hd.jpg)
* 在caffe基本数据结构blob中以如下形式保存数据：
```python
blob = [batch, channel, height, width] = [N, C, H, W]
```
* bounding box regression原理
> 窗口一般使用四维向量 (x, y, w, h) 表示，分别表示窗口的`中心点`坐标、`宽`和`高`.
> 先做平移，再做缩放。
* 对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 (t_x, t_y, t_w, t_h)，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。
那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 (t_x, t_y, t_w, t_h)，显然即可用来修正Anchor位置了。
* 解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，`im_info`=[M, N, scale_factor]则保存了此次缩放的所有信息。
* 经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中`feature_stride=16`则保存了该信息，用于计算anchor偏移量。
* RPN网络处理流程：
> 生成anchors -> softmax分类器提取fg anchors -> bbox reg回归fg anchors -> Proposal Layer生成proposal boxes=[x1, y1, x2, y2]
* See [COS_Net.jpg].
* 从PoI Pooling获取到`7x7=49`大小的proposal feature maps.
* 在训练和检测阶段生成和存储anchors的顺序完全一样，这样训练结果才能被用于检测！

# 2019.01.27
- [x] **GDB调试工具**
* [RMS's gdb Debugger Tutorial](http://www.unknownroad.com/rtfm/gdbtut/)
```shell
$ gcc -g inf.c			# Compile the program with debugging flags.
$ gdb a.out				# Lets load up gdb.
$ run & ctrl+c			# Set off the infinite loop, then press `Ctrl-C` to send the program a SIGINT. 
$ backtrace				# We will use the `backtrace` command to examine the stack
$ frame 1
```

- [ ] **gdb 调试入门**
* [gdb 调试入门](http://blog.jobbole.com/107759/)

## Qt学习
- [ ] **Qt入门学习——Qt Creator的使用**
* [Qt入门学习——Qt Creator的使用](https://blog.csdn.net/tennysonsky/article/details/48004119)

- [ ] **Qt 学习之路 2**
* [Qt 学习之路 2](https://www.devbean.net/category/qt-study-road-2/)

# 2019.01.28
- [x] **服务器安装Anaconda环境**
* [blog](https://blog.csdn.net/qq_17534301/article/details/80869998)
* [Installing on Linux](http://docs.anaconda.com/anaconda/install/linux/)
* [Anaconda installer archive](https://repo.anaconda.com/archive/)

# 2019.01.29
- [x] **安装Caffe**
* see update of `03-Install_caffe.md`

- [x] **跑FCN**
* git clone https://github.com/shelhamer/fcn.berkeleyvision.org.git
* Download voc-fcn8s caffe model.
```shell
$ cd fcn.berkeleyvision.org/voc-fcn8s/
$ proxychains wget http://dl.caffe.berkeleyvision.org/fcn8s-heavy-pascal.caffemodel
```
* run
```shell
$ conda activate py27	# for `added by Anaconda3 5.3.0 installer` in '~/.bashrc'
or
$ source activate py27	# for `export PATH="/home/jun/anaconda3/bin:$PATH"` in '~/.bashrc'
$ python infer.py
```

* vis of caffe net.
[NetScope](http://ethereon.github.io/netscope/#/editor)

## sublime 高亮当前行
* [blog](https://yijile.com/log/128.html)


# 2019.02.13
- [x] **跑通Faster R-CNN**
* [github](https://github.com/smallcorgi/Faster-RCNN_TF)

## Install dependeces.
```shell
# change to python 2.7
$ sudo pip install easydict
```
## Build.
```shell
$ cd Faster-RCNN_TF/lib
$ make
```

## Run demo.py
```shell
$ python ./tools/demo.py --model ./data/model/VGGnet_fast_rcnn_iter_70000.ckpt
```
### Run Errors.
* ERROR 1
> tensorflow.python.framework.errors_impl.NotFoundError: /home/jun/Documents/Faster-RCNN_TF/tools/../lib/roi_pooling_layer/roi_pooling.so: undefined symbol: _ZTIN10tensorflow8OpKernelE

```shell
TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')
# Define `TF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')` and modify the `g++` call to include `-L$TF_LIB -ltensorflow_framework`
TF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')

CUDA_PATH=/usr/local/cuda/
CXXFLAGS=''

if [[ "$OSTYPE" =~ ^darwin ]]; then
	CXXFLAGS+='-undefined dynamic_lookup'
fi

cd roi_pooling_layer

if [ -d "$CUDA_PATH" ]; then
	nvcc -std=c++11 -c -o roi_pooling_op.cu.o roi_pooling_op_gpu.cu.cc \
		-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC $CXXFLAGS \
# sm_61 for 1080Ti
		-arch=sm_61
# add `-L$TF_LIB -ltensorflow_framework` & `-D_GLIBCXX_USE_CXX11_ABI=`
	g++ -std=c++11 -shared -o roi_pooling.so roi_pooling_op.cc \
		roi_pooling_op.cu.o -I $TF_INC  -D GOOGLE_CUDA=1 -fPIC $CXXFLAGS -D_GLIBCXX_USE_CXX11_ABI=0 \
		-lcudart -L $CUDA_PATH/lib64 -L $TF_LIB -ltensorflow_framework
else
	g++ -std=c++11 -shared -o roi_pooling.so roi_pooling_op.cc \
		-I $TF_INC -fPIC $CXXFLAGS
fi

cd ..
```
* ERROR 2
> feed_in, dim = (input, int(input_shape[-1])) TypeError: __int__ returned non-int (type NoneType)
```shell
# https://github.com/smallcorgi/Faster-RCNN_TF/issues/316
# add the following lines to lib/roi_pooling_layer/roi_pooling_op.cc, and make
L27 #include "tensorflow/core/framework/shape_inference.h"
L41-L53
.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      //https://github.com/tensorflow/.../core/framework/shape_inference.h
      int pooled_height;
      int pooled_width;
      c->GetAttr("pooled_height", &pooled_height);
      c->GetAttr("pooled_width", &pooled_width);
      auto pooled_height_h = c->MakeDim(pooled_height);
      auto pooled_width_h = c->MakeDim(pooled_width);

      auto output_shape = c->MakeShape({ c->Dim(c->input(1), 0), pooled_height_h, pooled_width_h, c->Dim(c->input(0), 3) });
      c->set_output(0, output_shape);
      return Status::OK();
    });
```

# 2019.02.13
- [x] **Object Detection and Classification using R-CNNs**
* [link](http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/)
* Network Architecture
![network](http://www.telesens.co/wp-content/uploads/2018/03/img_5a9ffec911c19.png)

# 2019.02.16
* seg featuremap 在voc数据集上输出有为[1, 21, 500, 500],经过np.argmax(axis=0)得到[500, 500]的segmentation image of class IDs，相当于在channel方向上对每个像素进行分类，取channel方向上概率最大值作为该像素类别。

# 2019.02.21 - 2019.02.27
- [x] **MultiNet源码**
* See `11-MultiNet.md`

# 2019.02.27
- [x] **复习 [一文读懂Faster RCNN]**
* See `update 2019.02.27`.
* [Faster-rcnn详解](https://blog.csdn.net/WZZ18191171661/article/details/79439212)


# 2019.02.28
## CSDN翻译专栏
[CSDN翻译专栏](https://blog.csdn.net/quincuntial/article/details/77263607)

# 2019.02.29 - 2019.03.03
- [ ] **Mask-RCNN**
* 专知语义分割专栏
```python
Mask-RCNN [https://arxiv.org/pdf/1703.06870.pdf]

https://github.com/CharlesShang/FastMaskRCNN [Tensorflow]

https://github.com/TuSimple/mx-maskrcnn [MxNet]

https://github.com/matterport/Mask_RCNN [Keras]

https://github.com/jasjeetIM/Mask-RCNN [Caffe]
```

# 2019.03.01
## Mask R-CNN
- [x] [Mask R-CNN详解](https://blog.csdn.net/WZZ18191171661/article/details/79453780)

- [x] **Mask_R-CNN.mp4**

- [x] **maskrcnn_slides.pdf**
* [region-of-interest-pooling-explained](https://deepsense.ai/region-of-interest-pooling-explained/)
* Backbone (`ResNeXt`): +1:6APbb

## `RoIPooling` & `RoIAlign`
### `RoIPooling`
1. Let’s consider a small example to see how it works. We’re going to perform region of interest pooling on a single 8×8 feature map, one region of interest and an output size of 2×2. Our input feature map looks like this:

![Feature Map](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/1.jpg)

2. Let’s say we also have a region proposal (top left, bottom right coordinates): (0, 3), (7, 8). In the picture it would look like this:

![region proposal](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/2.jpg)

3. Normally, there’d be multiple feature maps and multiple proposals for each of them, but we’re keeping things simple for the example.
By dividing it into (2×2) sections (because the output size is 2×2) we get:

![2×2 sections](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/3.jpg)

4. The max values in each of the sections are:

![output](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/output.jpg)

5. Here’s our example presented in form of a nice animation:

![gif](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/roi_pooling-1.gif)


### `RoIAlign`
1. unquantized (2×2) sections:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi0.png)

2. Sampling locations:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi1.png)

3. Bilinear interpolated values:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi2.png)

4. Max pooling output:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi3.png)


# 2019.03.02
## Mask R-CNN

###
- [x] **Mask-RCNN-Arc-How-RoI_Pooling-RoI_Warping_RoI-Align_Work.mp4**
* ROI Pooling和ROIAlign最大的区别是：前者使用了两次量化操作，而后者并没有采用量化操作，使用了线性插值算法，具体的解释如下所示:
![ROIPooling](https://img-blog.csdn.net/20180306110240257)
![ROIAlign](https://img-blog.csdn.net/20180306110334767)

###
- [x] **2017-Mask R-CNN.pdf**
* [Mask R-CNN 论文翻译](https://alvinzhu.xyz/2017/10/07/mask-r-cnn/#fn:18)
* [Mask R-CNN完整翻译](https://blog.csdn.net/myGFZ/article/details/79136610)
* `RoIPool` performs coarse spatial quantizationfor feature extraction. To fix the misalignment, we propose a simple, quantization-free layer, called `RoIAlign`, that faithfully preserves exact spatial locations.
*  Second, we found it essential to decouple mask and class prediction: we predict a binary mask for each class independently, without competition among classes, and rely on the network’s RoI classification branch to predict the category. 
* Our models can run at about 200ms per frame on a GPU, and training on COCO takes one to two days on a single 8-GPU machine. 
* Instead, our method is based on `parallel prediction of masks and class labels`, which is simpler and more flexible.
* The `mask branch` has a Km2-dimensional output for `each RoI`, which encodes K binary masks of resolution m × m, one for each of the `K classes`.
* For an RoI associated with ground-truth class k, Lmask is only defined on the k-th mask (other mask outputs do not contribute to the loss).
* we rely on the dedicated classification branch to predict the class label used to `select the output mask`. 
* collapsing it into a `vector representation` that `lacks spatial dimensions`.
* `RoIPool` is a standard operation for extracting a small feature map (e.g., 7×7) from each RoI
* non-maximum suppression
* The `mask branch` can predict K masks per RoI, but we only `use the k-th mask`, where k is the predicted class by the classification branch. The m×m floating-number mask output is then `resized to the RoI size`, and binarized at a threshold of 0.5

* 2016-FCIS.pdf
* 2016-Segment Proposal.pdf
* 2015-DeepMask.pdf

###
- [x] [Mask RCNN笔记](https://blog.csdn.net/xiamentingtao/article/details/78598511)
* ROI Align 的反向传播
> 常规的`ROI Pooling`的反向传播公式如下：

![](http://1.file.leanote.top/59fbd202ab644135b00006fa/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20171103101817.png?e=1551535688&token=ym9ZIrtxjozPN4G9he3-FHPOPxAe-OQmxzol5EOk:HQGMn_aJNffPMlsmOIilYXI1jR8)
> 这里，xi代表池化前特征图上的像素点；yrj代表池化后的第r个候选区域的第j个点；i*(r,j)代表点yrj像素值的来源（最大池化的时候选出的最大像素值所在点的坐标）。由上式可以看出，只有当池化后某一个点的像素值在池化过程中采用了当前点Xi的像素值（即满足i=i*(r，j)），才在xi处回传梯度。
> 类比于ROIPooling，`ROIAlign的反向传播`需要作出稍许修改：首先，在ROIAlign中，xi*（r,j）是一个浮点数的坐标位置(前向传播时计算出来的采样点)，在池化前的特征图中，每一个与 xi*(r,j) 横纵坐标均小于1的点都应该接受与此对应的点yrj回传的梯度，故ROI Align 的反向传播公式如下: 
![](http://1.file.leanote.top/59fbe350ab644137db000a4e/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20171103113216.png?e=1551535689&token=ym9ZIrtxjozPN4G9he3-FHPOPxAe-OQmxzol5EOk:8JFZp9cEc2vf2099pKVB_jqA4rE)
> 上式中，d(.)表示两点之间的距离，Δh和Δw表示 xi 与 xi*(r,j) 横纵坐标的差值，这里作为双线性内插的系数乘在原始的梯度上。

# 2019.03.03
## SE-Net
- [x] **85-ImageNet冠军模型SE-Net详解**
* SE-Net: `channel relationship`; GoogleNet: `spatial relationship`
* Mini-batch data sampling: 按照`类别`，不是按照`图像`进行训练。
* ResNet & SE_Net
![](https://github.com/kinglintianxia/note_book/blob/master/imgs/SE_Net.png)

* GFLOPs
网络        		| GFLOPs
--------   		| -----
ResNet-50  		| 3.86
ResNet-101 		| 7.58
ResNet-152 		| 11.30
BN-Inception  	| 2.03

* ResNet-50 Conv5层没必要加SE模块


# ==TODO==


- [ ] **吴恩达Conv卷积，边缘检测部分**


- [ ] **YOLO**


- [ ] **DANet**



# 2019.02.29 - 2019.03.03
- [ ] **DeepLab V3+论文代码**

- [ ] [UNIX Tutorial for Beginners](http://www.ee.surrey.ac.uk/Teaching/Unix/)
- [ ] [完全解析RNN, Seq2Seq, Attention注意力机制](https://zhuanlan.zhihu.com/p/51383402)
.
