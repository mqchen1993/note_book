# 1. 修改deeplab，简化语义标签，提高实时性。

# 2. 在MultiNet基础上修改，完成道路分割、车辆、行人等检测。

# 3. 打破FCN Encoder-Decoder架构,设计one step end-to-end网络。预测语义分割每一类物体在图片上的像素轮廓，而不是端到端输出图片。

# 4. 将[pooling层]换成[边缘检测层]试一下效果; 能不能让Pooling层在降size的同时，层内参数可训练化（让网络自己选择保留哪些信息，而不仅仅是MaxPooling或AVGPooling）

# 5. 注意力模型

# 6. 现在训练网络都是让网络“猜dog在哪里？在这里”(前向传播), "不对，你离正确答案多远"（计算loss）；“我再猜一下”（反向传播）；能不能设计一种训练结构，告诉网络“这张图中dog在哪里”。

# 7. 在分割网络中加入ROI-Pooling层
* Input -> ROI Pooling -> FCN

# 8. 在分割网络中加入边缘检测层，结合边缘对score层进行采样分类，当一个轮廓中大于阈值的像素点属于A类，则划为A类。

# 9. 在激活函数Relu等上做文章，让网络逐渐收敛到真值（比如正负二分类），不用softmax和argmax等。

# 10. 在feature map中手工加入边缘检测层， 让网络自己学习把边缘信息加入分割判断中。

# 11. 将卷积操作的 'bias'值换成相应区域的边缘图像试试。

# 12. COS_Net.jpg

#　13. 分割网络输出层21channels，每个channels表达一个类别的mask， 将分类和分割解耦。

# 14. yolo检测道路主要类别+语义分割道路，实时跑在北三环数据集上。

# 15. `DANet` + `DeepLabV3+` + `# 13.` + `Pooling -> Conv`

# 16. 在FCN Decoder 中加入多个seg loss (ROI Align生成feature map),训练网络。
The `auxiliary loss` helps `optimize the learning process`, while the master branch loss takes the most responsibility. We add weight to balance the auxiliary loss <br>
In the `testing phase`, we abandon this auxiliary branch and only use the well optimized master branch for final prediction.

# 17. 分割网络输入Image手工添加其他特征channels，比如Canny,Gray,hsv, self-attention等。可以增加显式特征表示。

# 18. 在seg map 中使用`per-pixel sigmoid` and a `binary loss`
Mask-RCNN: This is different from common practice when applying FCNs to `semantic segmentation`, which typically uses a `per-pixel softmax` and a `multinomial cross-entropy loss`. In that case, masks `across classes compete(竞争)`; `in our case`, with a `per-pixel sigmoid` and a `binary loss`, they do not. We show by experiments that this formulation `is key for good` instance segmentation results.

# 19. handle `checkerboard pattern`

## 使用ResNet实现的分割网络效果state-of-the-art(SOTA). 

------------------
# 2019.01.08
- [x] **跑通KittiSeg evaluate.py程序**	
- [x] **整理KittiSeg和MultiNet代码**	

# 2019.01.09(评价指标)
- [x] **阅读MaxF1论文(THE KITTI-ROAD DATASET)**
* [blog1](https://blog.csdn.net/sinat_28576553/article/details/80258619)
* ego-lane vs.opposing lane,(自我车道与对方车道)
* 2D Bird’s Eye View (BEV) space.
* For methods that output confidence maps (in contrast to binary road classification), the classification threshold τ is chosen to maximize the F-measure.

- [x] **IOU**
* See KittiSeg code.

# 2019.01.13
- [x] **评价MultNet训练结果**
```python
segmentation Evaluation Finished. Results
Raw Results:
[train] MaxF1 (raw)    :  98.9767 
[train] BestThresh (raw)    :  68.2353 
[train] Average Precision (raw)    :  92.5437 
[val] MaxF1 (raw)    :  95.9975 
[val] BestThresh (raw)    :  14.9020 
[val] Average Precision (raw)    :  92.3125 
Speed (msec) (raw)    :  42.8566 
Speed (fps) (raw)    :  23.3336
```

- [x] **读AP指标（VOC DATASET）**
* [AP wiki](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision)
* [blog1](https://blog.csdn.net/niaolianjiulin/article/details/53098437)
* [blog2](https://blog.csdn.net/hysteric314/article/details/54093734)
* 以recall为横坐标(0,1),precision为纵坐标(0,1)作图。得到一条曲线。该曲线下的面积即为AP.

$$ AP = \int_0^1 {p(r)} dr $$ 

# 2019.01.14
- [x] **标注Cityscape数据集**
* [blog1](https://blog.csdn.net/fabulousli/article/details/78633531)
* [labelme 工具](https://github.com/wkentaro/labelme) 

# 2019.01.15
- [x] **发现好博客**
* [深度学习数据集介绍及相互转换](https://www.cnblogs.com/ranjiewen/p/9860953.html)

- [x] **labelme 工具生成Kitti Road数据集**
* img.putpalette([0,0,0,0,255,0,0,0,255])
	
- [x] **labelme 工具生成CityScapes数据集：**
* from cityscapesscripts.helpers.labels     import name2label 

- [ ] **cityscapesScripts标注自己图片**

# 2019.01.16
- [x] **[Semantic Segmentation using Fully Convolutional Networks over the years]**
* [link](https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html)
* [vis of FCN](http://ethereon.github.io/netscope/#/preset/fcn-8s-pascal)
* [vis of common networks](http://ethereon.github.io/netscope/quickstart.html)
* [Deconvolutions](https://distill.pub/2016/deconv-checkerboard/)
* LinkNet: A feature map with shape [H, W, n_channels] is first convolved with a [1x1] kernel to get a feature map with shape [H, W, n_channels / 4 ] and then a deconvolution takes it to [2*H, 2*W, n_channels / 4 ] a final [1x1] kernel convolution to take it to [2*H, 2*W, n_channels / 2 ]. Thus the decoder block fewer parameters due to this channel reduction scheme.

--------------------
# 2019.01.20
- [x] **[Deconvolutions]**
* [link](https://distill.pub/2016/deconv-checkerboard/)
* When we look very closely at `images generated by neural networks`, we often see a strange `checkerboard pattern` of artifacts.
* `In theory`, our models could learn to carefully write to unevenly overlapping positions so that the output is evenly balanced; `In fact`, not only do models with uneven overlap not learn to avoid this, but models with even overlap often learn kernels that cause similar artifacts! 
* `Better Upsampling`
> 1. `One approach` is to make sure you use `a kernel size that is divided by your stride`, avoiding the overlap issue.
> 2. Another approach is to `separate out` `upsampling` to a higher resolution from `convolution` to compute features. For example: `resize the image (using [nearest-neighbor interpolation](https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation) or [bilinear interpolation](https://en.wikipedia.org/wiki/Bilinear_interpolation)) and then do a convolutional layer`.

![](https://distill.pub/2016/deconv-checkerboard/assets/upsample_DeconvTypes.svg)

* We’ve had our `best results` with `nearest-neighbor interpolation`, and had `difficulty making bilinear resize work`.
* We don’t necessarily think that either approach is the final solution to upsampling, but they do fix the checkerboard artifacts.
* Whenever we `compute the gradients of a convolutional layer`, we do `deconvolution` (transposed convolution) on the `backward pass`.

--------------------
- [x] **[2017-course_by_Tao Kong.pdf]**
* The region proposal network is a FCN which outputs `K*(4+2) sized vectors`.
* `Mask R-CNN` = Faster R-CNN with FCN on ROIs.
* Useful links for learning detection:
[RCNN/Fast R-CNN/Faster R-CNN](https://github.com/rbgirshick)
[YOLO/YOLOv2](https://pjreddie.com/darknet/yolo/)
[SSD](https://github.com/weiliu89/caffe/tree/ssd)
[R-FCN](https://github.com/daijifeng001/R-FCN)
[Tensorflow detector](https://github.com/tensorflow/models/tree/master/research/object_detection)

--------------------
# 2019.01.21
- [x] **基于深度卷积神经网络的目标检测算法研究_黄莉芝.caj**
* 注意力模型
* 语义分割分为“阈值分割”，“边缘分割”， “区域分割”
* 计算边框回归

--------------------
# 2019.01.22
- [x] **看第四章“目标定位优化”** 

--------------------
- [x] **专知[目标检测专栏]**
* [link](https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247485072&idx=2&sn=e9f4f0d1daeb3a144e37fcfcc61e908f&chksm=fc85e783cbf26e95d153af7c825ef9b8fc4fc6fd37f75709de98eae769e6b70f8da29ae65a73&scene=21#wechat_redirect)
* [CVPR'17 Tutorial](http://deeplearning.csail.mit.edu/)
* [图像目标检测（Object Detection）原理与实现 （1-6）](https://blog.csdn.net/marvin521/article/details/9058735)
* 下载了[基于特征共享的高效物体检测_任少卿.pdf]
Done Reading !
* 下载了[Bounding-box_regression详解.pdf]

--------------------
- [x] **[RCNN, Fast-RCNN, Faster-RCNN的一些事]**
* [link](http://closure11.com/rcnn-fast-rcnn-faster-rcnn%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/)
* 在Fast-RCNN中，作者巧妙的把bbox regression放进了神经网络内部，与region分类和并成为了一个multi-task模型
* Bounding-box Regression
* 我的CVPR 2016论文里自己弄的一个数据集就借助了Fast-RCNN.（`可以利用现有分割网络进行数据集粗略分割`）

--------------------
- [x] **[目标检测——从RCNN到Faster RCNN 串烧]**
* [link](https://blog.csdn.net/xyy19920105/article/details/50817725)

--------------------
# 2019.01.24 计划
- [x] **[机器学习随笔]**
* [link](https://zhuanlan.zhihu.com/p/35058212)

--------------------
- [x] **[图像语义分割+FCN/U-Net]**
* [link](https://zhuanlan.zhihu.com/p/31428783)
* [visualization of FCN](http://ethereon.github.io/netscope/#/preset/fcn-8s-pascal)
* 对于FCN-8s，首先进行pool4+2x upsampled feature`逐点相加`，然后又进行pool3+2x upsampled`逐点相加`，即进行更多次特征融合。
* 为了解决图像过小后 1/32 下采样后输出feature map太小情况，FCN原作者在第一个卷积层`conv1_1加入pad=100`。
* 在特征融合的时候，如何保证逐点相加的feature map是一样大的呢？这就要引入crop层了
* 在caffe中，存储数据的方式为：caffe blob = [num, channel, height, width]
1. 而score_pool4c设置了axis=2，相当于从第2维(index start from 0!)往后开始裁剪，即裁剪height和width两个维度，同时不改变num和channel纬度
2. 同时设置crop在height和width纬度的开始点为offset=5
3. 用Python语法表示，相当于score_pool4c层的输出为：
```python
crop-pool4 = score-pool4[:, :, 5:5+score2.height, 5:5+score2.width]
```
* `Deconvelution`计算：`score-fr`[1,21,16,16] -> `score2` [1,21,34,34] , kernel_size:4, stride:2
Conv: $out = (in+2*pad-kernel)/stride + 1$
DeConv: $out = (in-1)*stride + kernel -2*pad$
And Now: $34 = (16-1)*2+4-2*0$

-------------------
# 2019.01.25 
- [x] **[FCN学习:Semantic Segmentation]**
* [link](https://zhuanlan.zhihu.com/p/22976342)
* Faster-RCNN中使用了RPN(Region Proposal Network)替代Selective Search等产生候选区域的方法。`RPN是一种全卷积网络`，所以为了透彻理解这个网络，首先学习一下FCN
* [Caffe 中如何计算卷积](https://www.zhihu.com/question/28385679)
1. Input feature to Matrix

![Input feature to Matrix](https://pic3.zhimg.com/80/69e44f61e8ede5ba84534ca3b764d302_hd.jpg)

2. Filters to matrix

![Filters to matrix](https://pic1.zhimg.com/80/v2-339657291663a4e791a9b34952d5859c_hd.png)

3. `Filter Matrix`乘以`Feature Matrix`的转置,得到输出矩阵`Cout x (H x W)`,就可以解释为输出的三维Blob`(Cout x H x W)`

![](https://pic3.zhimg.com/80/6b1dde11bf30688b4f526ea77d54a196_hd.jpg)

4. Detail sample.

![Caffe conv](https://pic4.zhimg.com/80/a6421bae22236c0509623b8b7f7bbb03_hd.jpg)

5. 多.通道卷积计算方式

![多通道卷积](https://pic1.zhimg.com/80/v2-8d72777321cbf1336b79d839b6c7f9fc_hd.jpg)

* 矩阵微分公式:
$$\frac {d(Ax+b)}{dx} = {A^T}$$

--------------------
# 2019.01.26
- [x] **[一文读懂Faster RCNN]**
* [link](https://zhuanlan.zhihu.com/p/31426458)
* Faster R-CNN网络结构图:
![Faster R-CNN](https://pic4.zhimg.com/80/v2-e64a99b38f411c337f538eb5f093bdf3_hd.jpg)
* bounding box regression原理
* 对于一副任意大小PxQ图像,传入Faster RCNN前首先reshape到固定MxN,im_info=[M, N, scale_factor]则保存了此次缩放的所有信息,然后经过Conv Layers,经过4次pooling变为WxH=(M/16)x(N/16)大小,其中feature_stride=16则保存了该信息,用于计算anchor偏移量.
--------
## update 2019.02.27
* VGG Feature Map: "conv5_3".
* 9个anchors(矩形)共有3种形状，长宽比为大约为{width:height} = {1:1, 1:2, 2:1} 三种(ps. 每种比例有三个尺度)，如图所示:
![anchors](https://pic4.zhimg.com/80/v2-7abead97efcc46a3ee5b030a2151643f_hd.jpg)
* 在caffe基本数据结构blob中以如下形式保存数据：
```python
blob = [batch, channel, height, width] = [N, C, H, W]
```
* bounding box regression原理
> 窗口一般使用四维向量 (x, y, w, h) 表示，分别表示窗口的`中心点`坐标、`宽`和`高`.
> 先做平移，再做缩放。
* 对于训练bouding box regression网络回归分支，输入是`cnn feature Φ`，监督信号是Anchor与GT的差距`(t_x, t_y, t_w, t_h)`，即`训练目标`是：输入 Φ的情况下使网络输出与监督信号尽可能接近。
那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 (t_x, t_y, t_w, t_h)，显然即可用来修正Anchor位置了。
* 解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，`im_info`=[M, N, scale_factor]则保存了此次缩放的所有信息。
* 经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中`feature_stride=16`则保存了该信息，用于计算anchor偏移量。
* RPN网络处理流程：
> 生成anchors -> softmax分类器提取fg anchors -> bbox reg回归fg anchors -> Proposal Layer生成proposal boxes=[x1, y1, x2, y2]
* See [COS_Net.jpg].
* 从PoI Pooling获取到`7x7=49`大小的proposal feature maps.
* 在训练和检测阶段生成和存储anchors的顺序完全一样，这样训练结果才能被用于检测！

# 2019.01.27
- [x] **GDB调试工具**
* [RMS's gdb Debugger Tutorial](http://www.unknownroad.com/rtfm/gdbtut/)
```shell
$ gcc -g inf.c			# Compile the program with debugging flags.
$ gdb a.out				# Lets load up gdb.
$ run & ctrl+c			# Set off the infinite loop, then press `Ctrl-C` to send the program a SIGINT. 
$ backtrace				# We will use the `backtrace` command to examine the stack
$ frame 1
```

- [ ] **gdb 调试入门**
* [gdb 调试入门](http://blog.jobbole.com/107759/)

## Qt学习
- [ ] **Qt入门学习——Qt Creator的使用**
* [Qt入门学习——Qt Creator的使用](https://blog.csdn.net/tennysonsky/article/details/48004119)

- [ ] **Qt 学习之路 2**
* [Qt 学习之路 2](https://www.devbean.net/category/qt-study-road-2/)

# 2019.01.28
- [x] **服务器安装Anaconda环境**
* see `18-Anaconda.md`
* [blog](https://blog.csdn.net/qq_17534301/article/details/80869998)
* [Installing on Linux](http://docs.anaconda.com/anaconda/install/linux/)
* [Anaconda installer archive](https://repo.anaconda.com/archive/)

# 2019.01.29
- [x] **安装Caffe**
* see update of `03-Install_caffe.md`

- [x] **跑FCN**
* git clone https://github.com/shelhamer/fcn.berkeleyvision.org.git
* Download voc-fcn8s caffe model.
```shell
$ cd fcn.berkeleyvision.org/voc-fcn8s/
$ proxychains wget http://dl.caffe.berkeleyvision.org/fcn8s-heavy-pascal.caffemodel
```
* run
```shell
$ conda activate py27	# for `added by Anaconda3 5.3.0 installer` in '~/.bashrc'
or
$ source activate py27	# for `export PATH="/home/jun/anaconda3/bin:$PATH"` in '~/.bashrc'
$ python infer.py
```

* vis of caffe net.
[NetScope](http://ethereon.github.io/netscope/#/editor)

## sublime 高亮当前行
* [blog](https://yijile.com/log/128.html)


# 2019.02.13
- [x] **跑通Faster R-CNN**
* [github](https://github.com/smallcorgi/Faster-RCNN_TF)

## Install dependeces.
```shell
# change to python 2.7
$ sudo pip install easydict
```
## Build.
```shell
$ cd Faster-RCNN_TF/lib
$ make
```

## Run demo.py
```shell
$ python ./tools/demo.py --model ./data/model/VGGnet_fast_rcnn_iter_70000.ckpt
```
### Run Errors.
* ERROR 1
> tensorflow.python.framework.errors_impl.NotFoundError: /home/jun/Documents/Faster-RCNN_TF/tools/../lib/roi_pooling_layer/roi_pooling.so: undefined symbol: _ZTIN10tensorflow8OpKernelE

```shell
TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')
# Define `TF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')` and modify the `g++` call to include `-L$TF_LIB -ltensorflow_framework`
TF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')

CUDA_PATH=/usr/local/cuda/
CXXFLAGS=''

if [[ "$OSTYPE" =~ ^darwin ]]; then
	CXXFLAGS+='-undefined dynamic_lookup'
fi

cd roi_pooling_layer

if [ -d "$CUDA_PATH" ]; then
	nvcc -std=c++11 -c -o roi_pooling_op.cu.o roi_pooling_op_gpu.cu.cc \
		-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC $CXXFLAGS \
# sm_61 for 1080Ti
		-arch=sm_61
# add `-L$TF_LIB -ltensorflow_framework` & `-D_GLIBCXX_USE_CXX11_ABI=`
	g++ -std=c++11 -shared -o roi_pooling.so roi_pooling_op.cc \
		roi_pooling_op.cu.o -I $TF_INC  -D GOOGLE_CUDA=1 -fPIC $CXXFLAGS -D_GLIBCXX_USE_CXX11_ABI=0 \
		-lcudart -L $CUDA_PATH/lib64 -L $TF_LIB -ltensorflow_framework
else
	g++ -std=c++11 -shared -o roi_pooling.so roi_pooling_op.cc \
		-I $TF_INC -fPIC $CXXFLAGS
fi

cd ..
```
* ERROR 2
> feed_in, dim = (input, int(input_shape[-1])) TypeError: __int__ returned non-int (type NoneType)
```shell
# https://github.com/smallcorgi/Faster-RCNN_TF/issues/316
# add the following lines to lib/roi_pooling_layer/roi_pooling_op.cc, and make
L27 #include "tensorflow/core/framework/shape_inference.h"
L41-L53
.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      //https://github.com/tensorflow/.../core/framework/shape_inference.h
      int pooled_height;
      int pooled_width;
      c->GetAttr("pooled_height", &pooled_height);
      c->GetAttr("pooled_width", &pooled_width);
      auto pooled_height_h = c->MakeDim(pooled_height);
      auto pooled_width_h = c->MakeDim(pooled_width);

      auto output_shape = c->MakeShape({ c->Dim(c->input(1), 0), pooled_height_h, pooled_width_h, c->Dim(c->input(0), 3) });
      c->set_output(0, output_shape);
      return Status::OK();
    });
```

# 2019.02.13
- [x] **Object Detection and Classification using R-CNNs**
* [link](http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/)
* Faster R-CNN Network Architecture

![network](http://www.telesens.co/wp-content/uploads/2018/03/img_5a9ffec911c19.png)

question: 
`bbx_pred_net` shape=[21*4]?



# 2019.02.16
* `seg featuremap` 在voc数据集上`输出`为[1, 21, 500, 500],经过np.argmax(axis=0)得到[500, 500]的segmentation image of class IDs，相当于在channel方向上对每个像素进行分类，取channel方向上概率最大值作为该像素类别。

# 2019.02.21 - 2019.02.27
- [x] **MultiNet源码**
* See [11-MultiNet.md](https://github.com/kinglintianxia/note_book/blob/master/11-MultiNet.md)

# 2019.02.27
- [x] **复习 [一文读懂Faster RCNN]**
* See `update 2019.02.27`.
* [Faster-rcnn详解](https://blog.csdn.net/WZZ18191171661/article/details/79439212)


# 2019.02.28
## CSDN翻译专栏
[CSDN翻译专栏](https://blog.csdn.net/quincuntial/article/details/77263607)

# 2019.02.29 - 2019.03.03
## Mask R-CNN
* [专知语义分割专栏](https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247485464&idx=3&sn=77fd30180cf66e1cb7b276509b38f358&chksm=fc85e90bcbf2601dfe6439076bb00896befe621dcdd75dfb77a931839b3b362d3226c4cd2ad5&mpshare=1&scene=24&srcid=10099bEid5dfACiAXiznsOX8#rd)
```python
Mask-RCNN [https://arxiv.org/pdf/1703.06870.pdf]

https://github.com/CharlesShang/FastMaskRCNN [Tensorflow]

https://github.com/TuSimple/mx-maskrcnn [MxNet]

https://github.com/matterport/Mask_RCNN [Keras]

https://github.com/jasjeetIM/Mask-RCNN [Caffe]
```

--------------------------------------------------
# 2019.03.01
## Mask R-CNN
- [x] [Mask R-CNN详解](https://blog.csdn.net/WZZ18191171661/article/details/79453780)

- [x] **Mask_R-CNN.mp4**

- [x] **maskrcnn_slides.pdf**
* [region-of-interest-pooling-explained](https://deepsense.ai/region-of-interest-pooling-explained/)
* Backbone (`ResNeXt`): +1.6 AP(bbox)

## `RoIPooling` & `RoIAlign`
### `RoIPooling`
1. Let’s consider a small example to see how it works. We’re going to perform region of interest pooling on a single 8×8 feature map, one region of interest and an output size of 2×2. Our input feature map looks like this:

![Feature Map](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/1.jpg)

2. Let’s say we also have a region proposal (top left, bottom right coordinates): (0, 3), (7, 8). In the picture it would look like this:

![region proposal](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/2.jpg)

3. Normally, there’d be multiple feature maps and multiple proposals for each of them, but we’re keeping things simple for the example.
By dividing it into (2×2) sections (because the output size is 2×2) we get:

![2×2 sections](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/3.jpg)

4. The max values in each of the sections are:

![output](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/output.jpg)

5. Here’s our example presented in form of a nice animation:

![gif](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/roi_pooling-1.gif)


### `RoIAlign`
1. unquantized (2×2) sections:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi0.png)

2. Sampling locations:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi1.png)

3. Bilinear interpolated values:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi2.png)

4. Max pooling output:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi3.png)

---------------------------------------------------
# 2019.03.02
## Mask R-CNN

---------------
- [x] **Mask-RCNN-Arc-How-RoI_Pooling-RoI_Warping_RoI-Align_Work.mp4**
* ROI Pooling和ROIAlign最大的区别是：前者使用了两次量化操作，而后者并没有采用量化操作，使用了线性插值算法，具体的解释如下所示:

![ROIPooling](https://img-blog.csdn.net/20180306110240257)
![ROIAlign](https://img-blog.csdn.net/20180306110334767)

----------------
- [x] **Paper Reading**
* [Mask R-CNN 论文翻译](https://alvinzhu.xyz/2017/10/07/mask-r-cnn/#fn:18)
* [Mask R-CNN完整翻译](https://blog.csdn.net/myGFZ/article/details/79136610)
* `RoIPool` performs coarse spatial quantizationfor feature extraction. To fix the misalignment, we propose a simple, quantization-free layer, called `RoIAlign`, that faithfully preserves exact spatial locations.
*  Second, we found it essential to `decouple mask and class prediction`: we predict a `binary mask` for each class independently, without competition among classes, and rely on the network’s RoI `classification branch` to predict the category. 
* Our models can run at about 200ms per frame on a GPU, and training on COCO takes one to two days on a single 8-GPU machine. 
* Instead, our method is based on `parallel prediction of masks and class labels`, which is simpler and more flexible.
* The `mask branch` has a [K*m^2]-dimensional output for `each RoI`, which encodes K binary masks of resolution m × m, one for each of the `K classes`.
* For an RoI associated with `ground-truth class k`, $L_{mask}$ is only defined on the `k-th mask` (other mask outputs do not contribute to the loss).
* we rely on the dedicated classification branch to predict the class label used to `select the output mask`. 
* collapsing it into a `vector representation` that `lacks spatial dimensions`.
* `RoIPool` is a standard operation for extracting a small feature map (e.g., 7×7) from each RoI
* non-maximum suppression
* The `mask branch` can predict K masks per RoI, but we only `use the k-th mask`, where k is the predicted class by the classification branch. The m×m floating-number mask output is then `resized to the RoI size`, and binarized at a threshold of 0.5
### update 2019.03.16
* This is different from common practice when applying FCNs to `semantic segmentation`, which typically uses a `per-pixel softmax` and a `multinomial cross-entropy loss`. In that case, masks `across classes compete(竞争)`; `in our case`, with a `per-pixel sigmoid` and a `binary loss`, they do not. We show by experiments that this formulation `is key for good` instance segmentation results.


* 2016-FCIS.pdf
* 2016-Segment Proposal.pdf
* 2015-DeepMask.pdf

--------------------
- [x] [Mask RCNN笔记](https://blog.csdn.net/xiamentingtao/article/details/78598511)
* ROI Align 的反向传播
> 常规的`ROI Pooling`的反向传播公式如下：

![](http://1.file.leanote.top/59fbd202ab644135b00006fa/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20171103101817.png?e=1551535688&token=ym9ZIrtxjozPN4G9he3-FHPOPxAe-OQmxzol5EOk:HQGMn_aJNffPMlsmOIilYXI1jR8)

> 这里，xi代表池化前特征图上的像素点；yrj代表池化后的第r个候选区域的第j个点；i*(r,j)代表点yrj像素值的来源（最大池化的时候选出的最大像素值所在点的坐标）。由上式可以看出，只有当池化后某一个点的像素值在池化过程中采用了当前点Xi的像素值（即满足i=i*(r，j)），才在xi处回传梯度。
> 类比于ROIPooling，`ROIAlign的反向传播`需要作出稍许修改：首先，在ROIAlign中，xi*（r,j）是一个浮点数的坐标位置(前向传播时计算出来的采样点)，在池化前的特征图中，每一个与 xi*(r,j) 横纵坐标均小于1的点都应该接受与此对应的点yrj回传的梯度，故ROI Align 的反向传播公式如下: 

![](http://1.file.leanote.top/59fbe350ab644137db000a4e/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20171103113216.png?e=1551535689&token=ym9ZIrtxjozPN4G9he3-FHPOPxAe-OQmxzol5EOk:8JFZp9cEc2vf2099pKVB_jqA4rE)

> 上式中，d(.)表示两点之间的距离，Δh和Δw表示 xi 与 xi*(r,j) 横纵坐标的差值，这里作为双线性内插的系数乘在原始的梯度上。

----------------------------------------------------
# 2019.03.03
## SE-Net
- [x] **85-ImageNet冠军模型SE-Net详解**
* SE-Net: `channel relationship`; GoogleNet: `spatial relationship`
* Mini-batch data sampling: 按照`类别`，不是按照`图像`进行训练。
* SE-module流程：
1.  将输入特征进行`Global AVE pooling`，得到 `1*1* Channel` 
2. 然后`bottleneck`特征交互一下，先压缩 channel数，再重构回channel数最后接个 `sigmoid`，生成channel 间`0~1`的 attention weights，最后scale乘回原输入特征.

![](https://pic4.zhimg.com/80/v2-2e8c37ad7e40b7f1cdfd81ecbae4e95f_hd.jpg)

* ResNet & SE_Net
![](https://github.com/kinglintianxia/note_book/blob/master/imgs/SE_Net.png)

* GFLOPs

网络        		| GFLOPs
--------   		| -----
ResNet-50  		| 3.86
ResNet-101 		| 7.58
ResNet-152 		| 11.30
BN-Inception  	| 2.03

* ResNet-50 `Conv5层`没必要加`SE`模块


-----------------------
## Conv卷积，边缘检测
- [x] **吴恩达Conv卷积，边缘检测部分**
* 第一周 卷积神经网络-1.2/1.3节
* Why convolutions:
1. Parameter sharing.
2. Sparsity of connections.

### **第三周　目标检测**
* 跑道检测是`classification with localization`问题。
`yolov3` + `self-driving-car -> Project 4 - Advanced Lane Finding`
* The Network output is:<br>

|	output		|	meaning					| example 'pedestrian', 'car', 'motorcycle'	|	|	
|	------		|	------					|	------		|	------					|
|	Pc			|	Is there any object		|	1			|	0						|
|	bx			|	bbox center x			|	0.5			|	?	'Don't care'		|
|	by			|	bbox center y			|	0.5			|	?	'Don't care'		|
|	bh			|	bbox height				|	0.4			|	?	'Don't care'		|
|	bw			|	bbox width				|  	0.6			|	?	'Don't care'		|
|	c1			|	class 1 prob			|	0			|	?	'Don't care'		|
|	c2			|	class 2 prob			|	1			|	?	'Don't care'		|
|	c3			|	class 3 prob			|	0			|	?	'Don't care'		|



* The loss may be:

loss				|	case
------				|	------
sum(y^i-yi)^2		|	y0 = 1
(y^0-y0)^2			|	y0 = 0

* [bx, by, bh, bw] are parameted relative to the grid cell.


-----------------------------------------------------
## YOLO
see [24-yolov3.md](https://github.com/kinglintianxia/note_book/blob/master/24-yolov3.md)

------------------------------------------------------
# 2019.03.06
## DANet
see [22-DANet.md](https://github.com/kinglintianxia/note_book/blob/master/22-DANet.md)



------------------------------------------------------
# 2019.03.07
see [19-deeplabv3+.md](https://github.com/kinglintianxia/note_book/blob/master/19-deeplabv3%2B.md)



# 2019.03.14
## AlexeyAB/darknet

- [ ] **README.md**

* `YOLOv3-spp` better than `YOLOv3` - mAP = 60.6%, FPS = 20

* also create SO-library on Linux and DLL-library on Windows

* improved binary neural network performance 2x-4x times for Detection on CPU and GPU if you trained your own weights by using this XNOR-net model (bit-1 inference)

* improved neural network `performance ~7%` by fusing 2 layers into 1: `Convolutional + Batch-norm`

* added correct calculation of `mAP, F1, IoU, Precision-Recall` using command:
$ ./darknet detector map...

* added drawing of chart of average-Loss and accuracy-mAP (-map flag) during training

* To calculate anchors: 
$ ./darknet detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416

## How to compile on Linux
* `CUDNN_HALF=1` to build for Tensor Cores (on Titan V / Tesla V100 / DGX-2 and later) speedup Detection 3x, Training 2x

* LIBSO=1 to build a library darknet.so and binary runable file uselib that uses this library. 

* How to use this `SO-library` from your own code, you can look at [C++ example](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp) 

## How to train (to detect your custom objects):
* Only for `small datasets` sometimes better to `decrease learning rate`, for 4 GPUs set learning_rate = 0.00025 (i.e. learning_rate = 0.001 / GPUs). In this case also increase 4x times `burn_in` = and `max_batches` = in your cfg-file. I.e. use burn_in = 4000 instead of 1000.

* `Note:`
If during training you see `nan` values for `avg (loss)` field - then training goes wrong, but if `nan` is in some other lines - then training goes well.

* `Note:`
If you changed width= or height= in your cfg-file, then new width and height must be divisible by 32.

* cfg:
```python
# batch size
batch=64
# output stride	
subdivisions=8
# training steps
max_batches = 500200
```

## How to train tiny-yolo (to detect your custom objects):

* Yolo based on other models:
[DenseNet201-Yolo](https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/densenet201_yolo.cfg)
[ResNet50-Yolo](https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/resnet50_yolo.cfg)

## When should I stop training:

* Usually sufficient 2000 iterations for each class(object), but not less than 4000 iterations in total. 

* The `final avgerage loss` can be from 0.05 (for a small model and easy dataset) to 3.0 (for a big model and a difficult dataset).

* Or just train with -map flag:
$ ./darknet detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -map


## How to calculate mAP on PascalVOC 2007:

## How to improve object detection:
* set flag `random=1` in your `\*.cfg` file.

* Increase network resolution in your .cfg-file (`height=608, width=608` or any value `multiple of 32`.

* You should preferably have `2000` different images for each class or more, and you should train 2000*classes iterations or more

* desirable that your training dataset `include images with non-labeled objects` that you do not want to detect

* for training with a large number of objects in each image...
* for training for small objects...
* for training for `both small and large objects` use modified models:
* to speedup training (with decreasing detection accuracy) do `Fine-Tuning(微调)` instead of `Transfer-Learning(迁移学习)`, set param `stopbackward=1`.
* Increase network-resolution by set in your .cfg-file (height=608 and width=608) or (height=832 and width=832) or (any value multiple of 32) - this increases the precision and makes it possible to detect small objects:

## How to use Yolo as DLL and SO libraries
* on Linux - set `LIBSO=1` in the `Makefile` and do `make`.
* There are 2 APIs:
1. [C API](https://github.com/AlexeyAB/darknet/blob/master/include/darknet.h)
2. [C++ API](https://github.com/AlexeyAB/darknet/blob/master/include/yolo_v2_class.hpp)
<br>
[C++ example that uses C++ API](https://github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp)


----------------------------------------------
# 2019.03.15
- [x] **Check CityScapes tfrecord is correct?**
* tfrecord correct.
## 2019.03.16 !!!
set `--save_summaries_images=True` when `training`, and use `tensorboard` to see.
* Images can not match labels!!!

![tfrecord](https://github.com/kinglintianxia/note_book/blob/master/imgs/tfrecord_cityscapes.png)

- [x] **Train MobileNet on VOC dataset**
* mobilenet_v2, `re-use all the trained weights except the logits` [OK]
global step 8890: loss = 0.3322 (0.205 sec/step), miou_1.0[0.706052244] <br>
global step 30000: loss = 0.2057 (0.196 sec/step), miou_1.0[0.727050483]

- [x] **Show KittiSeg seg map**
* demo.py L184 
```python
# output: [batch*height*width, NUM_CLASSES].
output = sess.run([softmax], feed_dict=feed)    # Get probility image. <br>
"""
    output: 
     [array([[9.9968946e-01, 3.1052253e-04],
           [9.9980527e-01, 1.9472565e-04],
           [9.9978584e-01, 2.1418222e-04],
           ...,
           [9.9948078e-01, 5.1923015e-04],
           [9.9927455e-01, 7.2546635e-04],
           [9.9853718e-01, 1.4628501e-03]], dtype=float32)]
"""
# road 
    output_image = output[0][:, 1].reshape(shape[0], shape[1])
	out_bg = output[0][:, 0].reshape(shape[0], shape[1])
```
* road

![road](https://github.com/kinglintianxia/note_book/blob/master/imgs/demo_raw_road.png)

* background

![background](https://github.com/kinglintianxia/note_book/blob/master/imgs/demo_raw_bg.png)


------------------------------------------------
# 2019.03.16

- [x] **KittiSeg `seg map` 中使用`per-pixel sigmoid` and a `binary loss`**
* Review Mask-RCNN Paper.

## Read [Detectron](https://github.com/facebookresearch/Detectron)
* [INSTALL.md](https://github.com/facebookresearch/Detectron/blob/master/INSTALL.md)
* [GETTING_STARTED.md](https://github.com/facebookresearch/Detectron/blob/master/GETTING_STARTED.md)

# Update 2019.03.17
Find how implement `per-pixel sigmoid` and a `binary loss`. 
In 'Detectron/detectron/modeling/mask_rcnn_heads.py'
```python
"""Add Mask R-CNN specific losses."""
loss_mask = model.net.`SigmoidCrossEntropyLoss`(
    [blob_mask, 'masks_int32'],
    'loss_mask',
    scale=model.GetLossScale() * cfg.MRCNN.WEIGHT_LOSS_MASK
)
```



## Read tensorflow Mask R-CNN code: [FastMaskRCNN](https://github.com/CharlesShang/FastMaskRCNN).
Find how implement `per-pixel sigmoid` and a `binary loss`.

* FastMaskRCNN/libs/nets/pyramid_network.py#L538

```python
# mask_targets = slim.one_hot_encoding(mask_targets, 2, on_value=1.0, off_value=0.0)
# mask_binary_loss = mask_lw * tf.losses.softmax_cross_entropy(mask_targets, masks)
# NOTE: w/o competition between classes. 
mask_targets = tf.cast(mask_targets, tf.float32)
"""
For instance, one could perform multilabel classification where a picture can contain 
both an elephant and a dog at the same time.
tf.nn.sigmoid_cross_entropy_with_logits(
    _sentinel=None,
    labels=None,
    logits=None,
    name=None
)
Computes sigmoid cross entropy given logits.
For brevity, let x = logits, z = labels. The logistic loss is:
z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
"""
# mask_targets: learning targets of shape (M, pooled_height, pooled_width, num_classes) in {0, 1} values.
# mask: (N, h, w, num_classes)   
mask_loss = mask_lw * `tf.nn.sigmoid_cross_entropy_with_logits(labels=mask_targets, logits=masks)` 
mask_loss = tf.reduce_mean(mask_loss) 
"""
tf.cond(
    pred,
    true_fn=None,
    false_fn=None,
    strict=False,
    name=None,
    fn1=None,
    fn2=None
)
Return true_fn() if the predicate pred is true else false_fn(). (deprecated arguments)
"""
mask_loss = tf.cond(tf.greater(tf.size(labels), 0), lambda: mask_loss, lambda: tf.constant(0.0))
tf.add_to_collection(tf.GraphKeys.LOSSES, mask_loss)
mask_losses.append(mask_loss)

```


## Train KittiSeg 12000 steps.
see `KittiSeg.md`


----------------------
- [x] **KittiSeg add `poly` learning rate**
## deeplab code: 'deeplab/utils/train_utils.py#L162'
```python
  """Gets model's learning rate.

  Computes the model's learning rate for different learning policy.
  Right now, only "step" and "poly" are supported.
  (1) The learning policy for "step" is computed as follows:
    current_learning_rate = base_learning_rate *
      learning_rate_decay_factor ^ (global_step / learning_rate_decay_step)
  See tf.train.exponential_decay for details.
  (2) The learning policy for "poly" is computed as follows:
    current_learning_rate = base_learning_rate *
      (1 - global_step / training_number_of_steps) ^ learning_power

  Args:
    learning_policy: 				Learning rate policy for training.
    base_learning_rate: 			The base learning rate for model training.
    learning_rate_decay_step: 		Decay the base learning rate at a fixed step.
    learning_rate_decay_factor: 	The rate to decay the base learning rate.
    training_number_of_steps: 		Number of steps for training.
    learning_power: 				Power used for 'poly' learning policy.
    slow_start_step: 				Training model with small learning rate for the first
      few steps.
    slow_start_learning_rate: 		The learning rate employed during slow start.

  Returns:
    Learning rate for the specified learning policy.

  Raises:
    ValueError: If learning policy is not recognized.
  """
  global_step = tf.train.get_or_create_global_step()
  if learning_policy == 'step':
    learning_rate = tf.train.exponential_decay(
        base_learning_rate,
        global_step,
        learning_rate_decay_step,
        learning_rate_decay_factor,
        staircase=True)
  elif learning_policy == 'poly':
    learning_rate = tf.train.polynomial_decay(
        base_learning_rate,
        global_step,
        training_number_of_steps,
        end_learning_rate=0,
        power=learning_power)
  else:
    raise ValueError('Unknown learning policy.')

  # Employ small learning rate at the first few steps for warm start.
  return tf.where(global_step < slow_start_step, slow_start_learning_rate,
                  learning_rate)

```

----------------------
- [x] **解决道路label包含ignore的问题**

* [How to train on your own data](https://github.com/MarvinTeichmann/KittiSeg/blob/master/docu/inputs.md)
* `batch_size` 和 `reseize_image | crop_patch`对立.
## Find Answer!
`road` 和`background`取反了
```python
# label: [batch, height, width, class]
road = tf.expand_dims(tf.to_float(label[:, :, :, 1]), 3)
tf.summary.image(tensor_name + '/gt_image', road)
# king@2019.03.15
# add gt_bg to show.
gt_bg = tf.expand_dims(tf.to_float(label[:, :, :, 0]), 3)
tf.summary.image(tensor_name + '/gt_bg', gt_bg)

```



----------------
- [x] **KittiSeg add logits to tensorboard**
* `KittiSeg/submodules/TensorVision/tensorvision/core.py#L89`



--------------
# 2019.03.17

- [x] **Train KittiSeg**
* Add 'poly' and 'sigmoid binary loss'
* VGG16 backbone
* In logits I See 'checkerboard pattern'

![kitti_checkerboard_pattern](https://github.com/kinglintianxia/note_book/blob/master/imgs/kitti_checkerboard_pattern.png)


-----------------------
## Kitti Road submission

### Data format for result submission:
* For submission, results must be transfered into the Birds Eye View (BEV).
* see “python/transform2BEV.py” for further details.
* Faild !!






-----------------------
- [x] **Best threshold**
* KittiSeg/submodules/evaluation/kitti_devkit/seg_utils.py
```python
# F-measure operation point
beta = 1.0
betasq = beta**2
F = (1 + betasq) * (precision * recall)/((betasq * precision) + recall + 1e-10)
index = F.argmax()
MaxF= F[index]
# BestThresh
if thresh is not None:
        BestThresh= thresh[index]
        prob_eval_scores['BestThresh'] = BestThresh
```

----------------------------------------------
# 2019.03.18
- [x] **Train KittiSeg ResNet101**
Not Better ? <br>
Need to evalute on `Test` set.


-----------------------
## MobileNetV2
see `21-MobileNetV2.md`


-----------------------
## MobileNetV2
- [x] **MobileNetV2 Code Reading**
* [Official tensorflow code](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)
see `21-MobileNetV2.md`

-----------------------
## MobileNetV2
- [x] **Train DeepLab xception_65**
### xception_65, cityscapes, `re-use only the network backbone`
Not better enough! <br>
miou_1.0[0.722017109] VS 80.42%(OS=8) 
放弃 'xception_65'


-----------------------
## MobileNetV2
- [x] **DeepLab Code Reading**
see `19-deeplabv3+.md`



-----------------------
# 2019.03.21
- [x] add `eval.py` per-class IoU.


-----------------------
# 2019.03.22
- [x] **理清tensor 尺寸变化**
* 'model.py/split_separable_conv2d'

* Shape: 
`mobilenet_v2(Atros Conv)`: output_stride -> `ASPP`: output_stride -> `Decoder`: + short cut `layer_4`: OS=8 -> decoder_output_stride -> `ASPP`: decoder_output_stride


|	net					|	shape					|
|	------				|	------					|
| backbone(Atros Conv)	|	[batch, os, os, 320]	|
|	ASPP(dropout 0.1)	|	[batch, os, os, 256]	|
| Decoder+layer_4(OS=8)	|	decoder_output_stride	|
|	ASPP				|	decoder_output_stride	|



-------------------------
# 2019.03.23
- [x] **改进ASPP**
ASPP 仿照 PSPNet `Concat` 原feature map <br>
`deeplab/model.py#L441`


- [x] **add `multi_loss` seg map**
see `19-deeplabv3+.md`


-----------------
# 2019.03.25
- [x] **eval time**
* `deeplab_demo.ipynb`
('---eval time: ', 152.82487869262695) ms.


-----------------
- [x] add `sigmoid_cross_entropy` in `deeplab/utils/train_utils.py`

--------------
# 2019.03.25
- [x] **Train `dense_prediction_cell`**



--------------
# 2019.03.26
- [x] **self-attention**
see `19-deeplabv3+.md`

--------------
# 2019.03.26
- [x] **better decoder**
see `19-deeplabv3+.md`

--------------
# 2019.03.28
- [x] **kitti road dataset**
see `19-deeplabv3+.md`




--------------
# Update 2019.03.28
- [x] **Training cityscapes coarse dataset**
* `gt_Coarse` 生成的`*_gtCoarse_labelTrainIds.png`只有黑白两色?
暂时放置
* 视觉错误，脚本没错。





--------------
# 2019.03.29
- [x] **计算网络 Params & Multiply-Adds**

* [tensorflow统计网络参数量](https://blog.csdn.net/feynman233/article/details/79187304)
```python
# print model total params.
def show_params():
  total = 0
  for v in tf.trainable_variables():
    dims = v.get_shape().as_list()
    num  = int(np.prod(dims))
    total += num
    print('  %s \t\t Num: %d \t\t Shape %s ' % (v.name, num, dims))
  print('\nTotal number of params: %d' % total)

# Modify `eval.py`
```

* [How to count Multiply-Adds operations?](https://stackoverflow.com/questions/51077386/how-to-count-multiply-adds-operations)

* [how to calculate the flops from tfprof in tensorflow?](https://stackoverflow.com/questions/47387561/how-to-calculate-the-flops-from-tfprof-in-tensorflow/50680858#50680858) <br>

Ques: how could one get the exact number of FLOP disregarding the initialisation FLOP? <br>

Answer: Freeze the graph with a `pb`

## Multiply-Adds
* [GFLOPS百度百科](https://baike.baidu.com/item/GFLOPS/989595?fr=aladdin)
* GFLOPS 就是 Giga Floating-point Operations Per Second,即每秒10亿次的浮点运算数
* GFLOPS = Multiply-Adds(Billion)
* [](https://stackoverflow.com/questions/50288523/what-is-flops-calculated-by-tensorflow-tf-profile) <br>
their calculation only considers `multiply and add operations`. Whereas `tensorflow` further includes `batch norm or max operations of pooling, relu`. I think that is the reason for the difference.

* Code in `deeplab_demo_me.ipynb`.




----------------
# 2019.03.29
- [x] **frozen graph**
```shell
$ python export_model.py --logtostderr --model_variant="mobilenet_v2" --atrous_rates=6 --atrous_rates=12 --atrous_rates=18 --output_stride=16 --decoder_output_stride=4 --crop_size=1025 --crop_size=2049 --checkpoint_path=./datasets/cityscapes/exp/train_on_train_set/0328-backbone-aspp-decoder-bn-self_attentionv3/model.ckpt-90000 --export_path=./datasets/cityscapes/frozen_graph.pb --num_classes=19 --use_self_attention=True
```

---------------------------
- [x] **Better Upsampling**
see `19-deeplabv3+.md`


----------------
- [x] **prepare airplane lane dataset**


---------------------------
# 2019.03.30
- [x] **infer script on frozen graph**
* `infer.py`

- [x] **draw Cityscapes label colormap**
* `draw_citysacpes_colormap.py`


---------------------------------------------
# 2019.03.31
- [ ] **submit cityscapes & kitti**

## Cityscapes Submission.
* It is well-known that the global IoU measure is biased toward object instances that cover a large image area.
* [Create Submission](https://www.cityscapes-dataset.com/create-submission/)
----------------
### Requirements
* single zip archive
* maximum 100 MB
* Result files with filename "berlin_000123_000019*.ext" where `ext` is `png` for pixel-level and `txt` for instance-level semantic labeling. The files can be in arbitrary sub folders.
* Exactly one result file for each test image
* Result image size must be equal to the input image size, i.e. 2048 x 1024 pixels
* Labels must be encoded by labelIDs, not trainIDs, e.g. a car should have ID 26

* code submit script.



## Kitti Road Submission.




-----------------------------
# 2019.04.04
- [x] **better aspp**
* concat [features, 320], [1x1, 320//6], [2x2, 320//6], [3x3, 320//6], [3x3 rate 6, 320//6], [3x3 rate 12, 320//6], [3x3 rate 18, 320//6]
* then [1x1, 256] ouput.


---------------
- [x] **tf.losses.binary_crossentropy**
```python
tf.losses.binary_crossentropy(
    y_true,
    y_pred,
    from_logits=False,
    label_smoothing=0
)
Args:
    from_logits: Whether `y_pred` is expected to be a logits tensor. By default,
      we assume that `y_pred` encodes a probability distribution.
    label_smoothing: Float in [0, 1]. If > `0` then smooth the labels.
    reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to loss.
      Default value is `SUM_OVER_BATCH_SIZE`.
    name: Optional name for the op.
```

* [tf.losses all](https://stackoverflow.com/questions/47034888/how-to-choose-cross-entropy-loss-in-tensorflow)

> `K.binary_crossentropy` is `tf.nn.sigmoid_cross_entropy_with_logits`
> `K.categorical_crossentropy` is `tf.nn.softmax_cross_entropy_with_logits`

---------------------------
- [x] **train fine+coarse**
* +0.8 %



---------------------------
# 2019.04.06
- [x] **prepare cityscapes trainval set**




# ==TODO==


- [ ] **train camvid**

- [ ] **train trainval set**






--------------
- [ ] **输入Image手工添加其他特征channels，比如Canny,Gray,hsv, self-attention等**


--------------
- [ ] **搞清`atrous convolution` code实现**





- [ ] **+ DANet**

- [ ] **在seg map 中使用`per-pixel sigmoid` and a `binary loss`**
- [ ] **在FCN Decoder 中加入多个seg loss (ROI Align生成feature map),训练网络。**

- [ ] **Pooling -> Conv**
- [ ] **handle checkerboard pattern**

- [ ] **KittiBox 添加其他class**
- [ ] **MultiNet基础上修改，完成道路分割、车辆、行人等检测(或+分割)**

- [ ] **YOLOv3添加Segmap**

- [ ] **labelme my own dataset**







----------------------------------------------------------------------------------
- [ ] [UNIX Tutorial for Beginners](http://www.ee.surrey.ac.uk/Teaching/Unix/)
- [ ] [完全解析RNN, Seq2Seq, Attention注意力机制](https://zhuanlan.zhihu.com/p/51383402)
.
