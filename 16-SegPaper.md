# 1. 修改deeplab，简化语义标签，提高实时性。

# 2. 在MultiNet基础上修改，完成道路分割、车辆、行人等检测。

# 3. 打破FCN Encoder-Decoder架构,设计one step end-to-end网络。预测语义分割每一类物体在图片上的像素轮廓，而不是端到端输出图片。

# 4. 将[pooling层]换成[边缘检测层]试一下效果; 能不能让Pooling层在降size的同时，层内参数可训练化（让网络自己选择保留哪些信息，而不仅仅是MaxPooling或AVGPooling）

# 5. 注意力模型

# 6. 现在训练网络都是让网络“猜dog在哪里？在这里”(前向传播), "不对，你离正确答案多远"（计算loss）；“我再猜一下”（反向传播）；能不能设计一种训练结构，告诉网络“这张图中dog在哪里”。

# 7. 在分割网络中加入ROI-Pooling层
* Input -> ROI Pooling -> FCN

# 8. 在分割网络中加入边缘检测层，结合边缘对score层进行采样分类，当一个轮廓中大于阈值的像素点属于A类，则划为A类。

# 9. 在激活函数Relu等上做文章，让网络逐渐收敛到真值（比如正负二分类），不用softmax和argmax等。

# 10. 在feature map中手工加入边缘检测层， 让网络自己学习把边缘信息加入分割判断中。

# 11. 将卷积操作的 'bias'值换成相应区域的边缘图像试试。

# 12. COS_Net.jpg

#　13. 分割网络输出层21channels，每个channels表达一个类别的mask， 将分类和分割解耦。

# 14. yolo检测道路主要类别+语义分割道路，实时跑在北三环数据集上。

## 使用ResNet实现的分割网络效果state-of-art. 

------------------
# 2019.01.08
- [x] **跑通KittiSeg evaluate.py程序**	
- [x] **整理KittiSeg和MultiNet代码**	

# 2019.01.09(评价指标)
- [x] **阅读MaxF1论文(THE KITTI-ROAD DATASET)**
* [blog1](https://blog.csdn.net/sinat_28576553/article/details/80258619)
* ego-lane vs.opposing lane,(自我车道与对方车道)
* 2D Bird’s Eye View (BEV) space.
* For methods that output confidence maps (in contrast to binary road classification), the classification threshold τ is chosen to maximize the F-measure.

- [x] **IOU**
* See KittiSeg code.

# 2019.01.13
- [x] **评价MultNet训练结果**
```python
segmentation Evaluation Finished. Results
Raw Results:
[train] MaxF1 (raw)    :  98.9767 
[train] BestThresh (raw)    :  68.2353 
[train] Average Precision (raw)    :  92.5437 
[val] MaxF1 (raw)    :  95.9975 
[val] BestThresh (raw)    :  14.9020 
[val] Average Precision (raw)    :  92.3125 
Speed (msec) (raw)    :  42.8566 
Speed (fps) (raw)    :  23.3336
```

- [x] **读AP指标（VOC DATASET）**
* [AP wiki](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision)
* [blog1](https://blog.csdn.net/niaolianjiulin/article/details/53098437)
* [blog2](https://blog.csdn.net/hysteric314/article/details/54093734)
* 以recall为横坐标（0~1),precision为纵坐标（0~1）作图。得到一条曲线。该曲线下的面积即为AP.
$$ AP = \int_0^1 {p(r)} dr $$ 

# 2019.01.14
- [x] **标注Cityscape数据集**
* [blog1](https://blog.csdn.net/fabulousli/article/details/78633531)
* [labelme 工具](https://github.com/wkentaro/labelme) 

# 2019.01.15
- [x] **发现好博客**
* [深度学习数据集介绍及相互转换](https://www.cnblogs.com/ranjiewen/p/9860953.html)

- [x] **labelme 工具生成Kitti Road数据集**
* img.putpalette([0,0,0,0,255,0,0,0,255])
	
- [x] **labelme 工具生成CityScapes数据集：**
* from cityscapesscripts.helpers.labels     import name2label 

- [ ] **cityscapesScripts标注自己图片**

# 2019.01.16
- [x] **[Semantic Segmentation using Fully Convolutional Networks over the years]**
* [link](https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html)
* [vis of FCN](http://ethereon.github.io/netscope/#/preset/fcn-8s-pascal)
* [vis of common networks](http://ethereon.github.io/netscope/quickstart.html)
* [Deconvolutions](https://distill.pub/2016/deconv-checkerboard/)
* LinkNet: A feature map with shape [H, W, n_channels] is first convolved with a 1*1 kernel to get a feature map with shape [H, W, n_channels / 4 ] and then a deconvolution takes it to [2*H, 2*W, n_channels / 4 ] a final 1*1 kernel convolution to take it to [2*H, 2*W, n_channels / 2 ]. Thus the decoder block fewer parameters due to this channel reduction scheme.

# 2019.01.20
- [x] **[Deconvolutions]**
* [link](https://distill.pub/2016/deconv-checkerboard/)
* When we look very closely at `images generated by neural networks`, we often see a strange `checkerboard pattern` of artifacts.
* `In theory`, our models could learn to carefully write to unevenly overlapping positions so that the output is evenly balanced; `In fact`, not only do models with uneven overlap not learn to avoid this, but models with even overlap often learn kernels that cause similar artifacts! 
* Better Upsampling
> `One approach` is to make sure you use `a kernel size that is divided by your stride`, avoiding the overlap issue.
> Another approach is `resize the image (using nearest-neighbor interpolation or bilinear interpolation) and then do a convolutional layer`.
* We don’t necessarily think that either approach is the final solution to upsampling, but they do fix the checkerboard artifacts.
* Whenever we `compute the gradients of a convolutional layer`, we do `deconvolution` (transposed convolution) on the `backward pass`.

- [x] **[2017-course_by_Tao Kong.pdf]**
* The region proposal network is a FCN which outputs `K*(4+2) sized vectors`.
* `Mask R-CNN` = Faster R-CNN with FCN on ROIs.
* Useful links for learning detection:
> RCNN/Fast R-CNN/Faster R-CNN: https://github.com/rbgirshick
> YOLO/YOLOv2: https://pjreddie.com/darknet/yolo/
> SSD: https://github.com/weiliu89/caffe/tree/ssd
> R-FCN: https://github.com/daijifeng001/R-FCN
> Tensorflow detector:https://github.com/tensorflow/models/tree/master/research/object_detection

# 2019.01.21
- [x] **基于深度卷积神经网络的目标检测算法研究_黄莉芝.caj**
* 注意力模型
* 语义分割分为“阈值分割”，“边缘分割”， “区域分割”
* 计算边框回归

# 2019.01.22
- [x] **看第四章“目标定位优化”** 

- [x] **专知[目标检测专栏]**
* [link](https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247485072&idx=2&sn=e9f4f0d1daeb3a144e37fcfcc61e908f&chksm=fc85e783cbf26e95d153af7c825ef9b8fc4fc6fd37f75709de98eae769e6b70f8da29ae65a73&scene=21#wechat_redirect)
* [CVPR'17 Tutorial](http://deeplearning.csail.mit.edu/)
* [图像目标检测（Object Detection）原理与实现 （1-6）](https://blog.csdn.net/marvin521/article/details/9058735)
* 下载了[基于特征共享的高效物体检测_任少卿.pdf]
* 下载了[Bounding-box_regression详解.pdf]

- [x] **[RCNN, Fast-RCNN, Faster-RCNN的一些事]**
* [link](http://closure11.com/rcnn-fast-rcnn-faster-rcnn%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/)
* 在Fast-RCNN中，作者巧妙的把bbox regression放进了神经网络内部，与region分类和并成为了一个multi-task模型
* Bounding-box Regression
* 我的CVPR 2016论文里自己弄的一个数据集就借助了Fast-RCNN.（`可以利用现有分割网络进行数据集粗略分割`）

- [x] **[目标检测——从RCNN到Faster RCNN 串烧]**
* [link](https://blog.csdn.net/xyy19920105/article/details/50817725)

# 2019.01.24 计划
- [x] **[机器学习随笔]**
* [link](https://zhuanlan.zhihu.com/p/35058212)

- [x] **[图像语义分割+FCN/U-Net]**
* [link](https://zhuanlan.zhihu.com/p/31428783)
* [visualization of FCN](http://ethereon.github.io/netscope/#/preset/fcn-8s-pascal)
* 对于FCN-8s，首先进行pool4+2x upsampled feature`逐点相加`，然后又进行pool3+2x upsampled`逐点相加`，即进行更多次特征融合。
* 为了解决图像过小后 1/32 下采样后输出feature map太小情况，FCN原作者在第一个卷积层`conv1_1加入pad=100`。
* 在特征融合的时候，如何保证逐点相加的feature map是一样大的呢？这就要引入crop层了
* 在caffe中，存储数据的方式为：caffe blob = [num, channel, height, width]
1. 而score_pool4c设置了axis=2，相当于从第2维(index start from 0!)往后开始裁剪，即裁剪height和width两个维度，同时不改变num和channel纬度
2. 同时设置crop在height和width纬度的开始点为offset=5
3. 用Python语法表示，相当于score_pool4c层的输出为：
```python
crop-pool4 = score-pool4[:, :, 5:5+score2.height, 5:5+score2.width]
```
* `Deconvelution`计算：`score-fr`[1,21,16,16] -> `score2` [1,21,34,34] , kernel_size:4, stride:2
> Conv: $$ out = (in+2*pad-kernel)/stride + 1 $$
> DeConv: $$ out = (in-1)*stride + kernel -2*pad $$
> And Now: $$ 34 = (16-1)*2+4-2*0 $$

# 2019.01.25 
- [x] **[FCN学习:Semantic Segmentation]**
* [link](https://zhuanlan.zhihu.com/p/22976342)
* Faster-RCNN中使用了RPN(Region Proposal Network)替代Selective Search等产生候选区域的方法。`RPN是一种全卷积网络`，所以为了透彻理解这个网络，首先学习一下FCN
* [Caffe 中如何计算卷积](https://www.zhihu.com/question/28385679)
1. Input feature to Matrix
![Input feature to Matrix](https://pic1.zhimg.com/80/v2-701705db7504bb24f859122545b23174_hd.png)
2. Filters to matrix
![Filters to matrix](https://pic1.zhimg.com/80/v2-339657291663a4e791a9b34952d5859c_hd.png)
3. `Filter Matrix`乘以`Feature Matrix`的转置,得到输出矩阵`Cout x (H x W)`,就可以解释为输出的三维Blob`(Cout x H x W)`
4. Detail sample.
![Caffe conv](https://pic4.zhimg.com/80/a6421bae22236c0509623b8b7f7bbb03_hd.jpg)
5. 多.通道卷积计算方式
![多通道卷积](https://pic1.zhimg.com/80/v2-8d72777321cbf1336b79d839b6c7f9fc_hd.jpg)
* 矩阵微分公式:
$$ \frac {d(Ax+b)}{dx} = {A^T}$$

# 2019.01.26
- [x] **[一文读懂Faster RCNN]**
* [link](https://zhuanlan.zhihu.com/p/31426458)
* Faster R-CNN网络结构图:
![Faster R-CNN](https://pic4.zhimg.com/80/v2-e64a99b38f411c337f538eb5f093bdf3_hd.jpg)
* bounding box regression原理
* 对于一副任意大小PxQ图像,传入Faster RCNN前首先reshape到固定MxN,im_info=[M, N, scale_factor]则保存了此次缩放的所有信息,然后经过Conv Layers,经过4次pooling变为WxH=(M/16)x(N/16)大小,其中feature_stride=16则保存了该信息,用于计算anchor偏移量.
------
## update 2019.02.27
* VGG Feature Map: "conv5_3".
* 9个anchors(矩形)共有3种形状，长宽比为大约为{width:height} = {1:1, 1:2, 2:1} 三种(ps. 每种比例有三个尺度)，如图所示:
![anchors](https://pic4.zhimg.com/80/v2-7abead97efcc46a3ee5b030a2151643f_hd.jpg)
* 在caffe基本数据结构blob中以如下形式保存数据：
```python
blob = [batch, channel, height, width] = [N, C, H, W]
```
* bounding box regression原理
> 窗口一般使用四维向量 (x, y, w, h) 表示，分别表示窗口的`中心点`坐标、`宽`和`高`.
> 先做平移，再做缩放。
* 对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 (t_x, t_y, t_w, t_h)，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。
那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 (t_x, t_y, t_w, t_h)，显然即可用来修正Anchor位置了。
* 解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，`im_info`=[M, N, scale_factor]则保存了此次缩放的所有信息。
* 经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中`feature_stride=16`则保存了该信息，用于计算anchor偏移量。
* RPN网络处理流程：
> 生成anchors -> softmax分类器提取fg anchors -> bbox reg回归fg anchors -> Proposal Layer生成proposal boxes=[x1, y1, x2, y2]
* See [COS_Net.jpg].
* 从PoI Pooling获取到`7x7=49`大小的proposal feature maps.
* 在训练和检测阶段生成和存储anchors的顺序完全一样，这样训练结果才能被用于检测！

# 2019.01.27
- [x] **GDB调试工具**
* [RMS's gdb Debugger Tutorial](http://www.unknownroad.com/rtfm/gdbtut/)
```shell
$ gcc -g inf.c			# Compile the program with debugging flags.
$ gdb a.out				# Lets load up gdb.
$ run & ctrl+c			# Set off the infinite loop, then press `Ctrl-C` to send the program a SIGINT. 
$ backtrace				# We will use the `backtrace` command to examine the stack
$ frame 1
```

- [ ] **gdb 调试入门**
* [gdb 调试入门](http://blog.jobbole.com/107759/)

## Qt学习
- [ ] **Qt入门学习——Qt Creator的使用**
* [Qt入门学习——Qt Creator的使用](https://blog.csdn.net/tennysonsky/article/details/48004119)

- [ ] **Qt 学习之路 2**
* [Qt 学习之路 2](https://www.devbean.net/category/qt-study-road-2/)

# 2019.01.28
- [x] **服务器安装Anaconda环境**
* [blog](https://blog.csdn.net/qq_17534301/article/details/80869998)
* [Installing on Linux](http://docs.anaconda.com/anaconda/install/linux/)
* [Anaconda installer archive](https://repo.anaconda.com/archive/)

# 2019.01.29
- [x] **安装Caffe**
* see update of `03-Install_caffe.md`

- [x] **跑FCN**
* git clone https://github.com/shelhamer/fcn.berkeleyvision.org.git
* Download voc-fcn8s caffe model.
```shell
$ cd fcn.berkeleyvision.org/voc-fcn8s/
$ proxychains wget http://dl.caffe.berkeleyvision.org/fcn8s-heavy-pascal.caffemodel
```
* run
```shell
$ conda activate py27	# for `added by Anaconda3 5.3.0 installer` in '~/.bashrc'
or
$ source activate py27	# for `export PATH="/home/jun/anaconda3/bin:$PATH"` in '~/.bashrc'
$ python infer.py
```

* vis of caffe net.
[NetScope](http://ethereon.github.io/netscope/#/editor)

## sublime 高亮当前行
* [blog](https://yijile.com/log/128.html)


# 2019.02.13
- [x] **跑通Faster R-CNN**
* [github](https://github.com/smallcorgi/Faster-RCNN_TF)

## Install dependeces.
```shell
# change to python 2.7
$ sudo pip install easydict
```
## Build.
```shell
$ cd Faster-RCNN_TF/lib
$ make
```

## Run demo.py
```shell
$ python ./tools/demo.py --model ./data/model/VGGnet_fast_rcnn_iter_70000.ckpt
```
### Run Errors.
* ERROR 1
> tensorflow.python.framework.errors_impl.NotFoundError: /home/jun/Documents/Faster-RCNN_TF/tools/../lib/roi_pooling_layer/roi_pooling.so: undefined symbol: _ZTIN10tensorflow8OpKernelE

```shell
TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')
# Define `TF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')` and modify the `g++` call to include `-L$TF_LIB -ltensorflow_framework`
TF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')

CUDA_PATH=/usr/local/cuda/
CXXFLAGS=''

if [[ "$OSTYPE" =~ ^darwin ]]; then
	CXXFLAGS+='-undefined dynamic_lookup'
fi

cd roi_pooling_layer

if [ -d "$CUDA_PATH" ]; then
	nvcc -std=c++11 -c -o roi_pooling_op.cu.o roi_pooling_op_gpu.cu.cc \
		-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC $CXXFLAGS \
# sm_61 for 1080Ti
		-arch=sm_61
# add `-L$TF_LIB -ltensorflow_framework` & `-D_GLIBCXX_USE_CXX11_ABI=`
	g++ -std=c++11 -shared -o roi_pooling.so roi_pooling_op.cc \
		roi_pooling_op.cu.o -I $TF_INC  -D GOOGLE_CUDA=1 -fPIC $CXXFLAGS -D_GLIBCXX_USE_CXX11_ABI=0 \
		-lcudart -L $CUDA_PATH/lib64 -L $TF_LIB -ltensorflow_framework
else
	g++ -std=c++11 -shared -o roi_pooling.so roi_pooling_op.cc \
		-I $TF_INC -fPIC $CXXFLAGS
fi

cd ..
```
* ERROR 2
> feed_in, dim = (input, int(input_shape[-1])) TypeError: __int__ returned non-int (type NoneType)
```shell
# https://github.com/smallcorgi/Faster-RCNN_TF/issues/316
# add the following lines to lib/roi_pooling_layer/roi_pooling_op.cc, and make
L27 #include "tensorflow/core/framework/shape_inference.h"
L41-L53
.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      //https://github.com/tensorflow/.../core/framework/shape_inference.h
      int pooled_height;
      int pooled_width;
      c->GetAttr("pooled_height", &pooled_height);
      c->GetAttr("pooled_width", &pooled_width);
      auto pooled_height_h = c->MakeDim(pooled_height);
      auto pooled_width_h = c->MakeDim(pooled_width);

      auto output_shape = c->MakeShape({ c->Dim(c->input(1), 0), pooled_height_h, pooled_width_h, c->Dim(c->input(0), 3) });
      c->set_output(0, output_shape);
      return Status::OK();
    });
```

# 2019.02.13
- [x] **Object Detection and Classification using R-CNNs**
* [link](http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/)
* Network Architecture
![network](http://www.telesens.co/wp-content/uploads/2018/03/img_5a9ffec911c19.png)

# 2019.02.16
* seg featuremap 在voc数据集上输出有为[1, 21, 500, 500],经过np.argmax(axis=0)得到[500, 500]的segmentation image of class IDs，相当于在channel方向上对每个像素进行分类，取channel方向上概率最大值作为该像素类别。

# 2019.02.21 - 2019.02.27
- [x] **MultiNet源码**
* See `11-MultiNet.md`

# 2019.02.27
- [x] **复习 [一文读懂Faster RCNN]**
* See `update 2019.02.27`.
* [Faster-rcnn详解](https://blog.csdn.net/WZZ18191171661/article/details/79439212)


# 2019.02.28
## CSDN翻译专栏
[CSDN翻译专栏](https://blog.csdn.net/quincuntial/article/details/77263607)

# 2019.02.29 - 2019.03.03
## Mask R-CNN
* 专知语义分割专栏
```python
Mask-RCNN [https://arxiv.org/pdf/1703.06870.pdf]

https://github.com/CharlesShang/FastMaskRCNN [Tensorflow]

https://github.com/TuSimple/mx-maskrcnn [MxNet]

https://github.com/matterport/Mask_RCNN [Keras]

https://github.com/jasjeetIM/Mask-RCNN [Caffe]
```

# 2019.03.01
## Mask R-CNN
- [x] [Mask R-CNN详解](https://blog.csdn.net/WZZ18191171661/article/details/79453780)

- [x] **Mask_R-CNN.mp4**

- [x] **maskrcnn_slides.pdf**
* [region-of-interest-pooling-explained](https://deepsense.ai/region-of-interest-pooling-explained/)
* Backbone (`ResNeXt`): +1.6 AP(bbox)

## `RoIPooling` & `RoIAlign`
### `RoIPooling`
1. Let’s consider a small example to see how it works. We’re going to perform region of interest pooling on a single 8×8 feature map, one region of interest and an output size of 2×2. Our input feature map looks like this:

![Feature Map](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/1.jpg)

2. Let’s say we also have a region proposal (top left, bottom right coordinates): (0, 3), (7, 8). In the picture it would look like this:

![region proposal](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/2.jpg)

3. Normally, there’d be multiple feature maps and multiple proposals for each of them, but we’re keeping things simple for the example.
By dividing it into (2×2) sections (because the output size is 2×2) we get:

![2×2 sections](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/3.jpg)

4. The max values in each of the sections are:

![output](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/output.jpg)

5. Here’s our example presented in form of a nice animation:

![gif](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/02/roi_pooling-1.gif)


### `RoIAlign`
1. unquantized (2×2) sections:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi0.png)

2. Sampling locations:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi1.png)

3. Bilinear interpolated values:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi2.png)

4. Max pooling output:

![](https://github.com/kinglintianxia/note_book/blob/master/imgs/roi3.png)

---------------------------------------------------
# 2019.03.02
## Mask R-CNN

###
- [x] **Mask-RCNN-Arc-How-RoI_Pooling-RoI_Warping_RoI-Align_Work.mp4**
* ROI Pooling和ROIAlign最大的区别是：前者使用了两次量化操作，而后者并没有采用量化操作，使用了线性插值算法，具体的解释如下所示:

![ROIPooling](https://img-blog.csdn.net/20180306110240257)
![ROIAlign](https://img-blog.csdn.net/20180306110334767)

###
- [x] **2017-Mask R-CNN.pdf**
* [Mask R-CNN 论文翻译](https://alvinzhu.xyz/2017/10/07/mask-r-cnn/#fn:18)
* [Mask R-CNN完整翻译](https://blog.csdn.net/myGFZ/article/details/79136610)
* `RoIPool` performs coarse spatial quantizationfor feature extraction. To fix the misalignment, we propose a simple, quantization-free layer, called `RoIAlign`, that faithfully preserves exact spatial locations.
*  Second, we found it essential to decouple mask and class prediction: we predict a binary mask for each class independently, without competition among classes, and rely on the network’s RoI classification branch to predict the category. 
* Our models can run at about 200ms per frame on a GPU, and training on COCO takes one to two days on a single 8-GPU machine. 
* Instead, our method is based on `parallel prediction of masks and class labels`, which is simpler and more flexible.
* The `mask branch` has a Km2-dimensional output for `each RoI`, which encodes K binary masks of resolution m × m, one for each of the `K classes`.
* For an RoI associated with ground-truth class k, Lmask is only defined on the k-th mask (other mask outputs do not contribute to the loss).
* we rely on the dedicated classification branch to predict the class label used to `select the output mask`. 
* collapsing it into a `vector representation` that `lacks spatial dimensions`.
* `RoIPool` is a standard operation for extracting a small feature map (e.g., 7×7) from each RoI
* non-maximum suppression
* The `mask branch` can predict K masks per RoI, but we only `use the k-th mask`, where k is the predicted class by the classification branch. The m×m floating-number mask output is then `resized to the RoI size`, and binarized at a threshold of 0.5

* 2016-FCIS.pdf
* 2016-Segment Proposal.pdf
* 2015-DeepMask.pdf

###
- [x] [Mask RCNN笔记](https://blog.csdn.net/xiamentingtao/article/details/78598511)
* ROI Align 的反向传播
> 常规的`ROI Pooling`的反向传播公式如下：

![](http://1.file.leanote.top/59fbd202ab644135b00006fa/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20171103101817.png?e=1551535688&token=ym9ZIrtxjozPN4G9he3-FHPOPxAe-OQmxzol5EOk:HQGMn_aJNffPMlsmOIilYXI1jR8)

> 这里，xi代表池化前特征图上的像素点；yrj代表池化后的第r个候选区域的第j个点；i*(r,j)代表点yrj像素值的来源（最大池化的时候选出的最大像素值所在点的坐标）。由上式可以看出，只有当池化后某一个点的像素值在池化过程中采用了当前点Xi的像素值（即满足i=i*(r，j)），才在xi处回传梯度。
> 类比于ROIPooling，`ROIAlign的反向传播`需要作出稍许修改：首先，在ROIAlign中，xi*（r,j）是一个浮点数的坐标位置(前向传播时计算出来的采样点)，在池化前的特征图中，每一个与 xi*(r,j) 横纵坐标均小于1的点都应该接受与此对应的点yrj回传的梯度，故ROI Align 的反向传播公式如下: 

![](http://1.file.leanote.top/59fbe350ab644137db000a4e/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20171103113216.png?e=1551535689&token=ym9ZIrtxjozPN4G9he3-FHPOPxAe-OQmxzol5EOk:8JFZp9cEc2vf2099pKVB_jqA4rE)

> 上式中，d(.)表示两点之间的距离，Δh和Δw表示 xi 与 xi*(r,j) 横纵坐标的差值，这里作为双线性内插的系数乘在原始的梯度上。

# 2019.03.03
## SE-Net
- [x] **85-ImageNet冠军模型SE-Net详解**
* SE-Net: `channel relationship`; GoogleNet: `spatial relationship`
* Mini-batch data sampling: 按照`类别`，不是按照`图像`进行训练。
* ResNet & SE_Net
![](https://github.com/kinglintianxia/note_book/blob/master/imgs/SE_Net.png)

* GFLOPs

网络        		| GFLOPs
--------   		| -----
ResNet-50  		| 3.86
ResNet-101 		| 7.58
ResNet-152 		| 11.30
BN-Inception  	| 2.03

* ResNet-50 `Conv5层`没必要加`SE`模块



## Conv卷积，边缘检测
- [x] **吴恩达Conv卷积，边缘检测部分**
* 第一周 卷积神经网络-1.2/1.3节
* Why convolutions:
1. Parameter sharing.
2. Sparsity of connections.

### **第三周　目标检测**
* 跑道检测是`classification with localization`问题。
* The Network output is:<br>

|	output		|	meaning					| example 'pedestrian', 'car', 'motorcycle'	|	|	
|	------		|	------					|	------		|	------					|
|	Pc			|	Is there any object		|	1			|	0						|
|	bx			|	bbox center x			|	0.5			|	?	'Don't care'		|
|	by			|	bbox center y			|	0.5			|	?	'Don't care'		|
|	bh			|	bbox height				|	0.4			|	?	'Don't care'		|
|	bw			|	bbox width				|  	0.6			|	?	'Don't care'		|
|	c1			|	class 1 prob			|	0			|	?	'Don't care'		|
|	c2			|	class 2 prob			|	1			|	?	'Don't care'		|
|	c3			|	class 3 prob			|	0			|	?	'Don't care'		|



* The loss may be:

loss				|	case
------				|	------
sum(y^i-yi)^2		|	y0 = 1
(y^0-y0)^2			|	y0 = 0

* [bx, by, bh, bw] are parameted relative to the grid cell.



## YOLO
- [x] **跑通YOLO**
* [YOLO](https://pjreddie.com/darknet/yolo/)
* Darknet is an open source neural network framework written in `C and CUDA`. It is fast, easy to install, and supports `CPU and GPU computation`.

### Build & run
```shell
# Build with CPU
$ git clone https://github.com/pjreddie/darknet
$ cd darknet
$ make
$ wget https://pjreddie.com/media/files/yolov3.weights
# run with CPU
$ ./darknet detect cfg/yolov3.cfg weights/yolov3.weights data/dog.jpg

# Build with GPU
## change the first line of the `Makefile`
$ GPU=1
$ OPENCV=1
$ ARCH= -gencode arch=compute_30,code=sm_30 \
      -gencode arch=compute_35,code=sm_35 \
      -gencode arch=compute_50,code=[sm_50,compute_50] \
      -gencode arch=compute_52,code=[sm_52,compute_52] \
      -gencode arch=compute_61,code=[sm_61,compute_61]

## run with GPU
$ ./darknet detector test cfg/coco.data cfg/yolov3.cfg weights/yolov3.weights data/dog.jpg 
$ ./darknet detect cfg/yolov3.cfg weights/yolov3.weights data/dog.jpg
## use GPU 1
$ ./darknet -i 1 detect cfg/yolov3.cfg weights/yolov3.weights data/dog.jpg
## run with video file
### FPS:18.2
$ ./darknet detector demo cfg/coco.data cfg/yolov3.cfg weights/yolov3.weights data/1.avi 
### FPS:180.9
$ ./darknet detector demo cfg/coco.data cfg/yolov3-tiny.cfg weights/yolov3-tiny.weights data/1.avi

## Train on VOC
### modify 'cfg/yolov3-voc.cfg', uncomment the 'Training' parameters.
# Testing
# batch=1
# subdivisions=1
# Training
 batch=64
 subdivisions=16
###########################################
# max_batches = 50200,迭代次数
$ ./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg  weights/darknet53.conv.74 -gpus 0,1
# Test training
## FPS:47.6
$ ./darknet detector -i 1 demo cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3-voc.backup data/1.avi



## Train on COCO
# max_batches = 500200,迭代次数
$ ./darknet detector train cfg/coco.data cfg/yolov3.cfg weights/darknet53.conv.74 -gpus 0,1
```

* Where `x, y, width, and height` are `relative to the image's width and height`.

----------------------------------------------
# 2019.03.03
## YOLOv3
- [x] [How to implement a YOLO (v3) object detector from scratch in PyTorch: Part 1](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)
* `YOLO`, `SSD`, `Mask RCNN` and `RetinaNet`.

### What is YOLO?
* YOLO makes use of only convolutional layers, making it a fully convolutional network (FCN). It has `75 convolutional layers`, with `skip connections` and `upsampling` layers. `No form of pooling` is used, and a convolutional layer with stride 2 is used to downsample the feature maps. 
* `Being a FCN`, YOLO is invariant to the size of the input image. However, `in practice`, we might want to stick to a `constant input size`:
> Process our images in `batches` (images in batches can be processed in parallel by the GPU, leading to speed boosts), we need to have all images of fixed height and width. 
* The network `downsamples` the image by a factor called the **stride** of the network.

### Interpreting the output
* Now, the first thing to notice is `our output is a feature map`.
* `Depth-wise`, we have `(B x (5 + C)) entries` in the feature map.
> `B` represents the number of bounding boxes each cell can predict.  Each of the bounding boxes have 5 + C attributes, which describe the `center coordinates`, the `dimensions`, the `objectness score` and `C class confidences` for each bounding box. YOLO v3 predicts `3 bounding boxes for every cell`.
* To do that, we `divide` the `input image` into `a grid of dimensions` equal to that of `the final feature map`.
> Let us consider an example below, where the input image is 416 x 416, and stride of the network is 32. As pointed earlier, the dimensions of the feature map will be 13 x 13. We then divide the input image into 13 x 13 cells.

![](https://blog.paperspace.com/content/images/2018/04/yolo-5.png)
> Then, the cell (on the input image) containing `the center of the ground truth box` of an object is chosen to be the one `responsible for predicting the object`. In the image, it is the cell which marked `red`, which contains the center of the ground truth box (marked yellow).
* We divide the `input image` into a `grid` just to determine which cell of the prediction `feature map` is `responsible for prediction`.

### Anchor Boxes
* It might make sense to predict the width and the height of the bounding box, but `in practice`, that leads to `unstable gradients` during training. `Instead`, most of the modern object detectors predict log-space transforms, or `simply` offsets to pre-defined default bounding boxes called `anchors`.
* The bounding box `responsible for detecting the dog` will be the one whose anchor has the `highest IoU` with the `ground truth box`.

### Making Predictions
* The following formulae describe how the network output is transformed to obtain bounding box predictions.

！[](https://blog.paperspace.com/content/images/2018/04/Screen-Shot-2018-04-10-at-3.18.08-PM.png)
> `bx, by, bw, bh` are the x,y center cvoc persono-ordinates(坐标), width and height of our prediction. `tx, ty, tw, th` is what the `network outputs`. `cx and cy` are the top-left co-ordinates of the grid. `pw and ph` are anchors dimensions for the box.

### Center Coordinates
* Notice we are running our `center coordinates prediction` through a `sigmoid` function. This forces the value of the `output` to be between `0 and 1`. 
> For example, consider the case of our `dog image`. If the `prediction for center` is (0.4, 0.7), then this means that the center lies at (6.4, 6.7) on the 13 x 13 feature map. (Since the top-left co-ordinates of the red cell are (6,6)).

### Dimensions of the Bounding Box
* The `dimensions(bw, bh)` of the bounding box are predicted by applying a log-space transform to the `output(tw, th)` and then multiplying with an `anchor(pw, ph)`.
$$ {bw = pw*e^{tw}}, {bh = ph*e^{th}}  $$

![](https://blog.paperspace.com/content/images/2018/04/yolo-regression-1.png)

### Objectness Score
* `Object score` represents the `probability` that an object is contained inside a bounding box. It should be nearly 1 for the red and the neighboring grids, whereas almost 0 for, say, the grid at the corners.

* The `objectness score` is also passed through a `sigmoid`, as it is to be interpreted as a `probability`.

### Class Confidences
* Class confidences represent the probabilities of the detected object belonging to a particular class (Dog, cat, banana, car etc).
* Before v3, YOLO used to `softmax` the class scores.
* In v3, and authors have opted for using `sigmoid` instead.
* The reason is that `Softmaxing` class scores assume that the classes are `mutually exclusive`(相互排斥).
> In simple words, if an object belongs to one class, then it's guaranteed it cannot belong to another class. This is true for COCO database on which we will base our detector.

> `However`, this assumptions may not hold when we have classes like `Women` and `Person`. This is the reason that authors have steered clear of using a Softmax activation.

### Prediction across different scales.
* YOLO v3 makes prediction across `3 different scales`. The detection layer is used make detection at feature maps of three different sizes, having `strides 32, 16, 8 respectively`. This means, with an `input of 416 x 416`, we make detections on `scales 13 x 13, 26 x 26 and 52 x 52`.
* The network downsamples the input image until the first detection layer, where a detection is made using feature maps of a layer with strivoc personde 32. Further, layers are upsampled by a factor of 2 and concatenated with feature maps of a previous layers having identical feature map sizes. Another detection is now made at layer with stride 16. The same upsampling procedure is repeated, and a final detection is made at the layer of stride 8.

![](https://blog.paperspace.com/content/images/2018/04/yolo_Scales-1.png)

* `Upsampling` can help the network learn `fine-grained(细粒度) features` which are instrumental for detecting `small objects`.

### Output Processing
* For an image of size `416 x 416`, YOLO predicts ((52 x 52) + (26 x 26) + 13 x 13)) x 3 = `10647 bounding boxes`. 
* Thresholding by Object Confidence.
* Non-maximum Suppression.

------------------------
# 2019.03.05
## YOLOv3
- [x] [How to implement a YOLO (v3) object detector from scratch in PyTorch: Part 2](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-2/)
* Configuration File(cfg/文件下)详细解析。
```python
[net]
# Testing
batch=1
subdivisions=1
# Training
# batch=64
# subdivisions=16
```

- [x] [YOLOv3网络结构细致解析](https://blog.csdn.net/sum_nap/article/details/80568873)

### layer filters size input output

#### VOC dataset
```python
0 conv 32 3 x 3 / 1  416 x 416 x 3 -> 416 x 416 x 32  0.299 BFLOPs	
1 conv 64 3 x 3 / 2  416 x 416 x 32 -> 208 x 208 x 64  1.595 BFLOPs	
2 conv 32 1 x 1 / 1 208 x 208 x 64 -> 208 x 208 x 32 0.177 BFLOPs
3 conv 64 3 x 3 / 1 208 x 208 x 32 -> 208 x 208 x 64 1.595 BFLOPs
4 res 1 208 x 208 x 64 -> 208 x 208 x 64
5 conv 128 3 x 3 / 2 208 x 208 x 64 -> 104 x 104 x 128 1.595 BFLOPs
6 conv 64 1 x 1 / 1 104 x 104 x 128 -> 104 x 104 x 64 0.177 BFLOPs
7 conv 128 3 x 3 / 1 104 x 104 x 64 -> 104 x 104 x 128 1.595 BFLOPs
8 res 5 104 x 104 x 128 -> 104 x 104 x 128
9 conv 64 1 x 1 / 1 104 x 104 x 128 -> 104 x 104 x 64 0.177 BFLOPs
10 conv 128 3 x 3 / 1 104 x 104 x 64 -> 104 x 104 x 128 1.595 BFLOPs
11 res 8 104 x 104 x 128 -> 104 x 104 x 128
12 conv 256 3 x 3 / 2 104 x 104 x 128 -> 52 x 52 x 256 1.595 BFLOPs
13 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
14 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
15 res 12 52 x 52 x 256 -> 52 x 52 x 25voc person6
16 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
17 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
18 res 15 52 x 52 x 256 -> 52 x 52 x 256
19 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
20 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
21 res 18 52 x 52 x 256 -> 52 x 52 x 256
22 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
23 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
24 res 21 52 x 52 x 256 -> 52 x 52 x 256
25 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
26 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
27 res 24 52 x 52 x 256 -> 52 x 52 x 256
28 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
29 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
30 res 27 52 x 52 x 256 -> 52 x 52 x 256
31 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
32 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
33 res 30 52 x 52 x 256 -> 52 x 52 x 256
34 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
35 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
36 res 33 52 x 52 x 256 -> 52 x 52 x 256
37 conv 512 3 x 3 / 2 52 x 52 x 256 -> 26 x 26 x 512 1.595 BFLOPs
38 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
39 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
40 res 37 26 x 26 x 512 -> 26 x 26 x 512
41 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
42 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
43 res 40 26 x 26 x 512 -> 26 x 26 x 512
44 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
45 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
46 res 43 26 x 26 x 512 -> 26 x 26 x 512
47 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
48 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
49 res 46 26 x 26 x 512 -> 26 x 26 x 512
50 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
51 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
52 res 49 26 x 26 x 512 -> 26 x 26 x 512
53 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
54 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
55 res 52 26 x 26 x 512 -> 26 x 26 x 512
56 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
57 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
58 res 55 26 x 26 x 512 -> 26 x 26 x 512
59 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
60 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
61 res 58 26 x 26 x 512 -> 26 x 26 x 5voc person12
62 conv 1024 3 x 3 / 2 26 x 26 x 512 -> 13 x 13 x1024 1.595 BFLOPs
63 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
64 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
65 res 62 13 x 13 x1024 -> 13 x 13 x1024
66 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
67 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
68 res 65 13 x 13 x1024 -> 13 x 13 x1024
69 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
70 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
71 res 68 13 x 13 x1024 -> 13 x 13 x1024
72 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
73 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
74 res 71 13 x 13 x1024 -> 13 x 13 x1024
75 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
76 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
77 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
78 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
79 conv 512 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 512 0.177 BFLOPs
80 conv 1024 3 x 3 / 1 13 x 13 x 512 -> 13 x 13 x1024 1.595 BFLOPs
81 conv 75 1 x 1 / 1 13 x 13 x1024 -> 13 x 13 x 75 0.026 BFLOPs
82 yolo
83 route 79
84 conv 256 1 x 1 / 1 13 x 13 x 512 -> 13 x 13 x 256 0.044 BFLOPs
85 upsample 2x 13 x 13 x 256 -> 26 x 26 x 256
86 route 85 61
87 conv 256 1 x 1 / 1 26 x 26 x 768 -> 26 x 26 x 256 0.266 BFLOPs
88 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
89 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
90 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
91 conv 256 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 256 0.177 BFLOPs
92 conv 512 3 x 3 / 1 26 x 26 x 256 -> 26 x 26 x 512 1.595 BFLOPs
93 conv 75 1 x 1 / 1 26 x 26 x 512 -> 26 x 26 x 75 0.052 BFLOPs
94 yolo
95 route 91
96 conv 128 1 x 1 / 1 26 x 26 x 256 -> 26 x 26 x 128 0.044 BFLOPs
97 upsample 2x 26 x 26 x 128 -> 52 x 52 x 128
98 route 97 36
99 conv 128 1 x 1 / 1 52 x 52 x 384 -> 52 x 52 x 128 0.266 BFLOPs
100 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
101 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
102 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
103 conv 128 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 128 0.177 BFLOPs
104 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs
105 conv 75 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 75 0.104 BFLOPs
106 yolo
```

#### COCO dataset
```python
layer     filters    size              input                output
    0 conv     32  3 x 3 / 1   608 x 608 x   3   ->   608 x 608 x  32  0.639 BFLOPs
    1 conv     64  3 x 3 / 2   608 x 608 x  32   ->   304 x 304 x  64  3.407 BFLOPs
    2 conv     32  1 x 1 / 1   304voc person x 304 x  64   ->   304 x 304 x  32  0.379 BFLOPs
    3 conv     64  3 x 3 / 1   304 x 304 x  32   ->   304 x 304 x  64  3.407 BFLOPs
    4 res    1                 304 x 304 x  64   ->   304 x 304 x  64
    5 conv    128  3 x 3 / 2   304 x 304 x  64   ->   152 x 152 x 128  3.407 BFLOPs
    6 conv     64  1 x 1 / 1   152 x 152 x 128   ->   152 x 152 x  64  0.379 BFLOPs
    7 conv    128  3 x 3 / 1   152 x 152 x  64   ->   152 x 152 x 128  3.407 BFLOPs
    8 res    5                 152 x 152 x 128   ->   152 x 152 x 128
    9 conv     64  1 x 1 / 1   152 x 152 x 128   ->   152 x 152 x  64  0.379 BFLOPs
   10 conv    128  3 x 3 / 1   152 x 152 x  64   ->   152 x 152 x 128  3.407 BFLOPs
   11 res    8                 152 x 152 x 128   ->   152 x 152 x 128
   12 conv    256  3 x 3 / 2   152 x 152 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   13 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   14 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   15 res   12                  76 x  76 x 256   ->    76 x  76 x 256
   16 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   17 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   18 res   15                  76 x  76 x 256   ->    76 x  76 x 256
   19 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   20 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   21 res   18                  76 x  76 x 256   ->    76 x  76 x 256
   22 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   23 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   24 res   21                  76 x  76 x 256   ->    76 x  76 x 256
   25 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   26 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   27 res   24                  76 x  76 x 256   ->    76 x  76 x 256
   28 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   29 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   30 res   27                  76 x  76 x 256   ->    76 x  76 x 256
   31 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   32 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   33 res   30                  76 x  76 x 256   ->    76 x  76 x 256
   34 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
   35 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
   36 res   33                  76 x  76 x 256   ->    76 x  76 x 256
   37 conv    512  3 x 3 / 2    76 x  76 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   38 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   39 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   40 res   37                  38 x  38 x 512   ->    38 x  38 x 512
   41 conv    256  1 x 1 / 1    38voc person x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   42 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   43 res   40                  38 x  38 x 512   ->    38 x  38 x 512
   44 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   45 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   46 res   43                  38 x  38 x 512   ->    38 x  38 x 512
   47 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   48 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   49 res   46                  38 x  38 x 512   ->    38 x  38 x 512
   50 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   51 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   52 res   49                  38 x  38 x 512   ->    38 x  38 x 512
   53 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   54 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   55 res   52                  38 x  38 x 512   ->    38 x  38 x 512
   56 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   57 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   58 res   55                  38 x  38 x 512   ->    38 x  38 x 512
   59 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   60 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   61 res   58                  38 x  38 x 512   ->    38 x  38 x 512
   62 conv   1024  3 x 3 / 2    38 x  38 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   63 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   64 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   65 res   62                  19 x  19 x1024   ->    19 x  19 x1024
   66 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   67 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   68 res   65                  19 x  19 x1024   ->    19 x  19 x1024
   69 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   70 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   71 res   68                  19 x  19 x1024   ->    19 x  19 x1024
   72 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   73 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   74 res   71                  19 x  19 x1024   ->    19 x  19 x1024
   75 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   76 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   77 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   78 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   79 conv    512  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs
   80 conv   1024  3 x 3 / 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs
   81 conv    255  1 x 1 / 1    19 x  19 x1024   ->    19 x  19 x 255  0.189 BFLOPs
   82 yolo
   83 route  79
   84 conv    256  1 x 1 / 1    19 x  19 x 512   ->    19 x  19 x 256  0.095 BFLOPs
   85 upsample            2x    19 x  19 x 256   ->    38 x  38 x 256
   86 route  85 61
   87 conv    256  1 x 1 / 1    38 x  38 x 768   ->    38 x  38 x 256  0.568 BFLOPs
   88 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   89 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   90 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   91 conv    256  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs
   92 conv    512  3 x 3 / 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs
   93 conv    255  1 x 1 / 1    38 x  38 x 512   ->    38 x  38 x 255  0.377 BFLOPs
   94 yolo
   95 route  91
   96 conv    128  1 x 1 / 1    38 x  38 x 256   ->    38 x  38 x 128  0.095 BFLOPs
   97 upsample            2x    38 x  38 x 128   ->    76 x  76 x 128
   98 route  97 36
   99 conv    128  1 x 1 / 1    76 x  76 x 384   ->    76 x  76 x 128  0.568 BFLOPs
  100 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
  101 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
  102 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
  103 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs
  104 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs
  105 conv    255  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 255  0.754 BFLOPs
  106 yolo
```


### 从75到105层我为yolo网络的特征交互层，分为`三个尺度`，每个尺度内，通过卷积核的方式实现局部的特征交互，作用类似于全连接层但是是通过卷积核（3*3和1*1）的方式实现feature map之间的局部特征（fc层实现的是全局的特征交互）交互。

1. 最小尺度yolo层：
> 输入：`13*13`的feature map(74 res) ，一共1024个通道。
> 操作：一系列的卷积操作，feature map的大小不变，但是通道数最后减少为75个。
> 输出；输出13*13大小的feature map，`75`个通道，在此基础上进行分类和位置回归。
> VOC dataset:[tx,ty,tw,th,Objectness Score(, Class Confidences(20)]*3 = [4,1,20]*3 = 25*3 = 75.

2. 中尺度yolo层：
> 输入：将`79层`的13*13,512通道的feature map进行卷积操作,生成13*13、256通道的feature map,然后进行上采样,生成26*26 256通道的feature map,同时于`61层`的26*26、512通道的中尺度的feature map合并。再进行一系列卷积操作，
> 操作：一系列的卷积操作，feature map的大小不变，但是通道数最后减少为75个。
> 输出：26*26大小的feature map，75个通道，然后在此进行分类和位置回归。

3. 大尺度的yolo层：
> 输入：将`91层`的26*26、256通道的feature map进行卷积操作，生成26*26、128通道的feature map，然后进行上采样生成52*52、128通道的feature map，同时于`36层`的52*52、256通道的中尺度的feature map合并。再进行一系列卷积操作，
> 操作：一系列的卷积操作，feature map的大小不变，但是通道数最后减少为75个。
> 输出：52*52大小的feature map，75个通道，然后在此进行分类和位置回归。
------------------------------------------ 

## YOLOv3
- [x] [yolo系列之yolo v3 深度解析](https://blog.csdn.net/leviopku/article/details/82660381)
* yolo_v3结构图(COCO dataset):

![](https://img-blog.csdn.net/2018100917221176?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xldmlvcGt1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
> `DBL`: 如图1左下角所示，也就是代码中的Darknetconv2d_BN_Leaky，是yolo_v3的基本组件。就是卷积+BN+Leaky relu。对于v3来说，BN和leaky relu已经是和卷积层不可分离的部分了(最后一层卷积除外)，共同构成了最小组件。
> `resn`：n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。这是yolo_v3的大组件，yolo_v3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深(从v2的darknet-19上升到v3的darknet-53，前者没有残差结构)。对于res_block的解释，可以在图1的右下角直观看到，其基本组件也是DBL。
> `concat`：张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。
> COCO dataset:[tx,ty,tw,th,Objectness Score(, Class Confidences(80)]*3 = [4,1,80]*3 = 85*3 = 255.

* [模型结构可视化(vis)工具](https://github.com/lutzroeder/Netron)
* 轻量化网络: SqueezeNet.
* v3毫无疑问现在成为了工程界首选的检测算法之一了，结构清晰，实时性好。
* `疑问`:输出y1,y2,y3的预测是叠加一起成为最后的输出的吗?
* loss function
```python
xy_loss = object_mask * box_loss_scale * K.`binary_crossentropy`(raw_true_xy, raw_pred[..., 0:2],
                                                                       from_logits=True)
wh_loss = object_mask * box_loss_scale * 0.5 * K.`square`(raw_true_wh - raw_pred[..., 2:4])
confidence_loss = object_mask * K.`binary_crossentropy`(object_mask, raw_pred[..., 4:5], from_logits=True) + \
                          (1 - object_mask) * K.binary_crossentropy(object_mask, raw_pred[..., 4:5],
                                                                    from_logits=True) * ignore_mask
class_loss = object_mask * K.`binary_crossentropy`(true_class_probs, raw_pred[..., 5:], from_logits=True)

xy_loss = K.sum(xy_loss) / mf
wh_loss = K.sum(wh_loss) / mf
confidence_loss = K.sum(confidence_loss) / mf
class_loss = K.sum(class_loss) / mf
`loss += xy_loss + wh_loss + confidence_loss + class_loss`

```
> 除了w, h的损失函数依然采用`总方误差`之外，其他部分的损失函数用的是`二值交叉熵`.


## YOLOv3
- [x] [基于keras-yolov3，原理及代码细节的理解](https://blog.csdn.net/KKKSQJ/article/details/83587138)
### anchor box:
* yolov3 anchor box一共有9个，由k-means聚类得到。在COCO数据集上，9个聚类是：（10*13）;（16*30）;（33*23）;（30*61）;（62*45）; （59*119）; （116*90）; （156*198）; （373*326）。
> 不同尺寸特征图对应不同大小的先验框。

    13*13feature map对应[（116*90），（156*198），（373*326）]
    26*26feature map对应[（30*61），（62*45），（59*119）]
    52*52feature map对应[（10*13），（16*30），（33*23）]

原因：特征图越大，感受野越小。对小目标越敏感，所以选用小的anchor box。

     特征图越小，感受野越大。对大目标越敏感，所以选用大的anchor box。

### 边框预测：
1. 预测tx ty tw th
* 对tx和ty进行`sigmoid`，并加上对应的offset（Cx, Cy）.
* 对th和tw进行exp，并乘以对应的锚点值(pw,ph).
* 对tx,ty,th,tw乘以对应的步幅，即：416/13, 416/26, 416/52.
* 最后，使用sigmoid对Objectness和Classes confidence进行sigmoid得到0~1的概率，`之所以用sigmoid取代之前版本的softmax，原因是softmax会扩大最大类别概率值而抑制其他类别概率值`.

![边框预测](https://img2018.cnblogs.com/blog/1505200/201810/1505200-20181030204835020-902505029.png)
> (tx,ty) :目标中心点相对于该点所在网格左上角的偏移量，经过sigmoid归一化。即值属于[0,1]。如图约（0.3 , 0.4）
> (cx,cy):该点所在网格的左上角距离最左上角相差的格子数。如图（1,1）
> `(pw,ph):anchor box 的边长`
> (tw,th):预测边框的宽和高, `maybe > 1`
> PS：最终得到的边框坐标值是bx,by,bw,bh.而网络学习目标是tx,ty,tw,th


## YOLOv3
- [ ] [YOLO从零开始：基于YOLOv3的行人检测入门指南](https://zhuanlan.zhihu.com/p/47196727)
### 通过`voc_label.py`转化`voc数据`格式为`yolo支持`的格式.
### 10、性能检测
* 计算mAp
> ./darknet detector map cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3-voc_80172.weights
* 计算recall（2097张的结果）
> ./darknet detector recall cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3-voc_final.weights
* VOC2007test
```shell
# (会在/results生成默认的comp4_det_test_person.txt，这是在VOC2007 test上的结果)
$ ./darknet detector valid cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3-voc_final.weights -gpu 0,1
```

### VOC的图片格式
* 行列分布同pillow.Image，先行后列

### 训练过程
* 训练迭代数：`8w iters`
* [训练技巧](https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects)
* [**yolov3训练的集大成者**](https://blog.csdn.net/lilai619/article/details/79695109)
```python
Region xx: cfg文件中yolo-layer的索引；

Avg IOU:   当前迭代中，预测的box与标注的box的平均交并比，越大越好，期望数值为1；

Class:        标注物体的分类准确率，越大越好，期望数值为1；

obj:            越大越好，期望数值为1；

No obj:      越小越好；

.5R:            以IOU=0.5为阈值时候的recall; recall = 检出的正样本/实际的正样本

0.75R:         以IOU=0.75为阈值时候的recall;

count:        正样本数目。
```
![cfg](https://img-blog.csdn.net/20180430171058652)

### VOC数据集网络模型

![网络模型](https://img-blog.csdn.net/20180608100212649)

### 模型什么时候保存？
* 迭代次数小于1000时，每100次保存一次，大于1000时，没10000次保存一次。

### 图片上添加置信值
* 代码比较熟悉的童鞋，使用opencv在画框的函数里面添加一下就行了。

## Paper Reading
- [x] [YOLOv3 全文翻译](https://zhuanlan.zhihu.com/p/34945787)

* `Darknet` is an open source neural network framework written in `C and CUDA`. It is fast, easy to install, and supports `CPU and GPU computation`. 

* 优点：速度快，精度提升，小目标检测有改善；
* 不足：中大目标有一定程度的削弱，遮挡漏检，速度稍慢于V2。

* 哥们论文写的太随意了.

-------------------------------
## YOLOv3 train voc only person
1. 通过`ubuntu/datasets/VOC/extract_person_2007/2012.py`提取含人数据,生成文件：
```shell
$ cd /media/jun/ubuntu/datasets/VOC/
$ python extract_person_2007.py
## 'ubuntu/datasets/VOC/VOCdevkit/VOC2007/ImageSets/Main/train_person.txt'
## 'ubuntu/datasets/VOC/VOCdevkit/VOC2007/ImageSets/Main/test_person.txt'
$ python extract_person_2012.py
## 'ubuntu/datasets/VOC/VOCdevkit/VOC2012/ImageSets/Main/train_person.txt'
```

2. 通过`voc_label_person`转化voc数据格式为yolo支持的格式
```shell
$ python voc_label_person.py
## 'ubuntu/datasets/VOC/2007_train_person.txt'
## 'ubuntu/datasets/VOC/2007_test_person.txt'
## 'ubuntu/datasets/VOC/2012_train_person.txt'
```
3. 整合下训练集、测试集：
```shell
$ cat 2007_train_person.txt 2012_train_person.txt > train_person.txt
```

4. 配置`data/voc_person.names`
```python
person
```

5. 配置`cfg/voc_person.data`
```python
classes= 1
train  = /media/jun/ubuntu/datasets/VOC/train_person.txt
valid  = /media/jun/ubuntu/datasets/VOC/2007_test_person.txt
names = data/voc_person.names
backup = backup_person
```

6. 配置`cfg/yolov3-voc-person.cfg`
```python
# 1. 一共三处
# filters=75
`filters=18`
activation=linear

[yolo]
mask = 6,7,8
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
`classes=1`

# 2. 迭代次数
max_batches = 80200

# 3. uncomment the 'Training' parameters.
# Testing
# batch=1
# subdivisions=1
# Training
 batch=64
 subdivisions=16
```

7. 训练
```shell
$ ./darknet detector train cfg/voc-person.data cfg/yolov3-voc-person.cfg weights/darknet53.conv.74 -gpus 0,1 |tee -a backup_person/train_voc.txt
```

8. 用脚本`analyse.py`对`训练日志train7-loss.txt`的训练过程可视化。


# ==TODO==


- [ ] **DANet**



# 2019.02.29 - 2019.03.03
- [ ] **DeepLab V3+论文代码**

- [ ] [UNIX Tutorial for Beginners](http://www.ee.surrey.ac.uk/Teaching/Unix/)
- [ ] [完全解析RNN, Seq2Seq, Attention注意力机制](https://zhuanlan.zhihu.com/p/51383402)
.
