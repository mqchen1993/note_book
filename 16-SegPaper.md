# 1. 修改deeplab，简化语义标签，提高实时性。
# 2. 在MultiNet基础上修改，完成道路分割、车辆、行人等检测。
# 3. 打破FCN Encoder-Decoder架构,设计one step end-to-end网络。预测语义分割每一类物体在图片上的像素轮廓，而不是端到端输出图片。
# 4. 将pooling层换成边缘检测层试一下效果。
# 5. 注意力模型
# 6. 现在训练网络都是让网络“猜dog在哪里？在这里”(前向传播), "不对，你离正确答案多远"（计算loss）；“我再猜一下”（反向传播）；能不能设计一种训练结构，告诉网络“这张图中dog在哪里”。
# 7. 在分割网络中加入ROI-Pooling层
* Input -> ROI Pooling -> FCN
## 使用ResNet实现的分割网络效果state-of-art. 

------------------
# 2019.01.08
- [x] **跑通KittiSeg evaluate.py程序**	
- [x] **整理KittiSeg和MultiNet代码**	

# 2019.01.09(评价指标)
- [x] **阅读MaxF1论文(THE KITTI-ROAD DATASET)**
* [blog1](https://blog.csdn.net/sinat_28576553/article/details/80258619)
* ego-lane vs.opposing lane,(自我车道与对方车道)
* 2D Bird’s Eye View (BEV) space.
* For methods that output confidence maps (in contrast to binary road classification), the classification threshold τ is chosen to maximize the F-measure.

- [x] **IOU**
* See KittiSeg code.

# 2019.01.13
- [x] **评价MultNet训练结果**
```python
segmentation Evaluation Finished. Results
Raw Results:
[train] MaxF1 (raw)    :  98.9767 
[train] BestThresh (raw)    :  68.2353 
[train] Average Precision (raw)    :  92.5437 
[val] MaxF1 (raw)    :  95.9975 
[val] BestThresh (raw)    :  14.9020 
[val] Average Precision (raw)    :  92.3125 
Speed (msec) (raw)    :  42.8566 
Speed (fps) (raw)    :  23.3336
```

- [x] **读AP指标（VOC DATASET）**
* [AP wiki](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision)
* [blog1](https://blog.csdn.net/niaolianjiulin/article/details/53098437)
* [blog2](https://blog.csdn.net/hysteric314/article/details/54093734)
* 以recall为横坐标（0~1),precision为纵坐标（0~1）作图。得到一条曲线。该曲线下的面积即为AP.
$$ AP = \int_0^1 {p(r)} dr $$ 

# 2019.01.14
- [x] **标注Cityscape数据集**
* [blog1](https://blog.csdn.net/fabulousli/article/details/78633531)
* [labelme 工具](https://github.com/wkentaro/labelme) 

# 2019.01.15
- [x] **发现好博客**
* [深度学习数据集介绍及相互转换](https://www.cnblogs.com/ranjiewen/p/9860953.html)

- [x] **labelme 工具生成Kitti Road数据集**
* img.putpalette([0,0,0,0,255,0,0,0,255])
	
- [x] **labelme 工具生成CityScapes数据集：**
* from cityscapesscripts.helpers.labels     import name2label 

- [ ] **cityscapesScripts标注自己图片**

# 2019.01.16
- [x] **[Semantic Segmentation using Fully Convolutional Networks over the years]**
* [link](https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html)
* [visualization of FCN](http://ethereon.github.io/netscope/#/preset/fcn-8s-pascal)
* [vis of common networks](http://ethereon.github.io/netscope/quickstart.html)
* [Deconvolutions](https://distill.pub/2016/deconv-checkerboard/)
* LinkNet: A feature map with shape [H, W, n_channels] is first convolved with a 1*1 kernel to get a feature map with shape [H, W, n_channels / 4 ] and then a deconvolution takes it to [2*H, 2*W, n_channels / 4 ] a final 1*1 kernel convolution to take it to [2*H, 2*W, n_channels / 2 ]. Thus the decoder block fewer parameters due to this channel reduction scheme.

# 2019.01.20
- [x] **[Deconvolutions]**
* [link](https://distill.pub/2016/deconv-checkerboard/)
* When we look very closely at `images generated by neural networks`, we often see a strange `checkerboard pattern` of artifacts.
* `In theory`, our models could learn to carefully write to unevenly overlapping positions so that the output is evenly balanced; `In fact`, not only do models with uneven overlap not learn to avoid this, but models with even overlap often learn kernels that cause similar artifacts! 
* Better Upsampling
> `One approach` is to make sure you use `a kernel size that is divided by your stride`, avoiding the overlap issue.
> Another approach is `resize the image (using nearest-neighbor interpolation or bilinear interpolation) and then do a convolutional layer`.
* We don’t necessarily think that either approach is the final solution to upsampling, but they do fix the checkerboard artifacts.
* Whenever we `compute the gradients of a convolutional layer`, we do `deconvolution` (transposed convolution) on the `backward pass`.

- [x] **[2017-course_by_Tao Kong.pdf]**
* The region proposal network is a FCN which outputs `K*(4+2) sized vectors`.
* `Mask R-CNN` = Faster R-CNN with FCN on ROIs.
* Useful links for learning detection:
> RCNN/Fast R-CNN/Faster R-CNN: https://github.com/rbgirshick
> YOLO/YOLOv2: https://pjreddie.com/darknet/yolo/
> SSD: https://github.com/weiliu89/caffe/tree/ssd
> R-FCN: https://github.com/daijifeng001/R-FCN
> Tensorflow detector:https://github.com/tensorflow/models/tree/master/research/object_detection

# 2019.01.21
- [x] **基于深度卷积神经网络的目标检测算法研究_黄莉芝.caj**
* 注意力模型
* 语义分割分为“阈值分割”，“边缘分割”， “区域分割”
* 计算边框回归

# 2019.01.22
- [x] **看第四章“目标定位优化”** 

- [x] **专知[目标检测专栏]**
* [link](https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247485072&idx=2&sn=e9f4f0d1daeb3a144e37fcfcc61e908f&chksm=fc85e783cbf26e95d153af7c825ef9b8fc4fc6fd37f75709de98eae769e6b70f8da29ae65a73&scene=21#wechat_redirect)
* [CVPR'17 Tutorial](http://deeplearning.csail.mit.edu/)
* [图像目标检测（Object Detection）原理与实现 （1-6）](https://blog.csdn.net/marvin521/article/details/9058735)
* 下载了[基于特征共享的高效物体检测_任少卿.pdf]
* 下载了[Bounding-box_regression详解.pdf]

- [x] **[RCNN, Fast-RCNN, Faster-RCNN的一些事]**
* [link](http://closure11.com/rcnn-fast-rcnn-faster-rcnn%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/)
* 在Fast-RCNN中，作者巧妙的把bbox regression放进了神经网络内部，与region分类和并成为了一个multi-task模型
* Bounding-box Regression
* 我的CVPR 2016论文里自己弄的一个数据集就借助了Fast-RCNN.（`可以利用现有分割网络进行数据集粗略分割`）

- [x] **[目标检测——从RCNN到Faster RCNN 串烧]**
* [link](https://blog.csdn.net/xyy19920105/article/details/50817725)

# 2019.01.24 计划
- [x] **[机器学习随笔]**
* [link](https://zhuanlan.zhihu.com/p/35058212)

- [x] **[图像语义分割+FCN/U-Net]**
* [link](https://zhuanlan.zhihu.com/p/31428783)
* [visualization of FCN](http://ethereon.github.io/netscope/#/preset/fcn-8s-pascal)
* 对于FCN-8s，首先进行pool4+2x upsampled feature`逐点相加`，然后又进行pool3+2x upsampled`逐点相加`，即进行更多次特征融合。
* 为了解决图像过小后 1/32 下采样后输出feature map太小情况，FCN原作者在第一个卷积层`conv1_1加入pad=100`。
* 在特征融合的时候，如何保证逐点相加的feature map是一样大的呢？这就要引入crop层了
* 在caffe中，存储数据的方式为：caffe blob = [num, channel, height, width]
1. 而score_pool4c设置了axis=2，相当于从第2维(index start from 0!)往后开始裁剪，即裁剪height和width两个维度，同时不改变num和channel纬度
2. 同时设置crop在height和width纬度的开始点为offset=5
3. 用Python语法表示，相当于score_pool4c层的输出为：
```python
crop-pool4 = score-pool4[:, :, 5:5+score2.height, 5:5+score2.width]
```
* `Deconvelution`计算：`score-fr`[1,21,16,16] -> `score2` [1,21,34,34] , kernel_size:4, stride:2
> Conv: $$ out = (in+2*pad-kernel)/stride + 1 $$
> DeConv: $$ out = (in-1)*stride + kernel -2*pad $$
> And Now: $$ 34 = (16-1)*2+4-2*0 $$

# 2019.01.25 
- [x] **[FCN学习:Semantic Segmentation]**
* [link](https://zhuanlan.zhihu.com/p/22976342)
* Faster-RCNN中使用了RPN(Region Proposal Network)替代Selective Search等产生候选区域的方法。`RPN是一种全卷积网络`，所以为了透彻理解这个网络，首先学习一下FCN
* [Caffe 中如何计算卷积](https://www.zhihu.com/question/28385679)
1. Input feature to Matrix
![Input feature to Matrix](https://pic1.zhimg.com/80/v2-701705db7504bb24f859122545b23174_hd.png)
2. Filters to matrix
![Filters to matrix](https://pic1.zhimg.com/80/v2-339657291663a4e791a9b34952d5859c_hd.png)
3. `Filter Matrix`乘以`Feature Matrix`的转置,得到输出矩阵`Cout x (H x W)`,就可以解释为输出的三维Blob`(Cout x H x W)`
4. Detail sample.
![Caffe conv](https://pic4.zhimg.com/80/a6421bae22236c0509623b8b7f7bbb03_hd.jpg)
5. 多.通道卷积计算方式
![多通道卷积](https://pic1.zhimg.com/80/v2-8d72777321cbf1336b79d839b6c7f9fc_hd.jpg)
* 矩阵微分公式:
$$ \frac {d(Ax+b)}{dx} = {A^T}$$

# 2019.01.26
- [x] **[一文读懂Faster RCNN]**
* [link](https://zhuanlan.zhihu.com/p/31426458)
* ![Faster R-CNN](https://pic4.zhimg.com/80/v2-e64a99b38f411c337f538eb5f093bdf3_hd.jpg)
* bounding box regression原理
* 对于一副任意大小PxQ图像,传入Faster RCNN前首先reshape到固定MxN,im_info=[M, N, scale_factor]则保存了此次缩放的所有信息,然后经过Conv Layers,经过4次pooling变为WxH=(M/16)x(N/16)大小,其中feature_stride=16则保存了该信息,用于计算anchor偏移量.

# 2019.01.27
- [x] **GDB调试工具**
* [RMS's gdb Debugger Tutorial](http://www.unknownroad.com/rtfm/gdbtut/)
```shell
$ gcc -g inf.c			# Compile the program with debugging flags.
$ gdb a.out				# Lets load up gdb.
$ run & ctrl+c			# Set off the infinite loop, then press `Ctrl-C` to send the program a SIGINT. 
$ backtrace				# We will use the `backtrace` command to examine the stack
$ frame 1
```

- [ ] **gdb 调试入门**
* [gdb 调试入门](http://blog.jobbole.com/107759/)

## Qt学习
- [ ] **Qt入门学习——Qt Creator的使用**
* [Qt入门学习——Qt Creator的使用](https://blog.csdn.net/tennysonsky/article/details/48004119)

-[ ] **Qt 学习之路 2**
* [Qt 学习之路 2](https://www.devbean.net/category/qt-study-road-2/)

# 2019.01.28
- [ ] 服务器安装Anaconda环境
* [blog](https://blog.csdn.net/qq_17534301/article/details/80869998)
* [Installing on Linux](http://docs.anaconda.com/anaconda/install/linux/)
* [Anaconda installer archive](https://repo.anaconda.com/archive/)

# ==TODO==
- [ ] 安装Caffe,跑FCN
- [ ] 跑通Faster R-CNN

- [ ] [Object Detection and Classification using R-CNNs](http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/)

- [ ] [完全解析RNN, Seq2Seq, Attention注意力机制](https://zhuanlan.zhihu.com/p/51383402)

